
## how to do filters
from pyspark.sql.functions import col
# join empDF ,deptData and addData
mainDF=empDF.join(deptDF,empDF.emp_dept_id==deptDF.dept_id).join(addDF,empDF.emp_id==addDF.emp_id)\
.select(empDF.emp_id.alias("emp_name"),addDF.emp_id,empDF.name,deptDF.dept_name,addDF.addline1,addDF.city)\
.filter( (addDF.state  == "CA") & (addDF.city  == "SFO") ) 
mainDF.show()

print(pivoteentitlementdf.select(pivoteentitlementdf["entitlement_id"]).distinct().count())
##
from pyspark.sql.functions import distinct

distinct number and count 
## print count of unique pivoteentitlementdf values and show the table result aswell 
pivoteentitlementdf.select(pivoteentitlementdf["INDEPENT PRES"]).distinct().count()




if len(mainDF.columns)>2 and len(mainDF.columns)<5:
    print("good")
else:
    print("data frame lenght is bad")
if mainDF.select(mainDF.emp_id).distinct().count()==1:
    print("good")
elif mainDF.select(mainDF.emp_id).distinct().count()>1:
    print("bad")
elif mainDF.select(mainDF.emp_id).distinct().count()<1:
    print("bad")
if mainDF.select(mainDF.dept_name).distinct().count()==1:
    print("good")
elif mainDF.select(mainDF.dept_name).distinct().count()>1:
    print("bad")
elif mainDF.select(mainDF.dept_name).distinct().count()<1:
    print("bad")
    
    
    
    columns_name=mainDF1.columns

if columns_name == addDF2.columns:
    print("Columns matched")
else:
    print("Columns not matched")
    print("Columns not matched are:")
    for i in columns_name:
        if i not in addDF2.columns:  # if i not in mainDF1.columns then print i else print nothing 
            print(i)


    
    
    
    
    
    # combine two backslash and code comment\
mainDF=(empDF.join( deptDF,empDF.emp_dept_id==deptDF.dept_id).join(addDF,empDF.emp_id==addDF.emp_id) # this is to join empDF ,deptData and addData
.select(empDF.emp_id.alias("emp_name"),addDF.emp_id,empDF.name,deptDF.dept_name,addDF.addline1,addDF.city) # this is to select the columns
.filter( (addDF.state  == "CA") & (addDF.city  == "SFO"))                                       # this is to filter the data
)
mainDF.show()





from pyspark.sql.types import StructType, IntegerType, StringType
## transform the use indexer to encode the alphanumarical column guid_column
from pyspark.ml.feature import StringIndexer
indexer = StringIndexer(inputCol="guid_column", outputCol="indexed")
indexerModel = indexer.fit(empDF)
indexed = indexerModel.transform(empDF)
#indexed.show()


# change indexed column from decimal to integer column type
#indexed.select(col("indexed").cast(IntegerType())).show()

# change indexed column from decimal to integer column type and select all columns
indexed.select("*",col("indexed").cast(IntegerType())).show()




https://towardsdatascience.com/adding-sequential-ids-to-a-spark-dataframe-fa0df5566ff6

https://stackoverflow.com/questions/45035940/how-to-pivot-on-multiple-columns-in-spark-sql

azure cost link
https://docs.microsoft.com/en-us/azure/cost-management-billing/costs/tutorial-export-acm-data?tabs=azure-portal




empDF.select("*").where(col("DATE") == empDF.agg({"DATE":"max"}).first()["max(DATE)"]).show()


solving ambiguity 
empDF.alias("a").join(addDF.alias("b"),empDF.emp_id==addDF.emp_id).select(col('a.emp_id').alias('1'),'b.*').show()
comboDF=empDF.alias("a").join(addDF.alias("b"),empDF.emp_id==addDF.emp_id).select(col('a.emp_id').alias('emd'),'a.*','b.*').drop('emp_id')





# write a function to sort the columns by alphabetical order
def sortColumns(df):
    return df.select([col(c).alias(c) for c in sorted(df.columns)])
comboDF.show()
sortColumns(comboDF).show()

# test the function
comboDF.select(sortColumns(comboDF)).show()



# column reorder
sequence = ["addline1", "city", "state", "emp_id"]

addDF.select(sequence + [col for col in addDF.columns if col not in sequence]).show()

# column reorder
addDF.select(sorted(addDF.columns,reverse=True)).show()




#

SELECT 
 *
FROM orders
GROUP BY OrderID having max(OrderDate)


##
SELECT report_id, computer_id, MAX(CAST(date_entered AS CHAR))
FROM reports
GROUP BY report_id, computer_id

##
SELECT 
 *
FROM orders
GROUP BY OrderID having max(OrderDate)
##
select *, max(cast(OrderDate as date)) from orders
group by orderID


import pgeocode
import pandas as pd

def get_place_name(df):
    for index, row in df.iterrows():
        postcode = row['postcode']
        nomi = pgeocode.Nominatim('GB')
        postal_code = nomi.query_postal_code(postcode)
        df.loc[index, ['postal_code']] = postal_code
    return df

df = get_place_name(df)
df.head()




def get_place_name(df):
    df.loc[:, 'place_name'] = df['postcode'].apply(lambda x: pgeocode.Nominatim('GB').query_postal_code(x))
    return df


df = get_place_name(df)
df.head()



# VIEW SYNAPASE

https://docs.microsoft.com/en-us/power-apps/maker/data-platform/azure-synapse-link-create-view

https://www.youtube.com/watch?v=jE_-QLN7GT0




https://stackoverflow.com/questions/60811094/read-azure-synapse-table-with-spark


https://www.c-sharpcorner.com/article/analyse-data-with-spark-pool-in-azure-synapse-analyticspart2/


### Must Datavserse and Synapse Link
https://www.taygan.co/blog/2022/06/28/azure-synapse-link-for-dataverse

https://endjin.com/blog/2021/05/how-to-use-azure-synapse-sql-serverless-to-connect-data-lake-and-power-bi

https://docs.microsoft.com/en-us/power-apps/maker/data-platform/azure-synapse-link-create-view



best key word => How to use Azure Synapse SQL Serverless to connect Data Lake and Power BI
end to end synapase

https://www.youtube.com/watch?v=1fyc5806Xpw
https://www.youtube.com/watch?v=IUBlSyTDEIA&t=659s
https://www.youtube.com/watch?v=TpdoXR2p8iQ
https://www.youtube.com/watch?v=GaEmSA6GF6E

must video ###########################
https://www.youtube.com/watch?v=2xndDUXMnlc   





# remove the character 'w7' use regrex pattern matching to remove the character 'w7' from the place_name column 
# import re
# def remove_w7(df):
#     df['place_name'] = df['place_name'].apply(lambda x: re.sub(r'W71HS', '', x))
#     return df


# df = remove_w7(df)
# df.head()




# VIEW SYNAPASE

https://docs.microsoft.com/en-us/power-apps/maker/data-platform/azure-synapse-link-create-view

https://www.youtube.com/watch?v=jE_-QLN7GT0




https://stackoverflow.com/questions/60811094/read-azure-synapse-table-with-spark


https://www.c-sharpcorner.com/article/analyse-data-with-spark-pool-in-azure-synapse-analyticspart2/


### Must Datavserse and Synapse Link
https://www.taygan.co/blog/2022/06/28/azure-synapse-link-for-dataverse

https://www.youtube.com/watch?v=jNfR864_ZZQ

## creating view from data lake example

https://support.timextender.com/hc/en-us/articles/4423621859613-Create-VIEWS-based-on-Azure-Data-Lake-data-with-Azure-Synapse-Workspace

append_df.write.mode('append').format('parquet').save('/mnt/adls/covid/base/Covid19_Cases')












import pgeocode
import pandas as pd


df = pd.DataFrame(test_list, columns=columns)
df.head()


def get_place_name(df):
    df.loc[:, 'postal_code'] = df['postcode'].apply(lambda x: pgeocode.Nominatim('GB').query_postal_code(x))
    return df


df = get_place_name(df)
df.head()


# convert df to pyspark dataframe
df_spark = spark.createDataFrame(df)
df_spark.show()







empDF.filter(~col("post_code")
    .rlike("^[0-9]*$")
  ).show()







https://www.getthedata.com/open-postcode-geo-api
https://www.getthedata.com/open-postcode-geo
https://geoportal.statistics.gov.uk/
https://geoportal.statistics.gov.uk/datasets/national-statistics-postcode-lookup-2021-census-august-2022/about


################################################################################################


https://github.com/DenisCarriere/geocoder
https://github.com/topics/postcode?l=python





################power bi change source link################
https://businessintelligist.com/2020/07/09/how-to-switch-your-power-bi-table-to-a-different-data-source-type-without-breaking-anything/
