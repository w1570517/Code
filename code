
from pyspark.sql import functions as F

# Create a new column "total_hit" as the sum of all hits in columns "col a", "col b" and "col c"
df = df.withColumn("total_hit", F.sum(F.when(F.col("col a") == 'dd', 1).otherwise(0), F.when(F.col("col b") == 'dd', 1).otherwise(0), F.when(F.col("col c") == 'dd', 1).otherwise(0)))

# Create a new column "percentage" based on the percentage of hits in the "total_hit" column
df = df.withColumn("percentage", F.round((F.col("total_hit") / F.count("*")) * 100, 2))


https://docs.google.com/presentation/d/1WrkeJ9-CjuotTXoa4ZZlB3UPBXpxe4B3FMs9R9tn34I/edit#slide=id.g164b1bac824_0_5291


https://www.oreilly.com/radar/radar-trends-to-watch-december-2022/

https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai-in-2022-and-a-half-decade-in-review



from pyspark.sql.functions import col
from pyspark.sql.functions import udf
from pyspark.sql.types import DoubleType
from pyspark_fuzzy.functions import jaro_winkler_similarity

# Define a UDF for calculating the Jaro-Winkler similarity
udf_similarity = udf(lambda s1, s2: jaro_winkler_similarity(s1, s2), DoubleType())

# Join the two datasets
df1.join(df2, udf_similarity(col("cola"), col("col e")) >= 0.6)

# Select the columns of interest
result = df1.select("cola", "colb", "colc", "col e", "col f", udf_similarity(col("cola"), col("col e")).alias("similarity"))





from pyspark.sql.functions import levenshtein

# Join the two datasets
df_join = df1.alias("df1").join(df2.alias("df2"), levenshtein(col("df1.cola"), col("df2.col e")) <= 60)

# Select the columns of interest
result = df_join.select("df1.cola", "df1.colb", "df1.colc", "df2.col e", "df2.col f", levenshtein(col("df1.cola"), col("df2.col e")).alias("similarity"))





from pyspark.sql.functions import levenshtein, jaro_winkler

# Define a UDF to take the average of Jaro-Winkler and Levenshtein distance similarity scores
def combined_similarity(s1, s2):
    return (jaro_winkler(s1, s2) + levenshtein(s1, s2)) / 2

udf_combined_similarity = udf(combined_similarity, DoubleType())

# Join the two datasets
df_join = df1.alias("df1").join(df2.alias("df2"), udf_combined_similarity(col("df1.cola"), col("df2.col e")) >= threshold)

# Select the columns of interest
result = df_join.select("df1.cola", "df1.colb", "df1.colc", "df2.col e", "df2.col f", udf_combined_similarity(col("df1.cola"), col("df2.col e")).alias("similarity"))



