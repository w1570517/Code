from pyspark.sql.functions import lower, trim, regexp_replace

# Preprocess the postcode column in dataset1 and dataset2 by converting the values to lowercase 
# and removing spaces, and concatenating the first and second part of postcode if they are different
dataset1 = dataset1.withColumn("b", lower(trim(dataset1.b)))
dataset1 = dataset1.withColumn("b", regexp_replace(dataset1.b, "([a-z]+)(\s)([a-z]+)", "$1$3"))

dataset2 = dataset2.withColumn("c", lower(trim(dataset2.c)))
dataset2 = dataset2.withColumn("c", regexp_replace(dataset2.c, "([a-z]+)(\s)([a-z]+)", "$1$3"))









# Start a Spark session
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("Fuzzy Matching").getOrCreate()

# Install the fuzzywuzzy library
!pip install fuzzywuzzy[speedup]
from fuzzywuzzy import fuzz
from pyspark.sql.functions import udf
from pyspark.sql.types import IntegerType

# Define a UDF to compare the similarity of two strings
compare_strings = udf(lambda x, y: fuzz.token_set_ratio(x, y), IntegerType())

# Read dataset1 and dataset2 into dataframes
dataset1 = spark.read.format("csv") \
    .options(header="true", inferschema="true", 
             charset="UTF-8", delimiter=",", 
             path = "wasbs://container@storageAccount.blob.core.windows.net/path/to/dataset1.csv" ) \
    .load()
dataset2 = spark.read.format("csv") \
    .options(header="true", inferschema="true", 
             charset="UTF-8", delimiter=",", 
             path = "wasbs://container@storageAccount.blob.core.windows.net/path/to/dataset2.csv" ) \
    .load()

# Join the two dataframes on the column "b" of dataset1 and column "c" of dataset2
# using the compare_strings UDF to calculate the similarity between the values
# and only keep the rows where the similarity is greater than the threshold
similarity_threshold = 80
matched_data = dataset1.join(dataset2, dataset1.b == dataset2.c) \
    .select("dataset1.*", "dataset2.*", compare_strings(dataset1.b, dataset2.c).alias("similarity")) \
    .where(compare_strings(dataset1.b, dataset2.c) > similarity_threshold)

# Show the matched data
matched_data.show()










#####################################################################################################


from pyspark.sql.functions import udf
import pgeocode

# Initialize the pgeocode library
nomi = pgeocode.GeoDistance('GB')

# Define a UDF to extract geographic information from a postcode
def extract_geo_info(postcode):
    try:
        info = nomi.query_postal_code(postcode)
        return (info['place_name'], info['region_name'], info['area_name'], info['county_name'])
    except:
        return ("N/A", "N/A", "N/A", "N/A")

extract_geo_info_udf = udf(extract_geo_info, returnType=ArrayType(StringType()))

# Apply the UDF to the "b" column of the dataset1
dataset1 = dataset1.withColumn("geo_info", extract_geo_info_udf(dataset1.b))

# Extract each item of geo_info list and create new columns for them
dataset1 = dataset1.withColumn("place_name", dataset1.geo_info[0])
dataset1 = dataset1.withColumn("region_name", dataset1.geo_info[1])
dataset1 = dataset1.withColumn("area_name", dataset1.geo_info[2])
dataset1 = dataset1.withColumn("county_name", dataset1.geo_info[3])

# Show the updated dataset1
dataset1.show()





######################################################################################





from fuzzywuzzy import fuzz
from pyspark.sql.functions import udf
from pyspark.sql.types import DoubleType

# Define a UDF to compare the similarity of two strings
compare_strings = udf(lambda x, y: fuzz.token_set_ratio(x, y), DoubleType())

# Join the two dataframes on the column "b" of dataset1 and column "c" of dataset2
# using the compare_strings UDF to calculate the similarity between the values
# and only keep the rows where the similarity is greater than the threshold
similarity_threshold = 80
matched_data = dataset1.join(dataset2, dataset1.b == dataset2.c) \
    .select(dataset1.b, dataset2.c, compare_strings(dataset1.b, dataset2.c).alias("similarity")) \
    .where(compare_strings(dataset1.b, dataset2.c) > similarity_threshold)

# Show the matched data
matched_data.show()

%python
spark.conf.set("spark.synapse.python.library", "<library-name>")
spark.conf.set("spark.synapse.python.library.version", "<library-version>")


%pip install python-Levenshtein

%pip install fuzzywuzzy[speedup]


from pyspark.sql.functions import levenshtein





from synapse_py_tools.sql_functions import fuzzywuzzy
%pip install synapse-py-tools




compare_strings = udf(lambda x, y: SequenceMatcher(None, x, y).ratio()*100, DoubleType())


dataset1 = dataset1.withColumn("b", when((dataset1["id"] == 2) & (dataset1["b"] == "-"), lit(0)).otherwise(dataset1["b"]))





Hi Celso,

I have just rechecked and it is working now. Thank you very much for your help. Please feel free to close the ticket.

Regarding the other issue with the error message showing on the Azure Portal, have you been able to figure out why it is happening?

Thank you again for your assistance.

Best regards,
[Your Name]



from sklearn.decomposition import LatentDirichletAllocation
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

# Define the range of possible number of topics
num_topics = range(2, 20)

# Create an empty list to store the silhouette scores
sil_scores = []

for k in num_topics:
    # Create an instance of LDA with the number of topics
    lda = LatentDirichletAllocation(n_components=k)
    # Fit the LDA model to the data
    lda.fit(count_data)
    # Get the silhouette score for the current number of topics
    sil_scores.append(silhouette_score(count_data, lda.transform(count_data)))

# Plot the silhouette scores
plt.plot(num_topics, sil_scores)
plt.xlabel("Number of Topics")
plt.ylabel("Silhouette Score")
plt.title("Silhouette Score vs Number of Topics")
plt.show()





perplexities = []
for i in range(1, 20):
    lda = LatentDirichletAllocation(n_components=i, max_iter=5,
                                    learning_method='online',
                                    learning_offset=50.,
                                    random_state=0)
    lda.fit(count_data)
    perplexities.append(lda.perplexity(count_data))
plt.plot(range(1, 20), perplexities)
plt.title("Perplexity vs Number of Topics")
plt.xlabel("Number of Topics")
plt.ylabel("Perplexity")
plt.show()


from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("example").getOrCreate()

table_name = "mytable"

spark.sql("DROP TABLE IF EXISTS " + table_name)

spark.sql("USE <database_name>")
spark.sql("DROP TABLE <table_name>")







from pyspark.sql.functions import when, countDistinct, count, first

# Create a new column called 'check' with default value of 1
datasetLoc = datasetLoc.withColumn("check", lit(1))

# Get a list of all place names in the dataset
place_names = datasetLoc.select("place_name").rdd.flatMap(lambda x: x).collect()

# Loop through each place name
for place_name in place_names:
    # Check if the place name is unique 
    unique_loc_count = datasetLoc.filter(datasetLoc["place_name"] == place_name).agg(countDistinct("Loc_Name")).first()[0]
    if unique_loc_count > 1:
        # Check if the loc_name corresponding to place_name is repeating and all have the same name
        loc_name_count = datasetLoc.filter(datasetLoc["place_name"] == place_name).agg(count("Loc_Name"), first("Loc_Name")).first()
        if loc_name_count[0] != loc_name_count[1]:
            datasetLoc = datasetLoc.withColumn("check", when(datasetLoc["place_name"] == place_name, 0).otherwise(datasetLoc["check"]))

# Find the rows that meet the conditions
qualified_rows = datasetLoc.filter(datasetLoc["check"] == 1)













from pyspark.sql.functions import when, countDistinct, count, first

# Create a new column called 'check' with default value of 1
datasetLoc = datasetLoc.withColumn("check", lit(1))

# Get a list of all place names in the dataset
place_names = datasetLoc.select("place_name").rdd.flatMap(lambda x: x).collect()

# Loop through each place name
for place_name in place_names:
    # Check if the place name is unique 
    unique_loc_count = datasetLoc.filter(datasetLoc["place_name"] == place_name).agg(countDistinct("Loc_Name")).first()[0]
    if unique_loc_count > 1:
        # Check if the loc_name corresponding to place_name is repeating and all have the same name
        loc_name_count = datasetLoc.filter(datasetLoc["place_name"] == place_name).agg(count("Loc_Name"), first("Loc_Name")).first()
        if loc_name_count[0] != loc_name_count[1]:
            datasetLoc = datasetLoc.withColumn("check", when(datasetLoc["place_name"] == place_name, 0).otherwise(datasetLoc["check"]))

# Find the rows that meet the conditions
qualified_rows = datasetLoc.filter(datasetLoc["check"] == 1)


















from pyspark.sql.functions import count, max, when

# Create a new column called 'check' with default value of 1
datasetLoc = datasetLoc.withColumn("check", lit(1))

# Get a list of all place names in the dataset
place_names = datasetLoc.select("place_name").distinct().rdd.flatMap(lambda x: x).collect()

# Loop through each place name
for place_name in place_names:
    # check for condition 1
    loc_name_count = datasetLoc.filter(datasetLoc["place_name"] == place_name).groupBy("Loc_Name").agg(count("Loc_Name").alias("count"))
    loc_name_count = loc_name_count.filter(loc_name_count["count"] > 1).count()
    if loc_name_count == 0:
        datasetLoc = datasetLoc.withColumn("check", when(datasetLoc["place_name"] == place_name, 1).otherwise(datasetLoc["check"]))
    else:
        # check for condition 2
        max_loc_name = datasetLoc.filter(datasetLoc["place_name"] == place_name).groupBy("Loc_Name").agg(count("Loc_Name").alias("count"))
        max_loc_name = max_loc_name.agg(max("count")).first()[0]
        loc_name_count = datasetLoc.filter(datasetLoc["place_name"] == place_name).groupBy("Loc_Name").agg(count("Loc_Name").alias("count"))
        loc_name_count = loc_name_count.filter(loc_name_count["count"] == max_loc_name).count()
        if loc_name_count == 1:
            datasetLoc = datasetLoc.withColumn("check", when(datasetLoc["place_name"] == place_name, 1).otherwise(datasetLoc["check"]))
        else:
            datasetLoc = datasetLoc.withColumn("check", when(datasetLoc["place_name"] == place_name, 0).otherwise(datasetLoc["check"]))

# Find the rows that meet the conditions
qualified_rows = datasetLoc.filter(datasetLoc["check"] == 1)









df = df.groupBy("placename").count()

# Add a new column to the dataset indicating whether the "placename" is unique (1) or duplicate (0)
df = df.withColumn("check", when(col("count") > 1, 0).otherwise(1))

# Show the resulting dataset
df.show()






from pyspark.sql.functions import col, when, lead
from pyspark.sql import Window

# Load your dataframe
df = spark.read.csv("path/to/your_file.csv", inferSchema=True, header=True)

# Create a window to compare the current row's 'placename' with the next row's 'placename'
window = Window.partitionBy("placename").orderBy("placename")

# Create a new column 'duplicate' with value 1 if the current row's 'placename' is the same as the next row's 'placename' and the current row's 'loc' is the same as the next row's 'loc', 0 otherwise
df = df.withColumn("duplicate", when(col("placename") == lead("placename").over(window) & col("loc") == lead("loc").over(window), 1).otherwise(0))

# Select only the 'placename', 'loc', and 'duplicate' columns
df = df.select("placename", "loc", "duplicate")

# Show the resulting dataframe
df.show()














from pyspark.sql.functions import col, when, lead
from pyspark.sql import Window

# Load your dataframe
df = spark.read.csv("path/to/your_file.csv", inferSchema=True, header=True)

# Create a window to compare the current row's 'placename' with the next row's 'placename'
window = Window.partitionBy("placename").orderBy("placename")

# Create a new column 'duplicate' with value 1 if the current row's 'placename' is the same as the next row's 'placename' and the current row's 'loc' is the same as the next row's 'loc', 0 otherwise
df = df.withColumn("duplicate", when(col("placename") == lead("placename").over(window) & col("loc") == lead("loc").over(window), 1).otherwise(0))

# Select only the 'placename', 'loc', and 'duplicate' columns
df = df.select("placename", "loc", "duplicate")

# Show the resulting dataframe
df.show()














from pyspark.sql.functions import col, when, lead
from pyspark.sql import Window

# Load your dataframe
df = spark.read.csv("path/to/your_file.csv", inferSchema=True, header=True)

# Create a window to compare the current row's 'placename' with the next row's 'placename'
window = Window.partitionBy().orderBy("placename")

df = df.withColumn("duplicate", when((col("placename") == lead("placename").over(window)) & (col("loc") == lead("loc").over(window)), 1).otherwise(0))

# Select only the 'placename', 'loc', and 'duplicate' columns
df = df.select("placename", "loc", "duplicate")

# Show the resulting dataframe
df.show()
