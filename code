from pyspark.sql.functions import udf, col, broadcast
from pyspark.sql.types import DoubleType
from difflib import SequenceMatcher

# Create a UDF to calculate the Levenshtein distance
def levenshtein_distance(x,y):
  if x is None or y is None:
    return None
  else:
    return SequenceMatcher(None, x, y).ratio()

levenshtein_udf = udf(levenshtein_distance, DoubleType())

from pyspark.sql.functions import coalesce

# Replace null values in columns "Primary_Address_City2" and "place20nm2" with empty strings
dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2", coalesce(col("Primary_Address_City2"), F.lit("")))
df2_postalcode = df2_postalcode.withColumn("place20nm2", coalesce(col("place20nm2"), F.lit("")))

# Join the two dataframes on the fuzzy match
similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(broadcast(df2_postalcode.alias("df2")), (levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")) > similarity_threshold))

# Select columns from both dataframes and calculate Levenshtein distance
match_data = df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.ctry20nm", levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")).alias("levenshtein_distance"))




WITH subquery AS (
  SELECT studentid, studentcourse
  FROM student
  GROUP BY studentid, studentcourse
  HAVING COUNT(*) = 1
)
SELECT studentid, 
       STRING_AGG(studentcourse, ', ') WITHIN GROUP (ORDER BY studentcourse) AS courses
FROM subquery
GROUP BY studentid
HAVING COUNT(*) = 2;












WITH subquery AS (
  SELECT studentid, studentcourse
  FROM student
  GROUP BY studentid, studentcourse
  HAVING COUNT(*) = 1
)
SELECT studentid, 
       STRING_AGG(studentcourse, ', ') WITHIN GROUP (ORDER BY studentcourse) AS courses,
       CASE 
         WHEN COUNT(*) = 2 THEN 'Yes'
         ELSE 'No'
       END AS meeting_condition
FROM subquery
GROUP BY studentid;

















WITH subquery AS (
  SELECT studentid, studentcourse
  FROM student
  GROUP BY studentid, studentcourse
  HAVING COUNT(*) = 1
)
SELECT TOP 10 studentid, 
       STRING_AGG(studentcourse, ', ') WITHIN GROUP (ORDER BY studentcourse) AS courses,
       CASE 
         WHEN COUNT(*) = 2 THEN 'Yes'
         ELSE 'No'
       END AS meeting_condition
FROM subquery
GROUP BY studentid
ORDER BY COUNT(*) DESC;
















WITH subquery AS (
  SELECT studentid, studentcourse
  FROM student
  GROUP BY studentid, studentcourse
  HAVING COUNT(*) = 1
)
SELECT TOP 10 r.request_id, 
       STRING_AGG(s.studentcourse, ', ') WITHIN GROUP (ORDER BY s.studentcourse) AS courses,
       CASE 
         WHEN COUNT(*) = 2 THEN 'Yes'
         ELSE 'No'
       END AS meeting_condition
FROM registration r
LEFT JOIN (
  SELECT studentid, 
         studentcourse
  FROM subquery
  GROUP BY studentid
) s
ON r.studentid = s.studentid
GROUP BY r.request_id, r.studentid
ORDER BY COUNT(*) DESC;









WITH subquery AS (
  SELECT Registrant_ID, 
         Registrant_Name,
         CASE 
           WHEN COUNT(*) = 2 THEN 'Yes'
           ELSE 'No'
         END AS meeting_condition
  FROM (
    SELECT Registrant_ID, Registrant_Name
    FROM registration2
    GROUP BY Registrant_ID, Registrant_Name
    HAVING COUNT(*) = 1
  ) sub
  GROUP BY Registrant_ID
)
SELECT r.Registrant_ID,
       r.Registrant_Name,
       r.active,
       s.registrantdifference,
       s.meeting_condition
FROM registration r
LEFT JOIN (
  SELECT Registrant_ID, 
         STRING_AGG(Registrant_Name, ', ') WITHIN GROUP (ORDER BY Registrant_Name) AS registrantdifference,
         meeting_condition
  FROM subquery
) s
ON r.Registrant_ID = s.Registrant_ID;



WITH subquery AS (
  SELECT Registrant_ID, 
         STRING_AGG(Registration_Name, ', ') WITHIN GROUP (ORDER BY Registration_Name) AS registrantdifference,
         CASE 
           WHEN COUNT(DISTINCT Registration_Name) = 2 THEN 'Yes'
           ELSE 'No'
         END AS meeting_condition
  FROM (
    SELECT Registrant_ID, Registration_Name
    FROM Registration2
    GROUP BY Registrant_ID, Registration_Name
  ) sub
  GROUP BY Registrant_ID
)
SELECT r.Registrant_ID,
       r.Registrant_Name,
       r.active,
       s.registrantdifference,
       s.meeting_condition
FROM registration r
LEFT JOIN subquery s
ON r.Registrant_ID = s.Registrant_ID;






















































WITH subquery AS (
SELECT Registrant_ID,
STRING_AGG(Profession_Name, ', ') WITHIN GROUP (ORDER BY Profession_Name) AS registrantdifference,
CASE
WHEN COUNT(DISTINCT Profession_Name) = 2 THEN 'Yes'
ELSE 'No'
END AS meeting_condition,
CASE
WHEN SUM(CASE WHEN Regsitration_State_Description IN ('inactive', 'blank') THEN 1 ELSE 0 END) = 2 THEN 1
ELSE 0
END AS flag
FROM (
SELECT Registrant_ID,
Profession_Name,
Regsitration_State_Description
FROM [dbo].[registrations]
GROUP BY Registrant_ID,
Profession_Name,
Regsitration_State_Description
) sub
GROUP BY Registrant_ID
)

SELECT Optevia_Contact_ID AS Contact_ID ,
r.Registrant_ID,
r.[Regsitration_State_Description],
r.[Registration_Status_Description],
r.[Profession_Name],
r.[Date_Registered],
r.Registration_Name,
r.[First_Registration_Date] AS [First Registration Date],
r.[Date_Deregistered] AS [Date Deregistered],
r.[Date_Registered] AS [Date most recently registered],
r.[Date_Registered_From] AS [Date registered from],
r.[Date_Registered_To] AS [Date registered to],
s.registrantdifference,
s.meeting_condition AS [ProfessionChange(Yes_No)],
s.flag AS [Flag for Inactive and Blank]
FROM [dbo].[registrations] r
LEFT JOIN subquery s
ON r.Registrant_ID = s.Registrant_ID
Left join [dbo].[contacts] c
ON r.Registrant_ID = c.[Contact_ID]
GROUP BY Optevia_Contact_ID,
r.Registrant_ID,
r.Registration_Name,
r.[Regsitration_State_Description],
r.[Registration_Status_Description],
r.[Profession_Name],
r.[First_Registration_Date],
r.[Date_Registered],
r.[Date_Deregistered],
r.[Date_Registered_From],
r.[Date_Registered_To],
s.registrantdifference,
s.meeting_condition,
s.flag
































Hearingconcludedontime? =
IF(
ISBLANK(Raw_Data2[Panel Member Cancelled Date]),
"Unknown",
IF(
Raw_Data2[Panel Member Cancelled Date] > 0,
"Cancelled",
IF(
Raw_Data2[Hearing To] = Raw_Data2[Actual Hearing End Date],
"On time",
IF(
Raw_Data2[Hearing To] > Raw_Data2[Actual Hearing End Date],
"Early",
"Later"
)
)
)
)


= IF(Raw_Data2[Panel Member Cancelled Date] > 0, "Cancelled",
IF(Raw_Data2[Actual Hearing End Date]=Raw_Data2[Hearing To],"On time",
IF(Raw_Data2[Actual Hearing End Date] > Raw_Data2[Hearing To],"Early","Later")
)
)









Join the two dataframes on the fuzzy match
similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(df2_postalcode.alias("df2"), 
    (levenshtein_udf(
        col("df1.Primary_Address_City2").cast("string").alias("text1"),
        col("df2.place20nm2").cast("string").alias("text2")) > similarity_threshold))

Select columns from both dataframes
matched_data=df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Primary_Address_City2"
,"df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.place20nm2"
,"df2.eer20nm","df2.ctry20nm",levenshtein_udf(
    col("df1.Primary_Address_City2").cast("string"), 
    col("df2.place20nm2").cast("string")).alias("levenshtein_distance"))
    
    
    
    
    
    
    
    
    
    
 from diff_match_patch import diff_match_patch
from pyspark.sql.functions import udf, col, coalesce
from pyspark.sql import functions as F
from pyspark.sql.types import DoubleType

def levenshtein_distance(text1,text2):
    if text1 is None or text2 is None:
        return 0
    else:
        dmp = diff_match_patch()
        dmp.Diff_Timeout = 0.0
        diff = dmp.diff_main(text1, text2, False)
        common_text = sum([len(txt) for op, txt in diff if op == 0])
        text_length = max(len(text1), len(text2))
        sim = common_text / text_length
        return sim

levenshtein_udf = udf(levenshtein_distance, DoubleType())

# Make sure that the columns are of string type
dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2", coalesce(F.cast(col("Primary_Address_City2"), "string"), F.lit("")))
df2_postalcode = df2_postalcode.withColumn("place20nm2", coalesce(F.cast(col("place20nm2"), "string"), F.lit("")))

similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(df2_postalcode.alias("df2"), (levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")) > similarity_threshold))

matched_data=df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Primary_Address_City2",
"df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.place20nm2",
"df2.eer20nm","df2.ctry20nm",levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")).alias("levenshtein_distance"))






from diff_match_patch import diff_match_patch
from pyspark.sql.functions import coalesce, udf, lit, col, when
from pyspark.sql.types import DoubleType

def levenshtein_distance(text1,text2):
    if text1 is None or text2 is None:
        return 0
    else:
        dmp = diff_match_patch()
        dmp.Diff_Timeout = 0.0
        diff = dmp.diff_main(text1, text2, False)
        common_text = sum([len(txt) for op, txt in diff if op == 0])
        text_length = max(len(text1), len(text2))
        sim = common_text / text_length
        return sim

levenshtein_udf = udf(levenshtein_distance, DoubleType())

dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2", coalesce(col("Primary_Address_City2"), lit("")))
df2_postalcode = df2_postalcode.withColumn("place20nm2", coalesce(col("place20nm2"), lit("")))

similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(df2_postalcode.alias("df2"), when(levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")) > similarity_threshold, 1).otherwise(0) == 1)

matched_data = df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Primary_Address_City2",
                                "df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm",
                                "df2.eer20nm","df2.place20nm2","df2.eer20nm","df2.ctry20nm",
                                levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")).alias("levenshtein_distance"))
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                from diff_match_patch import diff_match_patch
from pyspark.sql.functions import udf, col, coalesce, when
from pyspark.sql.types import DoubleType, StringType

def levenshtein_distance(text1,text2):
    if text1 is None or text2 is None:
        return 0
    else:
        dmp = diff_match_patch()
        dmp.Diff_Timeout = 0.0
        diff = dmp.diff_main(text1, text2, False)
        common_text = sum([len(txt) for op, txt in diff if op == 0])
        text_length = max(len(text1), len(text2))
        sim = common_text / text_length
        return sim

levenshtein_udf = udf(levenshtein_distance, DoubleType())

# Cast the columns to string type
dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2", when(col("Primary_Address_City2").isNotNull(), col("Primary_Address_City2").cast(StringType())).otherwise(""))
df2_postalcode = df2_postalcode.withColumn("place20nm2", when(col("place20nm2").isNotNull(), col("place20nm2").cast(StringType())).otherwise(""))

# Join the two dataframes on the fuzzy match
similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(df2_postalcode.alias("df2"), (levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")) > similarity_threshold))

# Select columns from both dataframes
matched_data = df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Primary_Address_City2","df1.Work_PostalCode","df1.Work_PostalCode2",
                                "df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.place20nm2","df2.eer20nm","df2.ctry20nm",
                                levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")).alias("levenshtein_distance"))
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
import pyspark.sql.functions as F
from fuzzywuzzy import fuzz
from pyspark.sql.types import IntegerType

def matchstring(s1, s2):
    return fuzz.ratio(s1, s2)

MatchUDF = F.udf(matchstring, IntegerType())

similarity_threshold = 90
df_joined = dff_regrandcon.alias("df1").join(df2_postalcode.alias("df2"), (MatchUDF(F.col("df1.Primary_Address_City2"), F.col("df2.place20nm2")) > similarity_threshold))

df_result = df_joined.groupBy("df1.Primary_Address_City2").agg(F.max(MatchUDF(F.col("df1.Primary_Address_City2"), F.col("df2.place20nm2"))).alias("max_score")).filter(F.col("max_score") >= similarity_threshold)
df_result = df_result.select("df1.*").join(df_joined, (df_result["df1.Primary_Address_City2"] == df_joined["df1.Primary_Address_City2"]) & (df_result["max_score"] == MatchUDF(F.col("df1.Primary_Address_City2"), F.col("df2.place20nm2"))))
