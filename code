from pyspark.sql.functions import udf, col, broadcast
from pyspark.sql.types import DoubleType
from difflib import SequenceMatcher

# Create a UDF to calculate the Levenshtein distance
def levenshtein_distance(x,y):
  if x is None or y is None:
    return None
  else:
    return SequenceMatcher(None, x, y).ratio()

levenshtein_udf = udf(levenshtein_distance, DoubleType())

from pyspark.sql.functions import coalesce

# Replace null values in columns "Primary_Address_City2" and "place20nm2" with empty strings
dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2", coalesce(col("Primary_Address_City2"), F.lit("")))
df2_postalcode = df2_postalcode.withColumn("place20nm2", coalesce(col("place20nm2"), F.lit("")))

# Join the two dataframes on the fuzzy match
similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(broadcast(df2_postalcode.alias("df2")), (levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")) > similarity_threshold))

# Select columns from both dataframes and calculate Levenshtein distance
match_data = df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.ctry20nm", levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")).alias("levenshtein_distance"))




WITH subquery AS (
  SELECT studentid, studentcourse
  FROM student
  GROUP BY studentid, studentcourse
  HAVING COUNT(*) = 1
)
SELECT studentid, 
       STRING_AGG(studentcourse, ', ') WITHIN GROUP (ORDER BY studentcourse) AS courses
FROM subquery
GROUP BY studentid
HAVING COUNT(*) = 2;












WITH subquery AS (
  SELECT studentid, studentcourse
  FROM student
  GROUP BY studentid, studentcourse
  HAVING COUNT(*) = 1
)
SELECT studentid, 
       STRING_AGG(studentcourse, ', ') WITHIN GROUP (ORDER BY studentcourse) AS courses,
       CASE 
         WHEN COUNT(*) = 2 THEN 'Yes'
         ELSE 'No'
       END AS meeting_condition
FROM subquery
GROUP BY studentid;

















WITH subquery AS (
  SELECT studentid, studentcourse
  FROM student
  GROUP BY studentid, studentcourse
  HAVING COUNT(*) = 1
)
SELECT TOP 10 studentid, 
       STRING_AGG(studentcourse, ', ') WITHIN GROUP (ORDER BY studentcourse) AS courses,
       CASE 
         WHEN COUNT(*) = 2 THEN 'Yes'
         ELSE 'No'
       END AS meeting_condition
FROM subquery
GROUP BY studentid
ORDER BY COUNT(*) DESC;
















WITH subquery AS (
  SELECT studentid, studentcourse
  FROM student
  GROUP BY studentid, studentcourse
  HAVING COUNT(*) = 1
)
SELECT TOP 10 r.request_id, 
       STRING_AGG(s.studentcourse, ', ') WITHIN GROUP (ORDER BY s.studentcourse) AS courses,
       CASE 
         WHEN COUNT(*) = 2 THEN 'Yes'
         ELSE 'No'
       END AS meeting_condition
FROM registration r
LEFT JOIN (
  SELECT studentid, 
         studentcourse
  FROM subquery
  GROUP BY studentid
) s
ON r.studentid = s.studentid
GROUP BY r.request_id, r.studentid
ORDER BY COUNT(*) DESC;









WITH subquery AS (
  SELECT Registrant_ID, 
         Registrant_Name,
         CASE 
           WHEN COUNT(*) = 2 THEN 'Yes'
           ELSE 'No'
         END AS meeting_condition
  FROM (
    SELECT Registrant_ID, Registrant_Name
    FROM registration2
    GROUP BY Registrant_ID, Registrant_Name
    HAVING COUNT(*) = 1
  ) sub
  GROUP BY Registrant_ID
)
SELECT r.Registrant_ID,
       r.Registrant_Name,
       r.active,
       s.registrantdifference,
       s.meeting_condition
FROM registration r
LEFT JOIN (
  SELECT Registrant_ID, 
         STRING_AGG(Registrant_Name, ', ') WITHIN GROUP (ORDER BY Registrant_Name) AS registrantdifference,
         meeting_condition
  FROM subquery
) s
ON r.Registrant_ID = s.Registrant_ID;



WITH subquery AS (
  SELECT Registrant_ID, 
         STRING_AGG(Registration_Name, ', ') WITHIN GROUP (ORDER BY Registration_Name) AS registrantdifference,
         CASE 
           WHEN COUNT(DISTINCT Registration_Name) = 2 THEN 'Yes'
           ELSE 'No'
         END AS meeting_condition
  FROM (
    SELECT Registrant_ID, Registration_Name
    FROM Registration2
    GROUP BY Registrant_ID, Registration_Name
  ) sub
  GROUP BY Registrant_ID
)
SELECT r.Registrant_ID,
       r.Registrant_Name,
       r.active,
       s.registrantdifference,
       s.meeting_condition
FROM registration r
LEFT JOIN subquery s
ON r.Registrant_ID = s.Registrant_ID;






















































WITH subquery AS (
SELECT Registrant_ID,
STRING_AGG(Profession_Name, ', ') WITHIN GROUP (ORDER BY Profession_Name) AS registrantdifference,
CASE
WHEN COUNT(DISTINCT Profession_Name) = 2 THEN 'Yes'
ELSE 'No'
END AS meeting_condition,
CASE
WHEN SUM(CASE WHEN Regsitration_State_Description IN ('inactive', 'blank') THEN 1 ELSE 0 END) = 2 THEN 1
ELSE 0
END AS flag
FROM (
SELECT Registrant_ID,
Profession_Name,
Regsitration_State_Description
FROM [dbo].[registrations]
GROUP BY Registrant_ID,
Profession_Name,
Regsitration_State_Description
) sub
GROUP BY Registrant_ID
)

SELECT Optevia_Contact_ID AS Contact_ID ,
r.Registrant_ID,
r.[Regsitration_State_Description],
r.[Registration_Status_Description],
r.[Profession_Name],
r.[Date_Registered],
r.Registration_Name,
r.[First_Registration_Date] AS [First Registration Date],
r.[Date_Deregistered] AS [Date Deregistered],
r.[Date_Registered] AS [Date most recently registered],
r.[Date_Registered_From] AS [Date registered from],
r.[Date_Registered_To] AS [Date registered to],
s.registrantdifference,
s.meeting_condition AS [ProfessionChange(Yes_No)],
s.flag AS [Flag for Inactive and Blank]
FROM [dbo].[registrations] r
LEFT JOIN subquery s
ON r.Registrant_ID = s.Registrant_ID
Left join [dbo].[contacts] c
ON r.Registrant_ID = c.[Contact_ID]
GROUP BY Optevia_Contact_ID,
r.Registrant_ID,
r.Registration_Name,
r.[Regsitration_State_Description],
r.[Registration_Status_Description],
r.[Profession_Name],
r.[First_Registration_Date],
r.[Date_Registered],
r.[Date_Deregistered],
r.[Date_Registered_From],
r.[Date_Registered_To],
s.registrantdifference,
s.meeting_condition,
s.flag
































Hearingconcludedontime? =
IF(
ISBLANK(Raw_Data2[Panel Member Cancelled Date]),
"Unknown",
IF(
Raw_Data2[Panel Member Cancelled Date] > 0,
"Cancelled",
IF(
Raw_Data2[Hearing To] = Raw_Data2[Actual Hearing End Date],
"On time",
IF(
Raw_Data2[Hearing To] > Raw_Data2[Actual Hearing End Date],
"Early",
"Later"
)
)
)
)


= IF(Raw_Data2[Panel Member Cancelled Date] > 0, "Cancelled",
IF(Raw_Data2[Actual Hearing End Date]=Raw_Data2[Hearing To],"On time",
IF(Raw_Data2[Actual Hearing End Date] > Raw_Data2[Hearing To],"Early","Later")
)
)









Join the two dataframes on the fuzzy match
similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(df2_postalcode.alias("df2"), 
    (levenshtein_udf(
        col("df1.Primary_Address_City2").cast("string").alias("text1"),
        col("df2.place20nm2").cast("string").alias("text2")) > similarity_threshold))

Select columns from both dataframes
matched_data=df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Primary_Address_City2"
,"df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.place20nm2"
,"df2.eer20nm","df2.ctry20nm",levenshtein_udf(
    col("df1.Primary_Address_City2").cast("string"), 
    col("df2.place20nm2").cast("string")).alias("levenshtein_distance"))
    
    
    
    
    
    
    
    
    
    
 from diff_match_patch import diff_match_patch
from pyspark.sql.functions import udf, col, coalesce
from pyspark.sql import functions as F
from pyspark.sql.types import DoubleType

def levenshtein_distance(text1,text2):
    if text1 is None or text2 is None:
        return 0
    else:
        dmp = diff_match_patch()
        dmp.Diff_Timeout = 0.0
        diff = dmp.diff_main(text1, text2, False)
        common_text = sum([len(txt) for op, txt in diff if op == 0])
        text_length = max(len(text1), len(text2))
        sim = common_text / text_length
        return sim

levenshtein_udf = udf(levenshtein_distance, DoubleType())

# Make sure that the columns are of string type
dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2", coalesce(F.cast(col("Primary_Address_City2"), "string"), F.lit("")))
df2_postalcode = df2_postalcode.withColumn("place20nm2", coalesce(F.cast(col("place20nm2"), "string"), F.lit("")))

similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(df2_postalcode.alias("df2"), (levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")) > similarity_threshold))

matched_data=df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Primary_Address_City2",
"df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.place20nm2",
"df2.eer20nm","df2.ctry20nm",levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")).alias("levenshtein_distance"))






from diff_match_patch import diff_match_patch
from pyspark.sql.functions import coalesce, udf, lit, col, when
from pyspark.sql.types import DoubleType

def levenshtein_distance(text1,text2):
    if text1 is None or text2 is None:
        return 0
    else:
        dmp = diff_match_patch()
        dmp.Diff_Timeout = 0.0
        diff = dmp.diff_main(text1, text2, False)
        common_text = sum([len(txt) for op, txt in diff if op == 0])
        text_length = max(len(text1), len(text2))
        sim = common_text / text_length
        return sim

levenshtein_udf = udf(levenshtein_distance, DoubleType())

dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2", coalesce(col("Primary_Address_City2"), lit("")))
df2_postalcode = df2_postalcode.withColumn("place20nm2", coalesce(col("place20nm2"), lit("")))

similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(df2_postalcode.alias("df2"), when(levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")) > similarity_threshold, 1).otherwise(0) == 1)

matched_data = df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Primary_Address_City2",
                                "df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm",
                                "df2.eer20nm","df2.place20nm2","df2.eer20nm","df2.ctry20nm",
                                levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")).alias("levenshtein_distance"))
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                from diff_match_patch import diff_match_patch
from pyspark.sql.functions import udf, col, coalesce, when
from pyspark.sql.types import DoubleType, StringType

def levenshtein_distance(text1,text2):
    if text1 is None or text2 is None:
        return 0
    else:
        dmp = diff_match_patch()
        dmp.Diff_Timeout = 0.0
        diff = dmp.diff_main(text1, text2, False)
        common_text = sum([len(txt) for op, txt in diff if op == 0])
        text_length = max(len(text1), len(text2))
        sim = common_text / text_length
        return sim

levenshtein_udf = udf(levenshtein_distance, DoubleType())

# Cast the columns to string type
dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2", when(col("Primary_Address_City2").isNotNull(), col("Primary_Address_City2").cast(StringType())).otherwise(""))
df2_postalcode = df2_postalcode.withColumn("place20nm2", when(col("place20nm2").isNotNull(), col("place20nm2").cast(StringType())).otherwise(""))

# Join the two dataframes on the fuzzy match
similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(df2_postalcode.alias("df2"), (levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")) > similarity_threshold))

# Select columns from both dataframes
matched_data = df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Primary_Address_City2","df1.Work_PostalCode","df1.Work_PostalCode2",
                                "df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.place20nm2","df2.eer20nm","df2.ctry20nm",
                                levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")).alias("levenshtein_distance"))
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
import pyspark.sql.functions as F
from fuzzywuzzy import fuzz
from pyspark.sql.types import IntegerType

def matchstring(s1, s2):
    return fuzz.ratio(s1, s2)

MatchUDF = F.udf(matchstring, IntegerType())

similarity_threshold = 90
df_joined = dff_regrandcon.alias("df1").join(df2_postalcode.alias("df2"), (MatchUDF(F.col("df1.Primary_Address_City2"), F.col("df2.place20nm2")) > similarity_threshold))

df_result = df_joined.groupBy("df1.Primary_Address_City2").agg(F.max(MatchUDF(F.col("df1.Primary_Address_City2"), F.col("df2.place20nm2"))).alias("max_score")).filter(F.col("max_score") >= similarity_threshold)
df_result = df_result.select("df1.*").join(df_joined, (df_result["df1.Primary_Address_City2"] == df_joined["df1.Primary_Address_City2"]) & (df_result["max_score"] == MatchUDF(F.col("df1.Primary_Address_City2"), F.col("df2.place20nm2"))))
#









DJoined = dff_regrandcon.crossJoin(df2_postalcode)
matched_data = DJoined.withColumn("score", MatchUDF(F.col("Primary_Address_City2"), F.col("place20nm2")))
max_score_row = matched_data.agg(F.max("score").alias("max_score")).collect()[0]
selected_row = matched_data.filter(F.col("score") == max_score_row["max_score"]).collect()[0]



from pyspark.sql.functions import broadcast
DJoined = dff_regrandcon.crossJoin(broadcast(df2_postalcode))
matched_data = DJoined.withColumn("score", MatchUDF(F.col("Primary_Address_City2"), F.col("place20nm2")))
max_score_row = matched_data.agg(F.max("score").alias("max_score")).collect()[0]
selected_row = matched_data.filter(F.col("score") == max_score_row["max_score"]).collect()[0]

from pyspark.sql.functions import broadcast
DJoined = dff_regrandcon.crossJoin(broadcast(df2_postalcode))
matched_data = DJoined.withColumn("score", MatchUDF(F.col("Primary_Address_City2"), F.col("place20nm2")))
max_score_row = matched_data.agg(F.max("score").alias("max_score")).collect()[0]
selected_row = matched_data.filter(F.col("score") == max_score_row["max_score"]).collect()[0]




df2_postalcode = df.withColumn("place20nm2", regexp_replace(df.place20nm, "[^a-zA-Z0-9]+", ""))




from pyspark.sql.functions import trim, regexp_replace, lower

df2_postalcode = df
df2_postalcode = df2_postalcode.withColumn("place20nm2", regexp_replace(df2_postalcode.place20nm, "\s+", " "))
df2_postalcode = df2_postalcode.withColumn("place20nm2", regexp_replace(df2_postalcode.place20nm2, "[.']+", ""))
df2_postalcode = df2_postalcode.withColumn("place20nm2", regexp_replace(df2_postalcode.place20nm2, "[^a-zA-Z0-9]+", ""))
df2_postalcode = df2_postalcode.withColumn("place20nm2", trim(df2_postalcode.place20nm2))
df2_postalcode = df2_postalcode.withColumn("place20nm2", regexp_replace(df2_postalcode.place20nm2, "1", "one"))
df2_postalcode = df2_postalcode.withColumn("place20nm2", regexp_replace(df2_postalcode.place20nm2, "2", "two"))
df2_postalcode = df2_postalcode.withColumn("place20nm2", regexp_replace(df2_postalcode.place20nm2, "St", "Street"))
df2_postalcode = df2_postalcode.withColumn("place20nm2", lower(df2_postalcode.place20nm2))





from pyspark.sql.functions import col, expr

matched_data = dff_regrandcon.alias("xrt") \
                              .join(df2_postalcode.alias("xrtx"), 
                                    expr("xrt.Primary_Address_City like concat('%', xrtx.place20nm, '%')")) \
                              .select("xrt.*", "xrtx.*")



from pyspark.sql.functions import col, expr

matched_data = dff_regrandcon.alias("xrt") \
                              .join(df2_postalcode.alias("xrtx"), 
                                    expr("xrt.Primary_Address_City like concat(xrtx.place20nm, '_')")) \
                              .select("xrt.*", "xrtx.*")



SELECT *
FROM dff_regrandcon xrt
JOIN df2_postalcode xrtx
ON xrt.Primary_Address_City LIKE concat(xrtx.place20nm, '_')


















let
    Source = List.Dates(#date(2000, 1, 1), #date(2030, 12, 31)),
    #"Converted to Table" = Table.FromList(Source, Splitter.SplitByNothing(), {"Date"}),
    #"Expanded Date" = Table.ExpandColumn(#"Converted to Table", "Date", {"Year", "Quarter", "Month", "Week", "Day", "Weekday"}),
    #"Added Custom" = Table.AddColumn(#"Expanded Date", "Year Month", each [Year] * 100 + [Month]),
    #"Added Custom1" = Table.AddColumn(#"Added Custom", "Year Month Day", each [Year Month] * 100 + [Day]),
    #"Added Custom2" = Table.AddColumn(#"Added Custom1", "Month Name", each Date.MonthName([Month]), type text),
    #"Added Custom3" = Table.AddColumn(#"Added Custom2", "Quarter Name", each "Q" & Number.ToText([Quarter]), type text),
    #"Added Custom4" = Table.AddColumn(#"Added Custom3", "Weekday Name", each Date.DayOfWeekName([Weekday]), type text)
in
    #"Added Custom4"










https://exceleratorbi.com.au/conditional-formatting-using-icons-in-power-bi/

https://learn.microsoft.com/en-us/power-bi/visuals/power-bi-visualization-scorecard-visual

https://learn.microsoft.com/en-us/power-bi/visuals/power-bi-visualization-tables?tabs=powerbi-desktop


Current Month Sales =
CALCULATE(SUM(Sales[SalesValue]), DATESYTD(Calendar[Date]), Sales[Category]=EARLIER(Sales[Category]))

Last Month Sales =
CALCULATE(SUM(Sales[SalesValue]), DATESYTD(Calendar[Date])-1, Sales[Category]=EARLIER(Sales[Category]))




Last Month Sales =
CALCULATE(SUM(Sales[SalesValue]), FILTER(Sales, Sales[Date] >= EOMONTH(TODAY(), -1) && Sales[Date] < EOMONTH(TODAY(), 0)))







Last Month Sales =
CALCULATE(
    SUM(Sales[SalesValue]),
    FILTER(
        ALL(Calendar),
        Calendar[Date] >= MIN(Calendar[Date]) &&
        Calendar[Date] <= MAX(Calendar[Date]) &&
        MONTH(Calendar[Date]) = MONTH(TODAY()) - 1 &&
        YEAR(Calendar[Date]) = YEAR(TODAY())
    )
)





Current and Last Month Value =
VAR CurrentMonth = MONTH(TODAY())
VAR CurrentYear = YEAR(TODAY())
VAR LastMonth = MONTH(TODAY())-1
VAR LastYear = YEAR(TODAY())

RETURN
SUMX(
    FILTER(
        CALCULATETABLE(PeopleData, CALENDAR2),
        MONTH(MIN(Calendar2[Date])) = CurrentMonth && YEAR(MIN(Calendar2[Date])) = CurrentYear
    ),
    PeopleData[Value]
)
+
SUMX(
    FILTER(
        CALCULATETABLE(PeopleData, CALENDAR2),
        MONTH(MIN(Calendar2[Date])) = LastMonth && YEAR(MIN(Calendar2[Date])) = LastYear
    ),
    PeopleData[Value]
)














Last Month Value =
SUMX(
    FILTER(
        CALCULATETABLE(PeopleData, CALENDAR2),
        MONTH(MIN(Calendar2[Date])) = MONTH(TODAY())-1 && YEAR(MIN(Calendar2[Date])) = YEAR(TODAY())
    ),
    PeopleData[Value]
)










Previous Value = 
CALCULATE(
    SUM(PeopleData[Value]),
    FILTER(
        ALL(PeopleData),
        PeopleData[Date] = MAX(PeopleData[Date]) - 1
    )
)







Latest Month Value =
CALCULATE(SUM(peopledata[value]),
FILTER(ALL(peopledata), peopledata[date] = MAX(peopledata[date])))

And here's the DAX measure for the "Previous Month Value":

Previous Month Value =
CALCULATE(SUM(peopledata[value]),
FILTER(ALL(peopledata), peopledata[date] =
MAX(peopledata[date]) - 1))





Previous Month Value =
CALCULATE(
SUM(peopledata[value]),
FILTER(
peopledata,
peopledata[date] >= EOMONTH(MAX(peopledata[date]), -1) &&
peopledata[date] < MAX(peopledata[date])
)
)

















Previous Month Value =
CALCULATE(
SUM(peopledata[value]),
FILTER(
peopledata,
peopledata[categorypeopledata] = SELECTEDVALUE(peopledata[categorypeopledata]) &&
peopledata[date] >= EOMONTH(MAX(peopledata[date]), -1) &&
peopledata[date] < MAX(peopledata[date])
)
)


Previous Month Value =
VAR CurrentCategory = SELECTEDVALUE(peopledata[categorypeopledata])
VAR CurrentDate = MAX(peopledata[date])
VAR PreviousMonth = EOMONTH(CurrentDate, -1)

RETURN
SUMX(
FILTER(peopledata, peopledata[categorypeopledata] = CurrentCategory && peopledata[date] >= PreviousMonth && peopledata[date] < CurrentDate),
peopledata[value]
)










Previous Month Value =
VAR CurrentCategory = SELECTEDVALUE(peopledata[categorypeopledata])
VAR CurrentDate = MAX(peopledata[date])
VAR PreviousMonth = EOMONTH(CurrentDate, -1)

RETURN
SUMX(
FILTER(peopledata, peopledata[categorypeopledata] = CurrentCategory && peopledata[date] >= PreviousMonth && peopledata[date] < CurrentDate),
peopledata[value]
)


















Previous Value Reformatted = 
    VAR CurrentColumn = SELECTEDVALUE(df[col1])
    VAR PreviousValue = [Your Measure Name]
RETURN 
    IF(CurrentColumn = "food name", PreviousValue/1000 & "k", 
    IF(CurrentColumn = "house name", PreviousValue/1000 & "k", 
    IF(CurrentColumn = "invoice name", PreviousValue, BLANK())))
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    Previous Value Reformatted = 
    VAR CurrentColumn = SELECTEDVALUE(df[col1])
    VAR PreviousValue = [Your Measure Name]
RETURN 
    IF(CurrentColumn = "food name", FORMAT(PreviousValue, "0,0") & "k", 
    IF(CurrentColumn = "house name", FORMAT(PreviousValue, "0,0") & "k", 
    IF(CurrentColumn = "invoice name", FORMAT(PreviousValue, "0.0%"), BLANK())))
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
from pyspark.sql.functions import when, col, concat_ws

sex_values = df1.select("Sex").distinct().rdd.flatMap(lambda x: x).collect()

df1 = df1.withColumn(
    "gender/sex",
    when(
        col("Sex").isin(sex_values) & col("Sex").isNotNull() & (col("Sex") != ""),
        col("Sex")
    ).otherwise(
        col("Gender")
    )
)

df1 = df1.withColumn(
    "gender/sex",
    when(
        col("gender/sex").isNull(),
        col("Gender")
    ).otherwise(
        col("gender/sex")
    )
)

df1 = df1.withColumn(
    "gender/sex",
    when(
        col("gender/sex").isNull(),
        "Null"
    ).otherwise(
        col("gender/sex")
    )
)

df1 = df1.withColumn(
    "gender/sex",
    concat_ws("/", col("gender/sex"), col("contact_id"))
)
    
    
    
    
    
    
    
    from pyspark.sql.functions import regexp_replace, col

# Create sample DataFrame
data = [("John", "Doe", "   "), ("Jane", "Doe", " "), ("Bob", "Smith", "   ")]
df = spark.createDataFrame(data, ["first_name", "last_name", "middle_name"])

# Replace invisible characters with "N/A"
df = df.withColumn("middle_name", regexp_replace(col("middle_name"), "[^\\p{Print}]", "N/A"))

# Show updated DataFrame
df.show()

    
from pyspark.sql.functions import ascii, length

# Display the ASCII code and length of each character in the "middle_name" column
df.select("middle_name", length("middle_name"), ascii("middle_name")).show(truncate=False)


from pyspark.sql.functions import regexp_replace

# Replace all whitespace characters with "N/A"
df = df.withColumn("middle_name", regexp_replace("middle_name", "\\s+", "N/A"))




import pyspark.sql.functions as F

# assume df is your Synapse DataFrame and col is the column you want to change
df = df.withColumn(col, F.when(F.col(col).isNull(), None).otherwise(F.col(col)))




import pyspark.sql.functions as F

# assume df is your Synapse DataFrame and col is the column you want to change
df = df.withColumn("book", F.when(F.col("book").isNull(), None).otherwise(F.col("book")))
























from pyspark.sql.functions import when, col, concat_ws

df1 = df1.withColumn(
    "gender/sex",
    when(
        col("Sex").isNotNull() & (col("Sex") != ""),
        col("Sex")
    ).otherwise(
        col("Gender")
    )
)

df1 = df1.withColumn(
    "gender/sex",
    when(
        col("gender/sex").isNull() & col("Gender").isNotNull(),
        col("Gender")
    ).otherwise(
        col("gender/sex")
    )
)

df1 = df1.withColumn(
    "gender/sex",
    when(
        col("gender/sex").isNull(),
        None
    ).otherwise(
        col("gender/sex")
    )
)

df1 = df1.withColumn(
    "gender/sex",
    concat_ws("/", col("gender/sex"), col("contact_id"))
)

https://stackoverflow.com/questions/72144956/how-to-get-the-keyvault-name-in-the-notebook-from-the-keyvault-link-in-synapse









from pyspark.sql.functions import udf, col, broadcast
from pyspark.sql.types import DoubleType
from difflib import SequenceMatcher

# Create a UDF to calculate the Levenshtein distance
def levenshtein_distance(x,y):
  if x is None or y is None:
    return None
  else:
    return SequenceMatcher(None, x, y).ratio()

levenshtein_udf = udf(levenshtein_distance, DoubleType())

from pyspark.sql.functions import coalesce

# Replace null values in columns "Primary_Address_City2" and "place20nm2" with empty strings
dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2", coalesce(col("Primary_Address_City2"), F.lit("")))
df2_postalcode = df2_postalcode.withColumn("place20nm2", coalesce(col("place20nm2"), F.lit("")))

# Create an index on the join column for faster performance
df2_postalcode.createOrReplaceTempView("df2_postalcode")
spark.sql("CREATE INDEX index_place20nm2 ON df2_postalcode (place20nm2)")

# Broadcast the smaller dataframe for faster performance
df2_postalcode = broadcast(df2_postalcode)

# Join the two dataframes on the fuzzy match
similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(df2_postalcode.alias("df2"), (levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")) > similarity_threshold))

# Select columns from both dataframes and calculate Levenshtein distance
match_data = df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.ctry20nm", levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")).alias("levenshtein_distance"))

# Cache the resulting DataFrame for faster performance in subsequent operations
match_data.cache()

# Show the first 500 rows of the DataFrame
match_data.show(500)







# Create a UDF to calculate the Levenshtein distance
def levenshtein_distance(x,y):
  if x is None or y is None:
    return None
  else:
    return SequenceMatcher(None, x, y).ratio()

levenshtein_udf = udf(levenshtein_distance, DoubleType())

from pyspark.sql.functions import coalesce

# Replace null values in columns "Primary_Address_City2" and "place20nm2" with empty strings
dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2", coalesce(col("Primary_Address_City2"), F.lit("")))
df2_postalcode = df2_postalcode.withColumn("place20nm2", coalesce(col("place20nm2"), F.lit("")))

# Enable Automatic Data Skew Optimization on the dataframes
spark.sql("ALTER TABLE dff_regrandcon SET TBLPROPERTIES ('AutoDataSkew'='True')")
spark.sql("ALTER TABLE df2_postalcode SET TBLPROPERTIES ('AutoDataSkew'='True')")

# Join the two dataframes on the fuzzy match
similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(broadcast(df2_postalcode.alias("df2")), (levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")) > similarity_threshold))

# Select columns from both dataframes and calculate Levenshtein distance
match_data = df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.ctry20nm", levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")).alias("levenshtein_distance"))






from pyspark.sql.functions import udf, col, broadcast
from pyspark.sql.types import DoubleType
from difflib import SequenceMatcher

# Create a UDF to calculate the Levenshtein distance
def levenshtein_distance(x,y):
  if x is None or y is None:
    return None
  else:
    return SequenceMatcher(None, x, y).ratio()

levenshtein_udf = udf(levenshtein_distance, DoubleType())

from pyspark.sql.functions import coalesce

# Replace null values in columns "Primary_Address_City2" and "place20nm2" with empty strings
dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2", coalesce(col("Primary_Address_City2"), F.lit("")))
df2_postalcode = df2_postalcode.withColumn("place20nm2", coalesce(col("place20nm2"), F.lit("")))

# Join the two dataframes on the fuzzy match
similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(broadcast(df2_postalcode.alias("df2")), (levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")) > similarity_threshold))

# Select columns from both dataframes and calculate Levenshtein distance
match_data = df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.ctry20nm", levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")).alias("levenshtein_distance"))






















from pyspark.sql.functions import udf, col, broadcast, soundex
from pyspark.sql.types import DoubleType

# Create a UDF to calculate the Levenshtein distance
def levenshtein_distance(x,y):
  if x is None or y is None:
    return None
  else:
    return SequenceMatcher(None, x, y).ratio()

levenshtein_udf = udf(levenshtein_distance, DoubleType())

# Create a UDF to calculate the soundex code
soundex_udf = udf(soundex)

# Replace null values in columns "Primary_Address_City2" and "place20nm2" with empty strings
dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2", coalesce(col("Primary_Address_City2"), F.lit("")))
df2_postalcode = df2_postalcode.withColumn("place20nm2", coalesce(col("place20nm2"), F.lit("")))

# Filter out any rows where either column is null
dff_regrandcon = dff_regrandcon.filter(col("Primary_Address_City2") != "")
df2_postalcode = df2_postalcode.filter(col("place20nm2") != "")

# Calculate the soundex code for both columns
dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2_soundex", soundex_udf(col("Primary_Address_City2")))
df2_postalcode = df2_postalcode.withColumn("place20nm2_soundex", soundex_udf(col("place20nm2")))

# Join the two dataframes on the fuzzy match using soundex code
similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(broadcast(df2_postalcode.alias("df2")), (soundex_udf(col("df1.Primary_Address_City2")) == soundex_udf(col("df2.place20nm2")))\
                  & (levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")) > similarity_threshold))

# Select columns from both dataframes and calculate Levenshtein distance
match_data = df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.ctry20nm", levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")).alias("levenshtein_distance"))


















































from pyspark.sql.functions import udf, col, broadcast
from pyspark.sql.types import DoubleType
from pyspark.ml.feature import Tokenizer
from pyspark.ml.feature import StopWordsRemover

# Create a UDF to calculate the Jaccard similarity
def jaccard_similarity(x,y):
  if x is None or y is None:
    return None
  else:
    x = set(x.split())
    y = set(y.split())
    return float(len(x & y))/float(len(x | y))

jaccard_udf = udf(jaccard_similarity, DoubleType())

# Tokenize and remove stop words from "Primary_Address_City2" and "place20nm2" columns
tokenizer = Tokenizer(inputCol="Primary_Address_City2", outputCol="tokens")
dff_regrandcon = tokenizer.transform(dff_regrandcon)
remover = StopWordsRemover(inputCol="tokens", outputCol="Primary_Address_City2_filtered")
dff_regrandcon = remover.transform(dff_regrandcon)

tokenizer = Tokenizer(inputCol="place20nm2", outputCol="tokens")
df2_postalcode = tokenizer.transform(df2_postalcode)
remover = StopWordsRemover(inputCol="tokens", outputCol="place20nm2_filtered")
df2_postalcode = remover.transform(df2_postalcode)

# Join the two dataframes on the fuzzy match
similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(broadcast(df2_postalcode.alias("df2")), (jaccard_udf(col("df1.Primary_Address_City2_filtered"), col("df2.place20nm2_filtered")) > similarity_threshold))

# Select columns from both dataframes and calculate Jaccard similarity
match_data = df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.ctry20nm", jaccard_udf(col("df1.Primary_Address_City2_filtered"), col("df2.place20nm2_filtered")).alias("jaccard_similarity"))

# Filter for rows with Jaccard similarity > 0.9
match_data_filtered = match_data.filter(col("jaccard_similarity") > 0.9)

# Show the resulting dataframe
match_data_filtered.show()


df2_postalcode = df2_postalcode.withColumn("place20nm2", coalesce(col("place20nm2"), F.lit("")).cast("string"))
df2_postalcode = df2_postalcode.withColumn("place20nm2_arr", split(col("place20nm2"), " "))




















from pyspark.sql.functions import udf
from pyspark.sql.types import DoubleType

# Create a UDF to calculate Jaccard similarity
def jaccard_similarity(x,y):
  if x is None or y is None:
    return None
  else:
    set_x = set(str(x).split())
    set_y = set(str(y).split())
    return float(len(set_x.intersection(set_y)) / len(set_x.union(set_y)))

jaccard_udf = udf(jaccard_similarity, DoubleType())

# Join the two dataframes on the fuzzy match
similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(broadcast(df2_postalcode.alias("df2")), (jaccard_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")) > similarity_threshold))

# Select columns from both dataframes and calculate Jaccard similarity
match_data = df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.ctry20nm", jaccard_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")).alias("jaccard_similarity"))

# Filter for rows with Jaccard similarity > threshold
match_data = match_data.filter(col("jaccard_similarity") > similarity_threshold)

# Show results
match_data.show()

















Switch Background Color = 
SWITCH(TRUE(),
    [Variable 1] > [Variable 2], 
        "Red",
    [Variable 1] < [Variable 2], 
        "Blue",
    "No color"
)


Background Text = 
IF([Switch Background Color] = "Red", "R",
    IF([Switch Background Color] = "Blue", "B", BLANK())
)














Switch Background Color =
SWITCH(TRUE(),
    NOT(ISBLANK([Variable 1])) && ISBLANK([Variable 2]), "Yellow",
    ISBLANK([Variable 1]) && NOT(ISBLANK([Variable 2])), "White",
    [Variable 1] > [Variable 2], "Red",
    [Variable 1] < [Variable 2], "Blue",
    "No color"
)




























# Import Libraries
from fuzzywuzzy import fuzz
from pyspark.sql.functions import col, udf, current_date
from pyspark.sql.types import StringType

# Read Files
df1_regrandcon = read("FUZZY.Roster")
df2_postalcode = read("FUZZY.Customer")

# Get Column Names in Data Frame
print(df1_regrandcon.columns)
print(df2_postalcode.columns)

# Create Temp Views
df1_regrandcon.createOrReplaceTempView("df1_regrandcon")
df2_postalcode.createOrReplaceTempView("df2_postalcode")

# Join
df = spark.sql("SELECT df1_regrandcon.registrant_id, df1_regrandcon.Work_PostalCode2, df1_regrandcon.Work_PostalCode, df2_postalcode.pcd, df2_postalcode.pcd2 FROM df2_postalcode CROSS JOIN df1_regrandcon ON df1_regrandcon.Work_PostalCode2 = df2_postalcode.pcd2")

# Apply Fuzzy Function
def matchstring(s1, s2):
    return fuzz.token_sort_ratio(s1, s2)

MatchUDF = udf(matchstring, StringType())

# Add columns and filter score more than 90
dvf = df.withColumn("similarity_score", MatchUDF(col("Work_PostalCode2"), col("pcd2"))).withColumn("run_date", current_date())
dvf = dvf.filter(dvf.similarity_score > 90)
dvf = dvf.drop("Address1")
dvf.show()

# Save Data Frame
dvf.write.save("output_folder", format="csv", header=True)


def matchstring(s1,s2):
    return float(fuzz.token_sort_ratio(s1,s2))/100

MatchUDF = udf(matchstring, FloatType())












from pyspark.sql.functions import col

df.select('a', 'b', 'c', 'd').where((col('a').isNotNull()) & (col('a') != '') & (col('b').isNotNull()) & (col('b') != ''))






from pyspark.sql.functions import col, regexp_extract, when

df = spark.read.csv('path/to/data.csv', header=True)

# Extract any letters from postcode column
postcode_letters = regexp_extract(col('postcode'), r'[a-zA-Z]+', 0)

# Replace city with blank if postcode contains only numbers
df = df.withColumn('city', when(postcode_letters == '', '', col('city')))


























from pyspark.sql.functions import input_file_name

# Set the path for the CSV files
path = '<your-path>'

# Load all CSV files into a single DataFrame
df = spark.read.csv(path + '/nspl21_aug_2022_uk*.csv', header=True)

# Add a column to the DataFrame to indicate the source file for each row
df = df.withColumn('filename', input_file_name())


































from pyspark.sql.functions import udf, col, round
from pyspark.sql.types import DoubleType, StringType
from difflib import SequenceMatcher

# Define the UDF for string comparison
compare_strings = udf(lambda x, y: SequenceMatcher(None, x, y).ratio() * 100, DoubleType())

# Set the similarity threshold as a decimal value
similarity_threshold = 0.9

# Join the dataframes and perform the string comparison
matched_dataJoin2 = df1_regrandconH.join(df2_postalcodeH, df1_regrandconH.Home_PostalCode == df2_postalcodeH.pcd2) \
    .select(df1_regrandconH.registrant_id, df1_regrandconH.Primary_Address_PostalCode, df1_regrandconH.Home_PostalCode,
            df2_postalcodeH.pcd, df2_postalcodeH.pcd2, compare_strings(df1_regrandconH.Home_PostalCode,
                                                                      df2_postalcodeH.pcd2).alias("similarity"))

# Round the similarity score to 2 decimal places and filter the results by the similarity threshold
matched_data2 = matched_dataJoin2.withColumn("similarity", round(col("similarity"), 2)) \
    .filter(col("similarity") > similarity_threshold)















from fuzzywuzzy import fuzz
from pyspark.sql.functions import udf, lower, levenshtein
from pyspark.sql.types import DoubleType, StringType
from pyspark.sql.functions import round

compare_strings = udf(lambda x, y: fuzz.token_sort_ratio(x, y), StringType())

similarity_threshold = 90.0

matched_data = df1_regrandconH.join(df2_postalcodeH, lower(df1_regrandconH.Home_PostalCode) == lower(df2_postalcodeH.pcd2)) \
    .withColumn("similarity", levenshtein(lower(df1_regrandconH.Home_PostalCode), lower(df2_postalcodeH.pcd2))) \
    .filter(compare_strings(lower(df1_regrandconH.Home_PostalCode), lower(df2_postalcodeH.pcd2)) > similarity_threshold) \
    .withColumn("similarity", round(col("similarity").cast(DoubleType()), 2)) \
    .select(df1_regrandconH.registrant_id, df1_regrandconH.Primary_Address_PostalCode, df1_regrandconH.Home_PostalCode, df2_postalcodeH.pcd, df2_postalcodeH.pcd2, col("similarity"))





https://www.data.gov.uk/dataset/7ec10db7-c8f4-4a40-8d82-8921935b4865/national-statistics-postcode-lookup-uk















Questions 1:  To determine those who do not Geocode to the UK, their postcode/address will have to be invalid as a UK geodata or not existing in the geodata (ONS) dictionary for us to identify this group.  
Question 2:  For the UK region, my suspension is this is the flat structure I have used to map the data which is also coming from ONS 2021. The one you shared IPN_GB_2021 does has limited regions so was not able to use it.   
Question 3: for this, I did not considered if they where in the UK, I can tweak this further by filtering nationality | country  to be in UK before doing the match










Days_Hours_Minutes = 
VAR Minutes = SUM('df1'[value])
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
RETURN
    SWITCH (
        TRUE (),
        Days >= 7, INT(Days / 7) & " weeks, " & MOD(Days, 7) & " days, ",
        Days > 1, Days & " days, ",
        Days = 1, "1 day, ",
        Days = 0, "",
        "",
    ) & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00")















Days_Hours_Minutes = 
VAR Minutes = SUM('df1'[value])
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
RETURN
    Days + Hours / 24 + Minutes_Remaining / 1440
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    Days_Hours_Minutes = 
VAR Minutes = SUM('df1'[value])
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
RETURN
    SWITCH (
        TRUE (),
        Days >= 7, INT(Days / 7) & " weeks, " & INT(MOD(Days, 7)) & " days, ",
        Days > 1, Days & " days, ",
        Days = 1, "1 day, ",
        Days = 0, "",
        TRUE (), ""
    ) & 
    IF(Hours > 0, FORMAT(Hours, "0") & " hours, ", "") &
    FORMAT(Minutes_Remaining, "0") & " minutes"

















Days_Hours_Minutes = 
VAR Minutes = SUM('df1'[value])
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
RETURN
    IF(
        Days >= 7,
        INT(Days / 7) & " weeks, " & INT(MOD(Days, 7)) & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
        IF(
            Days > 1,
            Days & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
            IF(
                Days = 1,
                "1 day, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
                FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00")
            )
        )
    )













Days_Hours_Minutes Label = 
VAR TotalMinutes = SUM('df1'[value])
VAR TotalHours = TotalMinutes / 60
VAR Days = INT(TotalHours / 24)
VAR Hours = INT(MOD(TotalHours, 24))
VAR Minutes = INT(MOD(TotalMinutes, 60))
RETURN
    Days & " days, " &
    FORMAT(Hours, "00") & " hours, " &
    FORMAT(Minutes, "00") & " minutes"







Days_Hours_Minutes = 
VAR Minutes = SUM('df1'[value])
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
RETURN
    TIME(Days * 24 + Hours, Minutes_Remaining, 0)




Days_Hours_Minutes_formatted = 
FORMAT(
    Days_Hours_Minutes, 
    IF(DAYS(Days_Hours_Minutes) > 0, "d.", "") & "hh:mm"
)




If you are getting an error that says "Cannot convert type text to type number", it's likely because you are trying to use a text-based DAX measure in a numerical context.

To fix this error, you can modify the DAX measure to return a numerical value instead of a text value. Here's the modified DAX measure:

Days_Hours_Minutes =
VAR Minutes = SUM('df1'[value])
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
RETURN
Days * 1440 + Hours * 60 + Minutes_Remaining

This DAX measure will return the total number of minutes as a numerical value. You can then use this measure in the field value in Power BI and format the result using the "Duration" format in the "Modeling" tab.

To display the result in days, hours, and minutes format, you can create a calculated column using the original DAX measure and format the result as a text value. Here's the modified DAX measure for the calculated column:

Days_Hours_Minutes_Text =
VAR Minutes = [Days_Hours_Minutes]
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
RETURN
IF(
Days >= 7,
INT(Days / 7) & " weeks, " & INT(MOD(Days, 7)) & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
IF(
Days > 1,
Days & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
IF(
Days = 1,
"1 day, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00")
)
)
)

This DAX measure will return a text value in the days, hours, and minutes format. You can use this calculated column in the field value in Power BI.



Days_Hours = 
VAR Minutes = SUM('df1'[value])
VAR Days = INT(Minutes / 1440)
VAR Hours = INT(MOD(Minutes, 1440) / 60)
RETURN
    Days & " days, " & FORMAT(Hours, "00") & " hours"
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
Days_Hours_Minutes = 
VAR Minutes = SUM('df1'[value])
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
RETURN
    IF(
        Days >= 7,
        INT(Days / 7) & " weeks, " & INT(MOD(Days, 7)) & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
        IF(
            Days > 1,
            Days & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
            IF(
                Days = 1,
                "1 day, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
                FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00")
            )
        )
    )
    
    
    
    
    
  Days_Hours_Minutes = 
VAR Minutes = SUM('df1'[value])
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
VAR Label = 
    IF(
        Days >= 7,
        INT(Days / 7) & " weeks, " & INT(MOD(Days, 7)) & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
        IF(
            Days > 1,
            Days & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
            IF(
                Days = 1,
                "1 day, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
                FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00")
            )
        )
    )
RETURN
    IF(HASONEVALUE('df1'[value]), Label, BLANK())
  
    











Minutes = SUM('df1'[value])
Create a measure that calculates the number of days, hours, and minutes in the format that you want:
less
Copy code
Days_Hours_Minutes = 
VAR Minutes = SUM('df1'[value])
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
RETURN
    IF(
        Days >= 7,
        INT(Days / 7) & " weeks, " & INT(MOD(Days, 7)) & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
        IF(
            Days > 1,
            Days & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
            IF(
                Days = 1,
                "1 day, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
                FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00")
            )
        )
    )
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    VAR Minutes = SELECTEDVALUE('df1'[value])
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
RETURN
    IF(
        Days >= 7,
        INT(Days / 7) & " weeks, " & INT(MOD(Days, 7)) & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
        IF(
            Days > 1,
            Days & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
            IF(
                Days = 1,
                "1 day, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
                FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00")
            )
        )
    )












Days_Hours_Minutes = 
VAR Minutes = [Your_Column_Name]
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
RETURN
    IF(
        Days >= 7,
        INT(Days / 7) & " weeks, " & INT(MOD(Days, 7)) & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
        IF(
            Days > 1,
            Days & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
            IF(
                Days = 1,
                "1 day, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
                FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00")
            )
        )
    )
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    Duration =
VAR Minutes = SUM('df1'[value])
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
RETURN
    IF(
        Days >= 7,
        INT(Days / 7) & " weeks, " & INT(MOD(Days, 7)) & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
        IF(
            Days > 1,
            Days & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
            IF(
                Days = 1,
                "1 day, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
                FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00")
            )
        )
    )










=CONCATENATEX(VALUES('df1'[value]), [Duration], ", ")































To display the result of the above DAX expression as a label in the value field of a Power BI visual, you can use a workaround by converting the text output of the measure into a numeric value.

Here's an example of how to do it:

Create a new measure in the Power BI data model and name it "Duration (in minutes)":

scss
Copy code
Duration (in minutes) = SUM('df1'[value])
This measure simply calculates the sum of the "value" field in the 'df1' table.

Create another new measure in the Power BI data model and name it "Duration (text)":

less
Copy code
Duration (text) =
VAR Minutes = [Duration (in minutes)]
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
RETURN
    IF(
        Days >= 7,
        INT(Days / 7) & " weeks, " & INT(MOD(Days, 7)) & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
        IF(
            Days > 1,
            Days & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
            IF(
                Days = 1,
                "1 day, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
                FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00")
            )
        )
    )
This measure uses the same DAX expression as before to calculate the duration in days, hours, and minutes, but returns the result as a text string.

Create a new visual on your report canvas and drag the "Duration (in minutes)" measure into the Values field.

Click on the "Duration (in minutes)" measure in the Values field and select "Don't summarize" from the dropdown list.

Create a new measure and name it "Duration (numeric)":

scss
Copy code
Duration (numeric) = VALUE([Duration (text)])
This measure converts the text output of the "Duration (text)" measure into a numeric value.

Drag the "Duration (numeric)" measure into the Values field of the visual.

Click on the "Duration (numeric)" measure in the Values field and select "Don't summarize" from the dropdown list.

In the Visualizations pane, select the visual type that you want to use to display the duration.

Add a new data label to the visual and select "Duration (text)" as the label field.

Adjust the formatting of the visual as desired.

Once you have completed these steps, the visual should display the duration as a label in the value field, while still allowing you to use a numeric measure for aggregation and other calculations.




Duration (numeric) =
IF(
    ISNUMBER(VALUE([Duration (text)])),
    VALUE([Duration (text)]),
    BLANK()
)










Duration (text) = 
VAR Minutes = SUM('df1'[value])
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
RETURN
    IF(
        Days >= 7,
        (Days + (INT(Days / 7) * 2)) & ":" & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
        IF(
            Days > 1,
            (Days * 1440 + Hours * 60 + Minutes_Remaining) & "",
            IF(
                Days = 1,
                (1440 + Hours * 60 + Minutes_Remaining) & "",
                (Hours * 60 + Minutes_Remaining) & ""
            )
        )
    )


Duration (numeric) = 
VAR DurationText = [Duration (text)]
RETURN
    VALUE(
        SUBSTITUTE(
            REPLACE(
                SUBSTITUTE(DurationText, ":", "."),
                ".",
                "",
                FIND(".", SUBSTITUTE(DurationText, ":", "."))
            ),
            ",",
            "."
        )
    )





Days_Hours_Minutes = 
VAR SelectedValue = SELECTEDVALUE('Table'[Column])
VAR Minutes = CALCULATE(SUM('df1'[value]), 'Table'[Column] = SelectedValue)
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
RETURN
    IF(
        Days >= 7,
        INT(Days / 7) & " weeks, " & INT(MOD(Days, 7)) & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
        IF(
            Days > 1,
            Days & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
            IF(
                Days = 1,
                "1 day, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
                FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00")
            )
        )
    )
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
Formatted Time =
VAR TotalMinutes = SUM(TechData2[value])
VAR Days = INT(TotalMinutes / 1440)
VAR Hours = INT((TotalMinutes - (Days * 1440)) / 60)
VAR Minutes = TotalMinutes - (Days * 1440) - (Hours * 60)
RETURN
IF(
    Days > 0,
    Days & " day, " & Hours & " hr, " & Minutes & " mins",
    IF(
        Hours > 0,
        Hours & " hr, " & Minutes & " mins",
        Minutes & " mins"
    )
)




Formatted Time =
VAR TotalMinutes = SUM(TechData2[value])
VAR Days = INT(TotalMinutes / 1440)
VAR Hours = INT((TotalMinutes - (Days * 1440)) / 60)
VAR Minutes = TotalMinutes - (Days * 1440) - (Hours * 60)
RETURN
VALUE(
    IF(
        Days > 0,
        Days & " day, " & Hours & " hr, " & Minutes & " mins",
        IF(
            Hours > 0,
            Hours & " hr, " & Minutes & " mins",
            Minutes & " mins"
        )
    )
)

[>1440]d" day, "h" hr, "m" mins";[>60]h" hr, "m" mins";m" mins"


























Formatted Time =
VAR TotalMinutes = [Total Minutes]
VAR Days = INT(TotalMinutes / 1440)
VAR Hours = INT(MOD(TotalMinutes, 1440) / 60)
VAR Minutes = MOD(TotalMinutes, 60)
VAR Result =
    SWITCH(
        TRUE(),
        Days > 0, Days & " day" & IF(Days > 1, "s", "") & ", " & Hours & " hr" & IF(Hours > 1, "s", "") & ", " & Minutes & " min" & IF(Minutes > 1, "s", ""),
        Hours > 0, Hours & " hr" & IF(Hours > 1, "s", "") & ", " & Minutes & " min" & IF(Minutes > 1, "s", ""),
        TRUE(), Minutes & " min" & IF(Minutes > 1, "s", "")
    )
RETURN Result


















Formatted Time =
VAR TotalMinutes = [Total Minutes]
VAR Days = INT(TotalMinutes / 1440)
VAR Hours = INT(MOD(TotalMinutes, 1440) / 60)
VAR Minutes = MOD(TotalMinutes, 60)
VAR Result =
    SWITCH(
        TRUE(),
        Days > 0, Days & " day" & IF(Days > 1, "s", "") & ", " & Hours & " hr" & IF(Hours > 1, "s", "") & ", " & Minutes & " min" & IF(Minutes > 1, "s", ""),
        Hours > 0, Hours & " hr" & IF(Hours > 1, "s", "") & ", " & Minutes & " min" & IF(Minutes > 1, "s", ""),
        TRUE(), Minutes & " min" & IF(Minutes > 1, "s", "")
    )
RETURN Result







Formatted Total = 
VAR TotalMinutes = TechData2[value]
VAR Days = INT(TotalMinutes / 1440)
VAR Hours = INT(MOD(TotalMinutes, 1440) / 60)
VAR Minutes = MOD(TotalMinutes, 60)
VAR Result =
    SWITCH(
        TRUE(),
        Days > 0, Days & " day" & IF(Days > 1, "s", "") & ", " & Hours & " hr" & IF(Hours > 1, "s", "") & ", " & Minutes & " min" & IF(Minutes > 1, "s", ""),
        Hours > 0, Hours & " hr" & IF(Hours > 1, "s", "") & ", " & Minutes & " min" & IF(Minutes > 1, "s", ""),
        TRUE(), Minutes & " min" & IF(Minutes > 1, "s", "")
    )
RETURN TotalMinutes & " mins, " & Result












Chelsie Eiden's Duration = 
VAR DurationInMins = SUM(TechData2[Value])
// There are 60 minutes in an hour
VAR Hours = INT(DurationInMins / 60)
// Remaining minutes are the remainder of the minutes divided by 60 after subtracting out the hours 
VAR Minutes = ROUNDUP(MOD(DurationInMins, 60), 0) // We round up here to get a whole number
RETURN
// We put the hours and minutes into the proper "place"
Hours * 100 + Minutes





Chelsie Eiden's Duration = 
VAR DurationInMins = SUM(TechData2[Value])
// There are 60 minutes in an hour
VAR TotalHours = INT(DurationInMins / 60)
// Remaining minutes are the remainder of the minutes divided by 60 after subtracting out the hours 
VAR RemainingMinutes = MOD(DurationInMins, 60)
// There are 24 hours in a day
VAR TotalDays = INT(TotalHours / 24)
// Remaining hours are the remainder of the hours divided by 24 after subtracting out the days
VAR RemainingHours = MOD(TotalHours, 24)
// Round up the remaining minutes to get a whole number
VAR RoundedMinutes = ROUNDUP(RemainingMinutes, 0)
RETURN
// We put the days, hours, and minutes into the proper "place"
TotalDays & "d " & RemainingHours & "h " & RoundedMinutes & "m"














Chelsie Eiden's Duration = 
VAR DurationInMins = SUM(TechData2[Value])
// There are 1440 minutes in a day (24 hours x 60 minutes)
VAR Days = INT(DurationInMins / 1440)
// There are 60 minutes in an hour
VAR Hours = INT(MOD(DurationInMins, 1440) / 60)
// Remaining minutes are the remainder of the minutes divided by 60 after subtracting out the hours 
VAR Minutes = ROUNDUP(MOD(DurationInMins, 60), 0) // We round up here to get a whole number
RETURN
// We put the days, hours, and minutes into the proper "place"
Days * 1440 + Hours * 60 + Minutes

[>=1440]d \d\a\y\s hh\:mm;[>=60]hh\:mm;mm


[>=1440]d\d\ h\h\:mm\m;[>=60]h\h\:mm\m;mm\m

FirstDateOfMonth = DATE(YEAR('Calendar'[Date]), MONTH('Calendar'[Date]), 1)





=IF(ISERROR(MATCH(A1, {"Country1","Country2","Country3","Country4"},0)),"No","Yes")
















from pyspark.sql.functions import when

# Create a dataframe with sample data
data = [("A", "gf"), ("B", "hj"), ("C", "tr"), ("D", "xyz")]
df = spark.createDataFrame(data, ["col1", "beter"])

# Define a mapping dictionary for recoding values
mapping = {"gf": "tg", "hj": "yu", "tr": "ts"}

# Use the 'when' function to create a new column 'new_beter' with recoded values
df = df.withColumn("new_beter", when(df.beter.isin(list(mapping.keys())), mapping[df.beter]).otherwise(df.beter))

# Show the final dataframe
df.show()













from pyspark.sql.functions import when

# Create a dataframe with sample data
data = [("A", "gf"), ("B", "hj"), ("C", "tr"), ("D", "xyz")]
df = spark.createDataFrame(data, ["col1", "beter"])

# Define a mapping dictionary for recoding values
mapping = {"gf": "tg", "hj": "yu", "tr": "ts"}

# Use the 'when' function to create a new column 'new_beter' with recoded values
df = df.withColumn("new_beter", when(df.beter.isin(list(mapping.keys())), mapping[df.beter]).otherwise(df.beter))

# Show the final dataframe
df.show()



from pyspark.sql.functions import when

# Create a dataframe with sample data
data = [("A", "gf"), ("B", "hj"), ("C", "tr"), ("D", "xyz")]
df = spark.createDataFrame(data, ["col1", "beter"])

# Use the 'when' function to create a new column 'new_beter' with recoded values
df = df.withColumn("new_beter", when(df.beter == "gf", "tg").when(df.beter == "hj", "yu").when(df.beter == "tr", "ts").otherwise(df.beter))

# Show the final dataframe
df.show()





from pyspark.sql.functions import when, col

# create example dataframe
df = spark.createDataFrame([(1, None, ''), (2, 'foo', 'bar'), (3, 'baz', None)], ['id', 'c', 'd'])

# add flag column
df_with_flag = df.withColumn('flag', when(col('c').isNull() | (col('c') == '') | col('d').isNull() | (col('d') == ''), 0).otherwise(1))

# show result
df_with_flag.show()






from pyspark.sql.functions import col, concat, sum

# Assuming your dataframe is called "df"
df_with_total = df.withColumn("total", concat(
    sum(col("a")).cast("string"),
    sum(col("b")).cast("string"),
    sum(col("c")).cast("string"),
    sum(col("d")).cast("string"),
    sum(col("e")).cast("string"),
    sum(col("f")).cast("string")
))

# Show the resulting dataframe
df_with_total.show()



from pyspark.sql.functions import col, expr, sum

# Define a list of column names to sum up
cols_to_sum = ["a", "b", "c", "d", "e", "f"]

# Sum up the columns in each row and create a new column called "total"
my_df_with_total = my_df.withColumn("total", sum(expr("+".join(cols_to_sum)).cast("int")).cast("string"))

# Show the resulting DataFrame
my_df_with_total.show()






from pyspark.sql.functions import col, expr, struct, sum

# Define a list of column names to sum up
cols_to_sum = ["a", "b", "c", "d", "e", "f"]

# Create a struct of all columns to sum up
cols_struct = struct(*[col(c).cast("int") for c in cols_to_sum])

# Sum up the columns in each row and create a new column called "total"
my_df_with_total = my_df.withColumn("total", sum(cols_struct).cast("string"))

# Show the resulting DataFrame
my_df_with_total.show()










from pyspark.sql.functions import col, sum

# Define a list of column names to sum up
cols_to_sum = ['Art_psychotherapist', 'Art_therapist', 'Clinical_psychologist', 
               'Counselling_psychologist', 'Diagnostic_radiographer', 'Drama_therapist',
               'Educational_psychologist', 'Health_psychologist', 'Forensic_psychologist',
               'Music_therapist', 'Occupational_psychologist', 'Orthotist', 'Prosthetist',
               'Sport_and_exercise_psychologist', 'Therapeutic_radiographer']

# Cast each column to an integer and sum them up
total_col = sum([col(c).cast('int') for c in cols_to_sum])

# Add the new column to the DataFrame
df1RegConmp1 = df1RegConmp1.withColumn('total', total_col.cast('string'))

# Show the resulting DataFrame
df1RegConmp1.show()









from pyspark.sql.functions import col, sum, try_cast

cols_to_sum = ['Art_psychotherapist', 'Art_therapist', 'Clinical_psychologist', 
               'Counselling_psychologist', 'Diagnostic_radiographer', 'Drama_therapist',
               'Educational_psychologist', 'Health_psychologist', 'Forensic_psychologist',
               'Music_therapist', 'Occupational_psychologist', 'Orthotist', 'Prosthetist',
               'Sport_and_exercise_psychologist', 'Therapeutic_radiographer']

total_col = sum([try_cast(col(c), 'integer') for c in cols_to_sum])

df1RegConmp1 = df1RegConmp1.withColumn('total', total_col.cast('string'))

df1RegConmp1.show()









amendfRegConmp = dfRegConmp.withColumn("Art_psychotherapist", when(dfRegConmp["Art_psychotherapist"].isNull(), lit(0)).otherwise(lit(1))) \
                           .withColumn("Art_therapist", when(dfRegConmp["Art_therapist"].isNull(), lit(0)).otherwise(lit(1))) \
                           .withColumn("Clinical_psychologist", when(dfRegConmp["Clinical_psychologist"].isNull(), lit(0)).otherwise(lit(1))) \
                           .withColumn("Counselling_psychologist", when(dfRegConmp["Counselling_psychologist"].isNull(), lit(0)).otherwise(lit(1))) \
                           .withColumn("Diagnostic_radiographer", when(dfRegConmp["Diagnostic_radiographer"].isNull(), lit(0)).otherwise(lit(1))) \
                           .withColumn("Drama_therapist", when(dfRegConmp["Drama_therapist"].isNull(), lit(0)).otherwise(lit(1))) \
                           .withColumn("Educational_psychologist", when(dfRegConmp["Educational_psychologist"].isNull(), lit(0)).otherwise(lit(1))) \
                           .withColumn("Forensic_psychologist", when(dfRegConmp["Forensic_psychologist"].isNull(), lit(0)).otherwise(lit(1))) \
                           .withColumn("Health_psychologist", when(dfRegConmp["Health_psychologist"].isNull(), lit(0)).otherwise(lit(1))) \
                           .withColumn("Music_therapist", when(dfRegConmp["Music_therapist"].isNull(), lit(0)).otherwise(lit(1))) \
                           .withColumn("Occupational_psychologist", when(dfRegConmp["Occupational_psychologist"].isNull(), lit(0)).otherwise(lit(1))) \
                           .withColumn("Orthotist", when(dfRegConmp["Orthotist"].isNull(), lit(0)).otherwise(lit(1))) \
                           .withColumn("Prosthetist", when(dfRegConmp["Prosthetist"].isNull(), lit(0)).otherwise(lit(1))) \
                           .withColumn("Sport_and_exercise_psychologist", when(dfRegConmp["Sport_and_exercise_psychologist"].isNull(), lit(0)).otherwise(lit(1))) \
                           .withColumn("Therapeutic_radiographer", when(dfRegConmp["Therapeutic_radiographer"].isNull(), lit(0)).otherwise(lit(1))) \
                           .drop('Registration_ID')
                           
                           
                           
                           
                           
                           
   https://digital.nhs.uk/services/organisation-data-service/export-data-files/csv-downloads/office-for-national-statistics-data          
                           
                           https://geoportal.statistics.gov.uk/datasets/national-statistics-postcode-lookup-2011-census-august-2022-1/about
                           
                           https://www.data.gov.uk/dataset/02c0321b-e9a4-4acd-97d9-d7e1914d57bb/national-statistics-postcode-lookup-camden

                           https://www.data.gov.uk/dataset/7ec10db7-c8f4-4a40-8d82-8921935b4865/national-statistics-postcode-lookup-uk
                           
                           
                           
                           
                           
                           
                           
                           
from pyspark.sql.functions import expr

cols_to_transpose = ['c', 'd', 'e']

stack_expr = ', '.join([f"stack(3, '{col}', {col})" for col in cols_to_transpose])
df = df.selectExpr("a", "b", expr(stack_expr).alias("cdet"))

df = df.selectExpr("a", "b", "cdet.col0 as c", "cdet.col1 as d", "cdet.col2 as e")







from pyspark.sql.functions import expr

cols_to_transpose = ['c', 'd', 'e']

stack_expr = ', '.join([f"stack(3, '{col}', {col})" for col in cols_to_transpose])
df = df.selectExpr("a", "b", f"explode(array({stack_expr})) as cdet")

df = df.selectExpr("a", "b", "cdet['0'] as c", "cdet['1'] as d", "cdet['2'] as e")




















from pyspark.sql.functions import posexplode

cols_to_transpose = ['c', 'd', 'e']

df = df.selectExpr("a", "b", f"posexplode(array({', '.join(cols_to_transpose)})) as (pos, cde)")

df = df.selectExpr("a", "b", "CASE pos WHEN 0 THEN cde ELSE NULL END as c", 
                   "CASE pos WHEN 1 THEN cde ELSE NULL END as d", 
                   "CASE pos WHEN 2 THEN cde ELSE NULL END as e")













from pyspark.sql.functions import concat_ws, array

cols_to_transpose = ['c', 'd', 'e']

df = df.selectExpr("a", "b", f"concat_ws(',', array({', '.join(cols_to_transpose)})) as cde")

df = df.selectExpr("a", "b", "split(cde, ',')[0] as c", 
                   "split(cde, ',')[1] as d", 
                   "split(cde, ',')[2] as e")














from pyspark.sql.functions import array, when, lit

cols_to_transpose = ['c', 'd', 'e']

df = df.selectExpr("a", "b", f"array({', '.join(cols_to_transpose)}) as cde")

for i, col in enumerate(cols_to_transpose):
    df = df.withColumn(f"modality_{i}", when(df.cde[i] == 1, col).otherwise(None))

modality_cols = [f"modality_{i}" for i in range(len(cols_to_transpose))]
df = df.selectExpr("a", "b", f"array({', '.join(modality_cols)}) as modality")

df = df.withColumn("modality_value", lit(cols_to_transpose)) \
       .select("a", "b", "modality", "modality_value")













from pyspark.sql.functions import concat_ws, array, expr

cols_to_transpose = ['c', 'd', 'e']

df = df.withColumn("cde", array(*[expr(f"IF({col}=1,'{col}',NULL)") for col in cols_to_transpose]))

modality_expr = f"filter(cde, x -> x is not NULL)"
df = df.select("a", "b", expr(modality_expr).alias("modality"))

df = df.withColumn("single", concat_ws(",", *[expr(f"coalesce({col},'')") for col in cols_to_transpose]))

df.show()




from pyspark.sql.functions import concat_ws, array, col, expr

cols_to_transpose = ['prostest', 'redio']

df = dftest.withColumn("cde", array(*[expr(f"IF({col}=1,'{col}',NULL)") for col in cols_to_transpose]))

modality_expr = f"filter(cde, x -> x is not NULL)"
df = df.select(col("createdbyname"), col("registrationname"), expr(modality_expr).alias("modality"))

df = df.withColumn("single", concat_ws(",", *[expr(f"coalesce(`{col}`, '')") for col in cols_to_transpose]))

df.show()














from pyspark.sql.functions import concat_ws, collect_list, struct

cols_to_transpose = ['prostest', 'redio']

df = df.groupBy("createdbyname", "registrationname").agg(
  collect_list(struct(*[col(col).alias('modality') for col in cols_to_transpose])).alias("cdelist")
).withColumn(
  "single",
  concat_ws(",",
    *[expr(f"IF(cde.modality['{col}'] IS NULL, '', cde.modality['{col}'])") for col in cols_to_transpose]
  )
).select("createdbyname", "registrationname", "single")

df.show()
























from pyspark.sql.functions import concat_ws, collect_list, struct, col

cols_to_transpose = ['prostest', 'redio']

df = df.groupBy("createdbyname", "registrationname").agg(
  collect_list(struct(*[col(col).alias('modality') for col in cols_to_transpose])).alias("cdelist")
).withColumn(
  "single",
  concat_ws(",",
    *[f"IF(cde.modality['{col}'] IS NULL, '', cde.modality['{col}'])" for col in cols_to_transpose]
  )
).select("createdbyname", "registrationname", "single")

df.show()


















from pyspark.sql.functions import array, col, concat, lit, split, when

# Create a DataFrame with sample data
dtest = spark.createDataFrame([
    ('User1', 'Patient1', 'Prosthetist1', 'Diagnostic_radiographer1',),
    ('User2', 'Patient2', 'Prosthetist2', 'Diagnostic_radiographer2',),
    ('User3', 'Patient3', 'Prosthetist3', 'Diagnostic_radiographer3',),
], ['createdbyname', 'Registration_Name', 'Prosthetist', 'Diagnostic_radiographer'])

# Transpose the 'Prosthetist' and 'Diagnostic_radiographer' columns into a single column 'modality'
dtest = dtest.select('createdbyname', 'Registration_Name',
                     concat(lit('Prosthetist'), lit('_'), col('Prosthetist')).alias('Prosthetist'),
                     concat(lit('Diagnostic_radiographer'), lit('_'), col('Diagnostic_radiographer')).alias('Diagnostic_radiographer')) \
             .select('createdbyname', 'Registration_Name', array('Prosthetist', 'Diagnostic_radiographer').alias('modality'))

# Explode the 'modality' array column to get separate rows for each modality
dtest = dtest.select('createdbyname', 'Registration_Name', explode('modality').alias('modality'))

# Split the 'modality' column into 'modality' and 'value' columns
dtest = dtest.select('createdbyname', 'Registration_Name', split('modality', '_')[0].alias('modality'),
                     when(split('modality', '_')[1] == 'true', 1).otherwise(0).alias('modality_value'))

# Show the final DataFrame
dtest.show()































from pyspark.sql.functions import array, col, concat, lit, split, when

# Create a DataFrame with sample data
dtest = spark.createDataFrame([
    ('User1', 'Patient1', True, False),
    ('User2', 'Patient2', False, True),
    ('User3', 'Patient3', True, True),
], ['createdbyname', 'Registration_Name', 'Prosthetist', 'Diagnostic_radiographer'])

# Transpose the 'Prosthetist' and 'Diagnostic_radiographer' columns into a single column 'modality'
dtest = dtest.select('createdbyname', 'Registration_Name',
                     concat(lit('Prosthetist'), lit('_'), col('Prosthetist')).alias('Prosthetist'),
                     concat(lit('Diagnostic_radiographer'), lit('_'), col('Diagnostic_radiographer')).alias('Diagnostic_radiographer')) \
             .select('createdbyname', 'Registration_Name', array('Prosthetist', 'Diagnostic_radiographer').alias('modality'))

# Explode the 'modality' array column to get separate rows for each modality
dtest = dtest.select('createdbyname', 'Registration_Name', explode('modality').alias('modality'))

# Split the 'modality' column into 'modality' and 'value' columns
dtest = dtest.select('createdbyname', 'Registration_Name', split('modality', '_')[0].alias('modality'),
                     when(split('modality', '_')[1] == 'true', 1).otherwise(0).alias('modality_value'))

# Group by 'createdbyname', 'Registration_Name', and 'modality', and sum the 'modality_value' column
dtest = dtest.groupBy('createdbyname', 'Registration_Name', 'modality') \
             .agg(sum('modality_value').alias('modality_value'))

# Show the final DataFrame
dtest.show()




















from pyspark.sql.functions import array, col, concat, lit, split, when

# Create a DataFrame with sample data
dtest = spark.createDataFrame([
    ('User1', 'Patient1', True, False),
    ('User2', 'Patient2', False, True),
    ('User3', 'Patient3', True, True),
], ['createdbyname', 'Registration_Name', 'Prosthetist', 'Diagnostic_radiographer'])

# Transpose the 'Prosthetist' and 'Diagnostic_radiographer' columns into a single column 'modality'
dtest = dtest.select('createdbyname', 'Registration_Name',
                     concat(lit('Prosthetist'), lit('_'), col('Prosthetist')).alias('Prosthetist'),
                     concat(lit('Diagnostic_radiographer'), lit('_'), col('Diagnostic_radiographer')).alias('Diagnostic_radiographer')) \
             .select('createdbyname', 'Registration_Name', array('Prosthetist', 'Diagnostic_radiographer').alias('modality'))

# Explode the 'modality' array column to get separate rows for each modality
dtest = dtest.select('createdbyname', 'Registration_Name', explode('modality').alias('modality'))

# Split the 'modality' column into 'modality' and 'value' columns
dtest = dtest.select('createdbyname', 'Registration_Name', split('modality', '_')[0].alias('modality'),
                     when(split('modality', '_')[1] == 'true', 1).otherwise(0).alias('modality_value'))

# Group by 'createdbyname', 'Registration_Name', and 'modality', and sum the 'modality_value' column
dtest = dtest.groupBy('createdbyname', 'Registration_Name', 'modality') \
             .agg({'modality_value': 'sum'}) \
             .withColumnRenamed('sum(modality_value)', 'modality_value')

# Show the final DataFrame
dtest.show()





























from pyspark.sql.functions import array, col, concat, lit, split, when

# Create a DataFrame with sample data
dtest = spark.createDataFrame([
    ('User1', 'Patient1', True, False),
    ('User2', 'Patient2', False, True),
    ('User3', 'Patient3', True, True),
], ['createdbyname', 'Registration_Name', 'Prosthetist', 'Diagnostic_radiographer'])

# Transpose the 'Prosthetist' and 'Diagnostic_radiographer' columns into a single column 'modality'
dtest = dtest.select('createdbyname', 'Registration_Name',
                     concat(lit('Prosthetist'), lit('_'), col('Prosthetist')).alias('Prosthetist'),
                     concat(lit('Diagnostic_radiographer'), lit('_'), col('Diagnostic_radiographer')).alias('Diagnostic_radiographer')) \
             .select('createdbyname', 'Registration_Name', array('Prosthetist', 'Diagnostic_radiographer').alias('modality'))

# Explode the 'modality' array column to get separate rows for each modality
dtest = dtest.select('createdbyname', 'Registration_Name', explode('modality').alias('modality'))

# Split the 'modality' column into 'modality' and 'value' columns
dtest = dtest.select('createdbyname', 'Registration_Name', split('modality', '_')[0].alias('modality'),
                     when(split('modality', '_')[1] == 'true', 1).otherwise(0).alias('modality_value'))
dtest.show()

# Group by 'createdbyname', 'Registration_Name', and 'modality', and sum the 'modality_value' column
dtest = dtest.groupBy('createdbyname', 'Registration_Name', 'modality') \
             .agg({'modality_value': 'sum'}) \
             .withColumnRenamed('sum(modality_value)', 'modality_value')
dtest.show()






















from pyspark.sql.functions import array, col, concat, lit, split, when

# Create a DataFrame with sample data
dtest = spark.createDataFrame([
    ('User1', 'Patient1', '1', '0'),
    ('User2', 'Patient2', '0', '1'),
    ('User3', 'Patient3', '1', '1'),
], ['createdbyname', 'Registration_Name', 'Prosthetist', 'Diagnostic_radiographer'])

# Transpose the 'Prosthetist' and 'Diagnostic_radiographer' columns into a single column 'modality'
dtest = dtest.select('createdbyname', 'Registration_Name',
                     concat(lit('Prosthetist'), lit('_'), col('Prosthetist')).alias('Prosthetist'),
                     concat(lit('Diagnostic_radiographer'), lit('_'), col('Diagnostic_radiographer')).alias('Diagnostic_radiographer')) \
             .select('createdbyname', 'Registration_Name', array('Prosthetist', 'Diagnostic_radiographer').alias('modality'))

# Explode the 'modality' array column to get separate rows for each modality
dtest = dtest.select('createdbyname', 'Registration_Name', explode('modality').alias('modality'))

# Split the 'modality' column into 'modality' and 'value' columns, and convert the binary values to integers
dtest = dtest.select('createdbyname', 'Registration_Name', split('modality', '_')[0].alias('modality'),
                     when(split('modality', '_')[1] == '1', 1).otherwise(0).alias('modality_value'))

# Group by 'createdbyname', 'Registration_Name', and 'modality', and sum the 'modality_value' column
dtest = dtest.groupBy('createdbyname', 'Registration_Name', 'modality') \
             .agg({'modality_value': 'sum'}) \
             .withColumnRenamed('sum(modality_value)', 'modality_value')
dtest.show()






























from pyspark.sql.functions import array, col, concat, lit, split, when,explode

modality_names = {
    'Art_psychotherapist': 'Art psychotherapist',
    'Art_therapist': 'Art therapist',
    'Clinical_psychologist': 'Clinical psychologist',
    'Counselling_psychologist': 'Counselling psychologist',
    'Diagnostic_radiographer': 'Diagnostic radiographer',
    'Drama_therapist': 'Drama therapist',
    'Educational_psychologist': 'Educational psychologist',
    'Forensic_psychologist': 'Forensic psychologist',
    'Health_psychologist': 'Health psychologist',
    'Music_therapist': 'Music therapist',
    'Occupational_psychologist': 'Occupational psychologist',
    'Orthotist': 'Orthotist',
    'Prosthetist': 'Prosthetist',
    'Sport_and_exercise_psychologist': 'Sport and exercise psychologist',
    'Therapeutic_radiographer': 'Therapeutic radiographer'
}

edimodalitytransposed = df8SurveyresponseFnl.select('Contact_ID', 'Registration_Name', *[
    concat(lit(name), lit('_'), col(name)).alias(name)
    for name in modality_names.keys()
])

edimodalitytransposed = edimodalitytransposed.select(
    'Contact_ID', 'Registration_Name', 
    array(*[concat(lit(name), lit('_'), col(name)) for name in modality_names.keys()]).alias('modality')
).select('Contact_ID', 'Registration_Name', explode('modality').alias('modality'))

edimodalitytransposed = edimodalitytransposed.select(
    'Contact_ID', 'Registration_Name', 
    split('modality', '_')[0].alias('modality'),
    when(split('modality', '_')[1] == '1', 1).otherwise(0).alias('modality_value')
)

edimodalitytransposed = edimodalitytransposed.replace(modality_names, subset=['modality'])

edimodalitytransposed = edimodalitytransposed.groupBy(
    'Contact_ID', 'Registration_Name', 'modality'
).agg({'modality_value': 'sum'}).withColumnRenamed('sum(modality_value)', 'modality_value')















































from pyspark.sql.functions import array, col, concat, explode, split, when

# Define a dictionary to map modality names to their full names
modality_dict = {
    "Art_psychotherapist": "Art Psychotherapist",
    "Art_therapist": "Art Therapist",
    "Clinical_psychologist": "Clinical Psychologist",
    "Counselling_psychologist": "Counselling Psychologist",
    "Diagnostic_radiographer": "Diagnostic Radiographer",
    "Drama_therapist": "Drama Therapist",
    "Educational_psychologist": "Educational Psychologist",
    "Forensic_psychologist": "Forensic Psychologist",
    "Health_psychologist": "Health Psychologist",
    "Music_therapist": "Music Therapist",
    "Occupational_psychologist": "Occupational Psychologist",
    "Orthotist": "Orthotist",
    "Prosthetist": "Prosthetist",
    "Sport_and_exercise_psychologist": "Sport and Exercise Psychologist",
    "Therapeutic_radiographer": "Therapeutic Radiographer"
}

# Select the columns of interest from the original DataFrame
edimodalitytransposed = df8SurveyresponseFnl.select(
    'Contact_ID', 'Registration_Name',
    'Art_psychotherapist', 'Art_therapist', 'Clinical_psychologist', 'Counselling_psychologist',
    'Diagnostic_radiographer', 'Drama_therapist', 'Educational_psychologist', 'Forensic_psychologist',
    'Health_psychologist', 'Music_therapist', 'Occupational_psychologist', 'Orthotist', 'Prosthetist',
    'Sport_and_exercise_psychologist', 'Therapeutic_radiographer'
)

# Reshape the DataFrame to have a 'modality' column with the full name of the modality
edimodalitytransposed = edimodalitytransposed.select(
    'Contact_ID', 'Registration_Name',
    array(*[
        concat(lit(full_name), lit('_'), col(name)).alias(name)
        for name, full_name in modality_dict.items()
    ]).alias('modalities')
)

# Explode the 'modalities' array column to get separate rows for each modality
edimodalitytransposed = edimodalitytransposed.select(
    'Contact_ID', 'Registration_Name',
    explode('modalities').alias('modality')
)

# Split the 'modality' column into 'modality_name' and 'modality_value' columns
edimodalitytransposed = edimodalitytransposed.select(
    'Contact_ID', 'Registration_Name',
    split('modality', '_')[0].alias('modality_name'),
    when(split('modality', '_')[1] == '1', 1).otherwise(0).alias('modality_value')
)

# Group by 'Contact_ID', 'Registration_Name', and 'modality_name', and sum the 'modality_value' column
edimodalitytransposed = edimodalitytransposed.groupBy(
    'Contact_ID', 'Registration_Name', 'modality_name'
).agg(
    {'modality_value': 'sum'}
).withColumnRenamed(
    'sum(modality_value)', 'modality_value'
)

df = spark.table("regdb_daily.ftp_Status")
df.write.mode("overwrite").option("path", "/path/to/location").saveAsTable("regdb_daily.ftp_Status")















rom pyspark.sql.functions import when, col

# create a sample dataframe
df = spark.createDataFrame([(1, None), (2, ''), (None, 'foo'), (4, 'bar')], ['a', 'b'])

# replace missing values with 'unknown'
df = df.withColumn('a', when(col('a').isNull(), 'unknown').otherwise(col('a')))
df = df.withColumn('b', when(col('b') == '', 'unknown').otherwise(col('b')))

df.show()



= Table.AddColumn(Applications, "Parent ID", each Table.SelectColumns(Table.NestedJoin(Applications,{"Contact ID"},Addresses,{"Parent ID"}),{"Addresses[Parent ID]"}){0}[Addresses[Parent ID]])




= Table.SelectColumns(Table.NestedJoin(Application,{"Contact ID"},Address,{"Parent ID"}),{"Parent ID"}){0}[Parent ID]






















OData.Feed("https://hcpcuk" & Environment & ".api.crm4.dynamics.com/api/data/v9.1/", null, [Implementation="2.0"]), hcpcreg_applications_table = Source{[Name="hcpcreg_applications",Signature="table"]}[Data], #"Removed Other Columns" =

Table.SelectColumns(hcpcreg_applications_table,{"hcpcreg_furtherverification", "hcpcreg_dateonapplication2", "hcpcreg_verificationcompleteddate2", "hcpcreg_applicantdob", "hcpcreg_datetoregisterifapplicantrequested2", "hcpcreg_qualificationstartdate", "_hcpcreg_institutionid_value", "_hcpcreg_profession_value", "hcpcreg_name", "hcpcreg_pendingreason", "hcpcreg_routetaken", "hcpcreg_inappeal", "hcpcreg_previousapplicationno", "hcpcreg_isrtprequired", "hcpcreg_practicedoutsideofuk", "hcpcreg_haveyoupreviouslyapplied", "_hcpcreg_professioncycle_value", "hcpcreg_datereceivedstamped2", "_hcpcreg_applicant_value", "statuscode", "hcpcreg_isenglishfirstlanguage", "hcpcreg_rejectedreason", "hcpcreg_declarationofinfochecked", "hcpcreg_applicationid", "hcpcreg_evidenceprovided", "_hcpcreg_registrationid_value", "_createdby_value", "hcpcreg_applicationtype", "hcpcreg_declarationforftp", "statecode", "hcpcreg_processingdate", "hcpcreg_acknowledgementdate", "hcpcreg_qualificationenddate", "createdon", "hcpcreg_scrutinyfee"}),
#"Renamed Columns" =
Table.RenameColumns(
#"Removed Other Columns", {

{"hcpcreg_applicationid" , "Application ID"}, {"_hcpcreg_applicant_value","Contact ID"}, {"_hcpcreg_profession_value","Profession ID"}, {"_hcpcreg_professioncycle_value","Professional Cycle ID"}, {"_hcpcreg_registrationid_value","Registration ID"}, {"_hcpcreg_institutionid_value","Institution ID"}, {"hcpcreg_applicationtype","Application Type ID"}, {"hcpcreg_acknowledgementdate","Acknowledgement Date"}, {"hcpcreg_applicantdob","Applicant DoB"},
// {"hcpcreg_approvedprogrammeid""Approved programme"},
{"hcpcreg_name","Application Name"}, {"hcpcreg_dateonapplication2","Date on Application"}, {"hcpcreg_datereceivedstamped2","Received Stamped Date"}, {"hcpcreg_datetoregisterifapplicantrequested2","Date to Register (Applicant Requested)"}, {"hcpcreg_declarationforftp","Declaration of FtP?"}, {"hcpcreg_declarationofinfochecked","Declaration of Info Signed?"}, {"hcpcreg_evidenceprovided","Evidence Provided?"}, {"hcpcreg_furtherverification","Further Verification Needed?"},

{"hcpcreg_haveyoupreviouslyapplied","Previously Applied?"}, {"hcpcreg_inappeal","In Appeal?"}, {"hcpcreg_isenglishfirstlanguage","English First Language"}, {"hcpcreg_isrtprequired","RtP Required?"}, {"hcpcreg_pendingreason","Pending Reason"}, {"hcpcreg_practicedoutsideofuk","Practiced Outside of UK?"}, {"hcpcreg_previousapplicationno","Previous Application Number"}, {"hcpcreg_processingdate","Processing Date"}, {"hcpcreg_qualificationenddate","Qualification End Date"}, {"hcpcreg_qualificationstartdate","Qualification Start Date"}, {"hcpcreg_rejectedreason","Rejected Reason ID"}, {"hcpcreg_scrutinyfee", "Scrutiny Fee"}, {"hcpcreg_routetaken","Route Taken ID"}, {"hcpcreg_verificationcompleteddate2","Verification Completed Date"}, {"createdon","Created Date"}, {"_createdby_value", "Created By"}, {"statecode","State ID"}, {"statuscode","Status ID"}

}

),

#"Added Custom" = Table.AddColumn(#"Renamed Columns", "New or Non-Graduate", each if

[Qualification End Date] = null or [Date on Application]= null or [Received Stamped Date] = null then "" else if [Qualification End Date] > Date.AddYears([Received Stamped Date],-2) then "New Graduate"

else "Non-Graduate"),
#"Reordered Columns" = Table.ReorderColumns(#"Added Custom",{
"Application ID", "Contact ID", "Profession ID",

"Professional Cycle ID", "Registration ID", "Institution ID", "Application Type ID", "Acknowledgement Date", "Applicant DoB",

// "Approved programme",
"Application Name",

"Date on Application", "Received Stamped Date", "Date to Register (Applicant Requested)", "Declaration of FtP?", "Declaration of Info Signed?", "Evidence Provided?", "Further Verification Needed?", "Previously Applied?", "In Appeal?", "English First Language", "RtP Required?", "Pending Reason", "Practiced Outside of UK?", "Previous Application Number", "Processing Date", "Qualification End Date", "Qualification Start Date", "Rejected Reason ID", "Scrutiny Fee", "Route Taken ID", "Verification Completed Date", "Created Date", "Created By", "State ID", "Status ID"

}),
#"Changed Type" = Table.TransformColumnTypes(#"Reordered Columns",{
{"Date on Application", type date}, {"Verification Completed Date", type date}, {"Applicant DoB", type date}, {"Created Date", type date}, {"Received Stamped Date", type date}, {"Qualification Start Date", type date}, {"Qualification End Date", type date}, {"Date to Register (Applicant Requested)", type date}}),
#"Split Column by Delimiter" = Table.SplitColumn(Table.TransformColumnTypes(#"Changed Type", {{"Acknowledgement Date", type text}}, "en-GB"), "Acknowledgement Date", Splitter.SplitTextByEachDelimiter({" "}, QuoteStyle.None, false), {"Acknowledgement Date.1", "Acknowledgement Date.2"}), #"Changed Type1" = Table.TransformColumnTypes(#"Split Column by Delimiter",{{"Acknowledgement Date.1", type date}, {"Acknowledgement Date.2", type text}}), #"Renamed Columns1" = Table.RenameColumns(#"Changed Type1",{{"Acknowledgement Date.2", "Acknowledgement Time"}}), #"Split Column by Delimiter1" = Table.SplitColumn(Table.TransformColumnTypes(#"Renamed Columns1", {{"Processing Date", type text}}, "en-GB"), "Processing Date", Splitter.SplitTextByEachDelimiter({" "}, QuoteStyle.None, false), {"Processing Date.1", "Processing Date.2"}), #"Changed Type2" = Table.TransformColumnTypes(#"Split Column by Delimiter1",{{"Processing Date.1", type date}, {"Processing Date.2", type time}}), #"Renamed Columns2" = Table.RenameColumns(#"Changed Type2",{

{"Processing Date.2", "Processing Time"}, {"Processing Date.1", "Processing Date"}, {"Acknowledgement Date.1", "Acknowledgement Date"} } ),
#"Added Custom1" = Table.AddColumn(#"Renamed Columns2", "Custom", each Table.SelectColumns(Table.NestedJoin(Applications,{"Contact ID"},Addresses,{"Parent ID"}),{"Parent ID"}){0}[Parent ID])

in
#"Added Custom1"



Table.SelectColumns(Table.NestedJoin([Applications],{"Contact ID"},[Addresses],{"Parent ID"}),{"Parent ID"}){0}[Parent ID]












Table.SelectColumns(Table.NestedJoin("Applications",{"Contact ID"},"Addresses",{"Parent ID"}),{"Parent ID"}){0}[Parent ID]


Table.SelectColumns(Table.NestedJoin([Applications],{"Contact ID"},[Addresses],{"Parent ID"}),{"Parent ID"}){0}[Parent ID]















from pyspark.sql.functions import rank, desc
from pyspark.sql.window import Window

# Assuming your DataFrame is called df and the date column is called 'date' and ID column is called 'id'
window_spec = Window.partitionBy('id').orderBy(desc('date'))
ranked_df = df.withColumn('rank', rank().over(window_spec))
latest_ids = ranked_df.filter('rank == 1').select('id')





















from pyspark.sql.functions import when, col

# Define the conditions for the new column
condition_uk = (col("workregon") == "unknown") & (col("country").isin(["Wales", "England", "United Kingdom", "Northern Ireland"]))
condition_overseas = ~(col("country").isin(["Wales", "England", "United Kingdom", "Northern Ireland"]))

# Apply the conditions to create the new column
df = df.withColumn("Groupcol", 
                   when(condition_uk, "UK")
                   .when(condition_overseas, "Overseas")
                   .otherwise(None)  # Replace with any default value if desired
                  )




from pyspark.sql.functions import when, col

# Define the conditions for the new column
condition_uk = (col("WorkCountry") == "unknown") & col("addressWorkCountry").isin(["Wales", "England", "United Kingdom", "Northern Ireland", "Scotland"])
condition_ukoverseas = (col("WorkCountry") == "unknown") & col("addressWorkCountry").isin(["Grand Cayman", "Falkland Islands", "Gibraltar", "British Virgin Islands", "Guernsey", "Jersey", "St Helena, South Atlantic Ocean", "Isle of Man", "Virgin Islands"])
condition_overseas = (col("WorkCountry") == "unknown") & ~(col("addressWorkCountry").isin(["Wales", "England", "United Kingdom", "Scotland", "Northern Ireland"]))

# Apply the conditions to create the new column
dftest = dfGeocodeFnlPip5.withColumn("UnknownGroup",
                                    when(condition_uk, "UK")
                                    .when(condition_ukoverseas, "UK_Overseas_Territory")
                                    .when(condition_overseas, "Overseas")
                                    .otherwise(None)  # Replace with any default value if desired
                                   )

condition_uk = (col("WorkCountry") == "unknown") & (col("addressWorkCountry").isin(["Wales", "England", "United Kingdom", "Northern Ireland", "Scotland"]) | col("addressWorkCountry").isin(["Wales", "England", "United Kingdom", "Northern Ireland", "Scotland"]))
condition_ukoverseas = (col("WorkCountry") == "unknown") & col("addressWorkCountry").isin(["Grand Cayman", "Falkland Islands", "Gibraltar", "British Virgin Islands", "Guernsey", "Jersey", "St Helena, South Atlantic Ocean", "Isle of Man", "Virgin Islands"])
condition_overseas = (col("WorkCountry") == "unknown") & ~(col("addressWorkCountry").isin(["Wales", "England", "United Kingdom", "Scotland", "Northern Ireland"]))




ambiguous_column = col("house")
print(ambiguous_column._jc.qualifiedName())

ambiguous_column = col("dmp.house")
print(ambiguous_column._jc.qualifiedName())





https://www.youtube.com/watch?v=T83OwV-d0AE











timetooltips =
VAR Minutes = SUM('TechData2'[Value])
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
VAR Rounded_Hours =
    IF(Minutes_Remaining >= 30, Hours + 1, Hours)
VAR Rounded_Days =
    IF(Rounded_Hours >= 24, Days + 1, Days)
VAR Result_Hours =
    IF(Minutes_Remaining >= 30, Rounded_Hours + 1, Rounded_Hours)
RETURN
    SWITCH(
        TRUE(),
        Rounded_Days >= 7,
            INT(Rounded_Days / 7) & "wks, " & INT(MOD(Rounded_Days, 7)) & "days,",
        Rounded_Days > 1,
            Rounded_Days & "days,",
        Rounded_Days = 1,
            "1day,",
        Rounded_Days = 0,
            "",
        TRUE(),
            ""
    )
    & IF(Result_Hours > 0, FORMAT(Result_Hours, "0") & "hrs", "")
    & IF(Minutes_Remaining < 30, FORMAT(Minutes_Remaining, "0") & "mins", "")
    & IF(Result_Hours > 0, IF(Minutes_Remaining >= 30, " (rounded)", ""), "")






















timetooltips =
VAR Minutes = SUM('TechData2'[Value])
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
VAR Rounded_Hours =
    IF(Minutes_Remaining >= 30, Hours + 1, Hours)
VAR Rounded_Days =
    IF(Rounded_Hours >= 24, Days + 1, Days)
RETURN
    IF(Minutes = 0, "Na",
        SWITCH(
            TRUE(),
            Rounded_Days >= 7,
                INT(Rounded_Days / 7) & "wks, " & INT(MOD(Rounded_Days, 7)) & "days,",
            Rounded_Days > 1,
                Rounded_Days & "days,",
            Rounded_Days = 1,
                "1day,",
            Rounded_Days = 0,
                "",
            TRUE(),
                ""
        )
        & IF(Rounded_Hours > 0, FORMAT(Rounded_Hours, "0") & "hrs", "")
    )





timetooltips =
VAR Minutes = SUM('TechData2'[Value])
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
VAR Rounded_Hours =
    IF(Minutes_Remaining >= 30, Hours + 1, Hours)
VAR Rounded_Days =
    IF(Rounded_Hours >= 24, Days + 1, Days)
RETURN
    IF(Minutes = 0, "Na",
        SWITCH(
            TRUE(),
            Rounded_Days >= 7,
                INT(Rounded_Days / 7) & "wks, " & INT(MOD(Rounded_Days, 7)) & "days",
            Rounded_Days > 1,
                Rounded_Days & "days",
            Rounded_Days = 1,
                "1 day",
            Rounded_Days = 0,
                "",
            TRUE(),
                ""
        )
        & IF(Rounded_Hours > 0, ", " & FORMAT(Rounded_Hours, "0") & "hrs", "")
    )
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    timetooltips =
VAR Minutes = SUM('TechData2'[Value])
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
VAR Rounded_Hours =
    IF(Minutes_Remaining >= 30, Hours + 1, Hours)
VAR Rounded_Days =
    IF(Rounded_Hours >= 24, Days + 1, Days)
RETURN
    IF(Minutes = 0, "Na",
        SWITCH(
            TRUE(),
            Minutes >= 1 && Minutes <= 60,
                FORMAT(Minutes, "0") & "mins",
            Rounded_Days >= 7,
                INT(Rounded_Days / 7) & "wks, " & INT(MOD(Rounded_Days, 7)) & "days",
            Rounded_Days > 1,
                Rounded_Days & "days",
            Rounded_Days = 1,
                "1 day",
            Rounded_Days = 0,
                "",
            TRUE(),
                ""
        )
        & IF(Rounded_Hours > 0 && (Minutes < 1 || Minutes > 60), ", " & FORMAT(Rounded_Hours, "0") & "hrs", "")
    )











SWITCH(
    TRUE(),
    OR(ISBLANK([Current Month Value]), [Current Month Value] = BLANK()), "White",
    OR(ISBLANK([Previous Month Value]), [Previous Month Value] = BLANK()), "White",
    [Current Month Value] > [Previous Month Value], "Green",
    [Current Month Value] < [Previous Month Value], "Red",
    [Current Month Value] = [Previous Month Value], "Green",
    "TrafficLowLight"
)

















PeoCurrent Value Reformatted =
VAR CurrentColumn = SELECTEDVALUE(PeopleData[People Operations])
VAR PreviousValue = [Current Month Value]
RETURN
IF(
    ISBLANK(PreviousValue),
    "N/A",
    IF(
        CurrentColumn = "Vacancy rate (%)",
        FORMAT(PreviousValue + 0, "") & "%",
        IF(
            CurrentColumn = "Voluntary turnover rate (%)",
            FORMAT(PreviousValue + 0, 0.0, "") & "%",
            IF(
                CurrentColumn = "Employee Satisfaction Score (pulse survey)",
                FORMAT(PreviousValue * 100, "") & "%",
                IF(
                    CurrentColumn = "FTC staff",
                    FORMAT(PreviousValue + 0, 0.0, ""),
                    IF(
                        CurrentColumn = "Internal offers made",
                        FORMAT(PreviousValue + 0, 0.0, ""),
                        IF(
                            CurrentColumn = "Number of campaigns",
                            FORMAT(PreviousValue + 0, 0.0, ""),
                            IF(
                                CurrentColumn = "Employee relations cases",
                                FORMAT(PreviousValue + 0, 0.0, ""),
                                IF(
                                    CurrentColumn = "Permanent staff",
                                    FORMAT(PreviousValue + 0, 0.0, ""),
                                    IF(
                                        CurrentColumn = "Agency staff",
                                        FORMAT(PreviousValue + 0, 0.0, ""),
                                        IF(
                                            CurrentColumn = "Average days to hire",
                                            FORMAT(PreviousValue + 0, ""),
                                            "N/A"
                                        )
                                    )
                                )
                            )
                        )
                    )
                )
            )
        )
    )
)



FinCurrents Value Reformatted =
VAR CurrentColumn = SELECTEDVALUE(FinanceData[Finance Operation])
VAR PreviousValue = [FinCurrent Month Value]
RETURN
IF(
    ISBLANK(PreviousValue),
    "N/A",
    IF(
        CurrentColumn = "Forecast surplus/defecit",
        FORMAT(PreviousValue / 1000, "") & "k",
        IF(
            CurrentColumn = "Procurement cost efficiencies",
            FORMAT(PreviousValue / 1000, "") & "k",
            IF(
                CurrentColumn = "Invoices paid on time",
                FORMAT(PreviousValue, "0%"),
                "N/A"
            )
        )
    )
)























EstCurrents Value Reformatted =
VAR CurrentColumn = SELECTEDVALUE(EstatesData[Estate Operation])
VAR PreviousValue = [EstCurrent Month Value]
RETURN
IF(
    ISBLANK(PreviousValue),
    "N/A",
    IF(
        CurrentColumn = "CO2 emissions",
        "N/A",
        IF(
            CurrentColumn = "Sustainability measure 2",
            "N/A",
            IF(
                CurrentColumn = "Sustainability measure 3",
                "N/A",
                IF(
                    CurrentColumn = "H+S incidents",
                    FORMAT(PreviousValue + 0, "") & "",
                    IF(
                        CurrentColumn = "DSE reimbursements (ytd)",
                        FORMAT(PreviousValue + 0, "") & "",
                        IF(
                            CurrentColumn = "DSE assessments (pcm)",
                            FORMAT(PreviousValue + 0, "") & "",
                            IF(
                                CurrentColumn = "Hearings utilisation",
                                FORMAT(PreviousValue * 100, "") & "%",
                                IF(
                                    CurrentColumn = "Desk utilisation",
                                    FORMAT(PreviousValue * 100, "") & "%",
                                    IF(
                                        CurrentColumn = "Office attendances",
                                        FORMAT(PreviousValue + 0, ""),
                                        "N/A"
                                    )
                                )
                            )
                        )
                    )
                )
            )
        )
    )
)





tech2previouse =
CALCULATE (
    IF (
        NOT ( ISBLANK ( [TIMEOUT] ) ),
        FILTER ( ALL ( 'CALENDER'[DATE] ), 'CALENDAR'[DATES] < MAX ( 'CALENDAR'[DATES] ) )
    ),
    BLANK ()
)


df.filter((~df["reg"].isin("1d", "1b")) & (df["deb"] == "1f"))
result = df.where(df.City == "New York").groupBy("City").count()
result = df.withColumn("a", when((col("a").isNull()) | (col("a") == ""), col("b")).otherwise(col("a")))
# Define the values to find and replace
values_to_find = ["Mike", "Sarah"]
replacement_value = "New Name"

# Find and replace values in the "Name" column using isin
result = df.withColumn("Name", when(col("Name").isin(values_to_find), replacement_value).otherwise(col("Name")))

# Show the result
result.show()











# Perform a left join and apply a where clause on two columns with cities not equal to New York, London, or Brussels
cities_to_exclude = ["New York", "London", "Brussels"]
result = df1.join(df2, (df1.Name == df2.Name) & (df1.Age < 40), "left").where(~df2.City.isin(cities_to_exclude))

# Show the result
result.show()
https://www.credly.com/badges/e6479bc4-5992-4f23-a899-47e0b36eb569/public_url


# Perform the required transformation
df = df.withColumn("colB", when((col("colB").isNull() | (col("colB") == "")) & (~col("colA").contains("#")), col("colA")).otherwise(col("colB")))

# Show the result
df.show()
This code creates a DataFrame df with two columns: colA and colB. It then uses the withColumn function to update colB based on the conditions you mentioned: if colB is null or blank (""), and colA does not contain the character #, it will move the value from colA to colB. Otherwise, the value in colB remains unchanged.

The resulting DataFrame will have the desired values in colB.


df = df.withColumn("colA", when(col("colA").contains("#"), col("colA")).otherwise(None))

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, split, array_distinct, concat_ws
# Perform the required transformation
df = df.withColumn("colA", concat_ws("#", array_distinct(split(col("colA"), "#"))))

# Show the result
df.show(truncate=False)








from pyspark.sql import SparkSession
from pyspark.sql.functions import col, split, array_distinct, concat_ws, sort_array

# Create a Spark session
spark = SparkSession.builder.getOrCreate()

# Create a sample DataFrame
data = [("house#house#rent", None),
        ("food#food", None),
        ("sky#sky#key#greate", None)]

df = spark.createDataFrame(data, ["colA", "colB"])

# Perform the required transformation
df = df.withColumn("colA", concat_ws("#", sort_array(array_distinct(split(col("colA"), "#")))))

# Show the result
df.show(truncate=False)


df = df.withColumn("colC", when((col("colA").isNotNull() & (col("colA") != "")), "Multi_Choice").otherwise(col("colC")))












df = df.withColumn("colC",
                   when((col("colA").isNotNull() & (col("colA") != "")), "Multi_Choice")
                   .when(col("colA").contains("I do not have a disability,long-term condition or important"), "I do not have a disability,long-term condition or important")
                   .when(col("colA").contains("group chance"), "group chance")
                   .otherwise(col("colC")))







df_cleaned = df.select(
    col("name"),
    when(trim(col("value")) == "", None).otherwise(col("value")).alias("value")
)



# Trim "invisible" blank or null values to proper null values for all columns
df_cleaned = df.select(
    *[
        when(trim(col(column)) == "", None).otherwise(col(column)).alias(column)
        for column in df.columns
    ]
)




from pyspark.sql.functions import when

df = df.withColumn("col2", when(df.col1 == "3", "bon").otherwise(df.col2))
df.show()




SELECT 
    eventime,
    CONVERT(VARCHAR(10), CONVERT(DATE, CONVERT(DATETIMEOFFSET, eventime) AT TIME ZONE 'UTC'), 111) AS date
FROM 
    your_table;



SELECT 
    eventime,
    CONVERT(VARCHAR(10), CONVERT(DATE, CONVERT(DATETIMEOFFSET, eventime) AT TIME ZONE 'UTC'), 111) AS date,
    DAY(CONVERT(DATE, CONVERT(DATETIMEOFFSET, eventime) AT TIME ZONE 'UTC')) AS day,
    DATEPART(WEEK, CONVERT(DATE, CONVERT(DATETIMEOFFSET, eventime) AT TIME ZONE 'UTC')) AS week,
    MONTH(CONVERT(DATE, CONVERT(DATETIMEOFFSET, eventime) AT TIME ZONE 'UTC')) AS month,
    DATEPART(QUARTER, CONVERT(DATE, CONVERT(DATETIMEOFFSET, eventime) AT TIME ZONE 'UTC')) AS quarter
FROM 
    your_table;










SELECT
    USERID,
    USERNAME,
    EVENTTIME,
    CONVERT(VARCHAR(10), CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC'), 111) AS DATE,
    DAY(CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) AS DAY,
    DATEPART(WEEK, CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) AS WEEK,
    MONTH(CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) AS MONTH,
    DATEPART(QUARTER, CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) AS QUARTER,
    CONVERT(TIME, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC') AS TIME,
    COUNT(*) OVER (PARTITION BY USERID, WEEK(CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC'))) AS OfficepresentCount
FROM
    OfficeAttendance;




SELECT
    USERID,
    USERNAME,
    EVENTTIME,
    CONVERT(VARCHAR(10), CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC'), 111) AS DATE,
    DAY(CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) AS DAY,
    DATEPART(WEEK, CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) AS WEEK,
    MONTH(CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) AS MONTH,
    DATEPART(QUARTER, CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) AS QUARTER,
    CONVERT(TIME, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC') AS TIME,
    COUNT(*) OVER (PARTITION BY USERID, DATEPART(WEEK, CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) ORDER BY EVENTTIME) AS OfficepresentCount
FROM
    OfficeAttendance;


















WITH DeduplicatedData AS (
    SELECT
        USERID,
        USERNAME,
        EVENTTIME,
        CONVERT(VARCHAR(10), CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC'), 111) AS DATE,
        DAY(CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) AS DAY,
        DATEPART(WEEK, CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) AS WEEK,
        MONTH(CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) AS MONTH,
        DATEPART(QUARTER, CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) AS QUARTER,
        CONVERT(TIME, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC') AS TIME,
        ROW_NUMBER() OVER (PARTITION BY USERID, DAY(CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC'))) AS RowNumber
    FROM
        OfficeAttendance
)
SELECT
    USERID,
    USERNAME,
    EVENTTIME,
    DATE,
    DAY,
    WEEK,
    MONTH,
    QUARTER,
    TIME,
    COUNT(*) OVER (PARTITION BY USERID, WEEK ORDER BY EVENTTIME) AS OfficepresentCount
FROM
    DeduplicatedData
WHERE
    RowNumber = 1;

        ROW_NUMBER() OVER (PARTITION BY USERID, DAY(CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) ORDER BY EVENTTIME) AS RowNumber















WITH DeduplicatedData AS (
    SELECT
        USERID,
        USERNAME,
        EVENTTIME,
        CONVERT(VARCHAR(10), CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC'), 111) AS DATE,
        DAY(CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) AS DAY,
        DATEPART(WEEK, CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) AS WEEK,
        MONTH(CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) AS MONTH,
        DATEPART(QUARTER, CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) AS QUARTER,
        CONVERT(TIME, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC') AS TIME,
        ROW_NUMBER() OVER (PARTITION BY USERID, CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC') ORDER BY EVENTTIME) AS RowNumber
    FROM
        OfficeAttendance
),
LastAttendance AS (
    SELECT
        USERID,
        EVENTTIME AS LastEventTime,
        LAG(EVENTTIME) OVER (PARTITION BY USERID ORDER BY EVENTTIME) AS PreviousEventTime
    FROM
        DeduplicatedData
)
SELECT
    DD.USERID,
    DD.USERNAME,
    DD.EVENTTIME,
    DD.DATE,
    DD.DAY,
    DD.WEEK,
    DD.MONTH,
    DD.QUARTER,
    DD.TIME,
    CASE WHEN L.LastEventTime IS NULL THEN 0 ELSE DATEDIFF(DAY, L.LastEventTime, DD.EVENTTIME) END AS DaysSinceLastAttendance,
    CASE WHEN DD.RowNumber = 1 THEN 1 ELSE 0 END AS OfficepresentCount
FROM
    DeduplicatedData AS DD
LEFT JOIN
    LastAttendance AS L ON DD.USERID = L.USERID AND DD.EVENTTIME = L.LastEventTime
WHERE
    DD.RowNumber = 1 AND
    CASE WHEN L.LastEventTime IS NULL THEN 0 ELSE DATEDIFF(DAY, L.LastEventTime, DD.EVENTTIME) END > 0;























WITH DeduplicatedData AS (
    SELECT
        USERID,
        USERNAME,
        EVENTTIME,
        CONVERT(VARCHAR(10), CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC'), 111) AS DATE,
        DAY(CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) AS DAY,
        DATEPART(WEEK, CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) AS WEEK,
        MONTH(CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) AS MONTH,
        DATEPART(QUARTER, CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) AS QUARTER,
        CONVERT(TIME, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC') AS TIME,
        ROW_NUMBER() OVER (PARTITION BY USERID, CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC') ORDER BY EVENTTIME) AS RowNumber
    FROM
        OfficeAttendance
)
SELECT *
FROM (
    SELECT
        USERID,
        USERNAME,
        EVENTTIME,
        DATE,
        DAY,
        WEEK,
        MONTH,
        QUARTER,
        TIME,
        CASE WHEN RowNumber = 1 THEN 1 ELSE 0 END AS OfficepresentCount,
        DATEDIFF(DAY, MAX(EVENTTIME) OVER (PARTITION BY USERID), EVENTTIME) AS DaysSinceLastAttendance
    FROM
        DeduplicatedData
    WHERE
        USERNAME = 'RAG'
) AS Subquery
WHERE
    OfficepresentCount = 1;


































WITH DeduplicatedData AS (
    SELECT
        USERID,
        USERNAME,
        EVENTTIME,
        CONVERT(VARCHAR(10), CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC'), 111) AS DATE,
        DAY(CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) AS DAY,
        DATEPART(WEEK, CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) AS WEEK,
        MONTH(CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) AS MONTH,
        DATEPART(QUARTER, CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) AS QUARTER,
        CONVERT(TIME, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC') AS TIME,
        ROW_NUMBER() OVER (PARTITION BY USERID, CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC') ORDER BY EVENTTIME) AS RowNumber
    FROM
        OfficeAttendance
)
SELECT *
FROM (
    SELECT
        USERID,
        USERNAME,
        EVENTTIME,
        DATE,
        DAY,
        WEEK,
        MONTH,
        QUARTER,
        TIME,
        CASE WHEN RowNumber = 1 THEN 1 ELSE 0 END AS OfficepresentCount,
        CASE WHEN MAX(EVENTTIME) OVER (PARTITION BY USERID) IS NULL THEN DATEDIFF(DAY, EVENTTIME, GETDATE()) 
             ELSE DATEDIFF(DAY, MAX(EVENTTIME) OVER (PARTITION BY USERID), GETDATE()) END AS DaysSinceLastAttendance
    FROM
        DeduplicatedData
    WHERE
        USERNAME = 'RAG'
) AS Subquery
WHERE
    OfficepresentCount = 1;










SELECT
    USERID,
    USERNAME,
    EVENTTIME,
    CONVERT(VARCHAR(10), CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC'), 111) AS DATE,
    DAY(CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) AS DAY,
    DATENAME(YEAR, CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) AS YEAR,
    DATENAME(MONTH, CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) AS MONTH,
    DATEPART(WEEK, CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) AS WEEK,
    DATEPART(QUARTER, CONVERT(DATE, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC')) AS QUARTER,
    CONVERT(TIME, CONVERT(DATETIMEOFFSET, EVENTTIME) AT TIME ZONE 'UTC') AS TIME

FROM
    OfficeAttendance











OfficePresentMondayToFriday = 
CALCULATE(
    COUNTROWS(Table1),
    FILTER(
        Table1,
        Table1[officepresent] = 1 &&
        WEEKDAY(Table1[datetime],2) <= 5
    )
)












DIVIDE(
    CALCULATE(
        COUNTROWS(Table1),
        FILTER(
            Table1,
            Table1[officepresent] = 1 &&
            WEEKDAY(Table1[datetime],2) <= 5
        )
    ),
    CALCULATE(
        COUNTROWS(Table1),
        FILTER(
            Table1,
            WEEKDAY(Table1[datetime],2) <= 5
        )
    )
) * 100























OfficePresentPercentage = 
DIVIDE(
    CALCULATE(
        COUNTROWS(
            DISTINCT(
                FILTER(
                    Table1,
                    Table1[officepresent] = 1 &&
                    WEEKDAY(Table1[datetime],2) <= 5
                )
            )
        ),
        DISTINCT(Table1[Emp Name])
    ),
    CALCULATE(
        COUNTROWS(
            DISTINCT(
                FILTER(
                    Table1,
                    WEEKDAY(Table1[datetime],2) <= 5
                )
            )
        ),
        DISTINCT(Table1[Emp Name])
    )
) * 100








Office Presence Percentage = 
DIVIDE(
    CALCULATE(
        COUNTROWS('Table 1'),
        'Table 1'[office present] = 1
    ),
    CALCULATE(
        COUNTROWS('Table 1'),
        'Table 1'[eventtime] >= MIN('Table 1'[eventtime]) &&
        'Table 1'[eventtime] <= MAX('Table 1'[eventtime]) &&
        WEEKDAY('Table 1'[eventtime], 2) <= 5
    )
)











Office Presence Percentage = 
DIVIDE(
    CALCULATE(
        SUM('Table 1'[office present]),
        FILTER(
            ALL('Table 1'),
            'Table 1'[eventtime] >= MIN('Table 1'[eventtime]) &&
            'Table 1'[eventtime] <= MAX('Table 1'[eventtime]) &&
            WEEKDAY('Table 1'[eventtime], 2) <= 5
        )
    ),
    CALCULATE(
        COUNTROWS('Table 1'),
        'Table 1'[eventtime] >= MIN('Table 1'[eventtime]) &&
        'Table 1'[eventtime] <= MAX('Table 1'[eventtime]) &&
        WEEKDAY('Table 1'[eventtime], 2) <= 5
    )
)




Office Presence Percentage = 
DIVIDE(
    COUNTROWS(
        FILTER(
            'Table 1',
            'Table 1'[office present] = 1 &&
            'Table 1'[eventtime] >= MIN('Table 1'[eventtime]) &&
            'Table 1'[eventtime] <= MAX('Table 1'[eventtime]) &&
            WEEKDAY('Table 1'[eventtime], 2) <= 5
        )
    ),
    COUNTROWS(
        FILTER(
            'Table 1',
            'Table 1'[eventtime] >= MIN('Table 1'[eventtime]) &&
            'Table 1'[eventtime] <= MAX('Table 1'[eventtime]) &&
            WEEKDAY('Table 1'[eventtime], 2) <= 5
        )
    )
)












Office Presence Percentage = 
DIVIDE(
    CALCULATE(
        COUNTROWS('Table 1'),
        'Table 1'[office present] = 1 &&
        'Table 1'[eventtime] >= MIN('Table 1'[eventtime]) &&
        'Table 1'[eventtime] <= MAX('Table 1'[eventtime]) &&
        WEEKDAY('Table 1'[eventtime], 2) <= 5
    ),
    CALCULATE(
        COUNTROWS('Table 1'),
        'Table 1'[eventtime] >= MIN('Table 1'[eventtime]) &&
        'Table 1'[eventtime] <= MAX('Table 1'[eventtime]) &&
        WEEKDAY('Table 1'[eventtime], 2) <= 5,
        ALLEXCEPT('Table 1', 'Table 1'[user id])
    )
)





PercentageOfTimeInOffice =
DIVIDE(
    CALCULATE(
        COUNTROWS(OfficeDoorReport),
        OfficeDoorReport[OfficePresentCount] = 1
    ),
    CALCULATE(
        COUNTROWS(OfficeDoorReport),
        OfficeDoorReport[OfficePresentCount] >= 0
    )
)



PercentageOfTimeInOffice =
DIVIDE(
    CALCULATE(
        COUNTROWS(OfficeDoorReport),
        OfficeDoorReport[OfficePresentCount] = 1
    ),
    CALCULATE(
        COUNTROWS(OfficeDoorReport),
        WEEKDAY(OfficeDoorReport[Date], 2) <= 5
    )
)







PercentageOfTimeInOffice = 
DIVIDE(
    CALCULATE(
        COUNTROWS(OfficeDoorReport),
        OfficeDoorReport[OfficePresentCount] = 1,
        WEEKDAY(OfficeDoorReport[Date], 2) <= 5
    ),
    CALCULATE(
        COUNTROWS(OfficeDoorReport),
        WEEKDAY(OfficeDoorReport[Date], 2) <= 5
    )
)















PercentageOfTimeInOffice =
DIVIDE(
    CALCULATE(
        COUNTROWS(OfficeDoorReport),
        OfficeDoorReport[OfficePresentCount] = 1
    ),
    COUNTROWS(
        SUMMARIZE(
            OfficeDoorReport,
            OfficeDoorReport[UserID],
            "WorkingDays", CALCULATE(
                COUNTROWS(OfficeDoorReport),
                WEEKDAY(OfficeDoorReport[Date], 2) <= 5
            )
        )
    )
)

PercentageOfTimeInOffice =
DIVIDE(
    CALCULATE(
        SUM(OfficeDoorReport[OfficePresentCount]),
        OfficeDoorReport[OfficePresentCount] = 1
    ),
    CALCULATE(
        COUNTROWS(OfficeDoorReport),
        WEEKDAY(OfficeDoorReport[Date], 2) <= 5
    )
)







TotalWorkingDays = CALCULATE(COUNTROWS(OfficeDoorReport), WEEKDAY(OfficeDoorReport[Date], 2) <= 5)

TotalOfficePresentCount = CALCULATE(SUM(OfficeDoorReport[OfficePresentCount]), OfficeDoorReport[OfficePresentCount] = 1)


PercentageOfTimeInOffice = DIVIDE([TotalOfficePresentCount], [TotalWorkingDays])






TotalWorkingDays = 
CALCULATE(
    COUNTROWS(OfficeDoorReport),
    WEEKDAY(OfficeDoorReport[Date], 2) <= 5,
    ALL(OfficeDoorReport[Date])
)



Percentage of Customers (Weekly) =
VAR TotalCustomers =
    CALCULATE(
        DISTINCTCOUNT(OfficeDoorReport[UserID]),
        WEEKDAY(OfficeDoorReport[Date to Event Time]) <= 5
    )
VAR PurchasedCustomers =
    CALCULATE(
        DISTINCTCOUNT(OfficeDoorReport[UserID]),
        OfficeDoorReport[Purchase] = 1,
        WEEKDAY(OfficeDoorReport[Date to Event Time]) <= 5
    )
RETURN
    IF(
        TotalCustomers = 0,
        BLANK(),
        IF(
            PurchasedCustomers = 0,
            0,
            DIVIDE(PurchasedCustomers, TotalCustomers) * 100
        )
    )




PercentageOfficeAttendance_Week = 
DIVIDE(
    SUM(OfficeDoorReport[officePresentCount]),
    COUNTROWS(OfficeDoorReport) * 7
) * 100



PercentageOfficeAttendance_Month = 
DIVIDE(
    SUM(OfficeDoorReport[officePresentCount]),
    COUNTROWS(OfficeDoorReport) * DAY(EOMONTH(MAX(OfficeDoorReport[EVENTIME]), 0))
) * 100









Title Text =
IF (
    ISFILTERED ( Drill1 ),
    "Drill 1: " & CONCATENATEX ( VALUES ( Drill1 ), Drill1, ", " ),
    ""
)
    & IF (
        ISFILTERED ( Drill2 ),
        " Drill 2: " & CONCATENATEX ( VALUES ( Drill2 ), Drill2, ", " ),
        ""
    )




Title Text =
IF (
    HASONEVALUE ( Drill1[Category] ),
    "Drill 1: " & VALUES ( Drill1[Category] ),
    ""
)
    & IF (
        HASONEVALUE ( Drill1[Dept] ),
        " Dept: " & VALUES ( Drill1[Dept] ),
        ""
    )
    & IF (
        HASONEVALUE ( Drill2[Category] ),
        " Drill 2: " & VALUES ( Drill2[Category] ),
        ""
    )
    & IF (
        HASONEVALUE ( Drill2[Month] ),
        " Month: " & VALUES ( Drill2[Month] ),
        ""
    )
    & IF (
        HASONEVALUE ( Drill2[Week] ),
        " Week: " & VALUES ( Drill2[Week] ),
        ""
    )












Title Text =
IF (
    ISFILTERED ( Drill1 ),
    IF (
        HASONEVALUE ( Drill1[Category] ),
        "Drill 1: " & VALUES ( Drill1[Category] ),
        ""
    )
    & IF (
        HASONEVALUE ( Drill1[Dept] ),
        " Dept: " & VALUES ( Drill1[Dept] ),
        ""
    ),
    CONCATENATEX ( VALUES ( Drill1[Category] ), Drill1[Category], ", " ) & " | " & CONCATENATEX ( VALUES ( Drill1[Dept] ), Drill1[Dept], ", " )
)



df = df.withColumn("cola1", when((col("col_a").isNull()) | (col("col_a") == ""), 1).otherwise(0))
(~col("age").between(20, 80)) | (col("age").isNull()), 1).otherwise(0))
~col("email").rlike(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'), 1).otherwise(0))



Last30Days = 
VAR TodayDate = TODAY()
VAR DaysDifference = DATEDIFF([Date], TodayDate, DAY)
RETURN IF(DaysDifference >= -30 && DaysDifference <= 0, "Last 30 Days", "Not in Last 30 Days")





Last60Days =
VAR TodayDate = TODAY()
VAR DaysDifference = DATEDIFF([Date], TodayDate, DAY)
RETURN IF(DaysDifference >= -60 && DaysDifference <= 0, "Last 60 Days", "Not in Last 60 Days")


~col("email").rlike(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$') |
                                 ~col("email").rlike(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{1,}\.[a-zA-Z]{2,}$'), 1).otherwise(0))

~col("email").rlike(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'), 1).otherwise(0))





~col("email").rlike(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$') |
                                 ~col("email").rlike(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{1,}\.[a-zA-Z]{2,}$'), 1).otherwise(0))







invalid_characters = '@\'":;\\/£$€¥'

# Add a new column 'cola1' and set it as 1 if 'col_a', 'col_b', or 'col_c' is null, blank, or doesn't contain any letter from A to Z
# Also, check the 'age' column to see if it's not between 20 and 80 (inclusive) or if it's null/missing
# Additionally, check the 'email' column to see if it contains any of the invalid characters or does not contain the '@' symbol
df = df.withColumn("cola1", when((col("col_a").isNull()) | (col("col_a") == "") | 
                                 (col("col_b").isNull()) | (col("col_b") == "") | 
                                 (col("col_c").isNull()) | (col("col_c") == "") |
                                 ~col("col_a").rlike("[A-Za-z]") |
                                 ~col("col_b").rlike("[A-Za-z]") |
                                 ~col("col_c").rlike("[A-Za-z]") |
                                 (~col("age").between(20, 80)) | (col("age").isNull()) |
                                 col("email").rlike(f"[{invalid_characters}]") |
                                 ~col("email").contains("@"), 1).otherwise(0))






email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'

# Add a new column 'cola1' and set it as 1 if 'col_a', 'col_b', or 'col_c' is null, blank, or doesn't contain any letter from A to Z
# Also, check the 'age' column to see if it's not between 20 and 80 (inclusive) or if it's null/missing
# Additionally, check the 'email' column to see if it matches the proper email address format
df = df.withColumn("cola1", when((col("col_a").isNull()) | (col("col_a") == "") | 
                                 (col("col_b").isNull()) | (col("col_b") == "") | 
                                 (col("col_c").isNull()) | (col("col_c") == "") |
                                 ~col("col_a").rlike("[A-Za-z]") |
                                 ~col("col_b").rlike("[A-Za-z]") |
                                 ~col("col_c").rlike("[A-Za-z]") |
                                 (~col("age").between(20, 80)) | (col("age").isNull()) |
                                 ~col("email").rlike(email_pattern), 1).otherwise(0))

# Show the resulting DataFrame
df.show()

# If you want to write the DataFrame to a new file, you can use the following line
# df.write.csv("path_to_output_file.csv", header=True, mode="overwrite")

# Don't forget to stop the SparkSession
spark.stop()
In this code, we define the email_pattern variable to hold the regular expression pattern to validate email addresses. The pattern matches the standard email address format: username@domain.tld, where username can contain letters, digits, and certain special characters, and domain.tld can contain letters, digits, dots, and hyphens. The top-level domain (TLD) part should have at least two letters, which satisfies the standard rule for most TLDs.

The code will now correctly flag the provided invalid email addresses as 1 in the 'cola1' column and valid email addresses as 0. The regular expression takes into account all the specified conditions for invalid email addresses.





ConditionalFormattingMeasure = 
IF(
    OR(
        MAX('YourTableName'[a]) = 1,
        MAX('YourTableName'[b]) = 1,
        MAX('YourTableName'[c]) = 1,
        MAX('YourTableName'[d]) = 1,
        MAX('YourTableName'[e]) = 1,
        MAX('YourTableName'[f]) = 1
    ),
    1,
    BLANK()
)


MeasureName = 
VAR ProfessionValue = SELECTEDVALUE('YourTableName'[profession])
VAR LevelValue1 = SELECTEDVALUE('YourTableName'[level col 1])
VAR LevelValue2 = SELECTEDVALUE('YourTableName'[level col 2])

RETURN
IF(
    (ProfessionValue = "teacher" && LevelValue1 = "senior") || (ProfessionValue = "teacher" && LevelValue2 = "junior") ||
    (ProfessionValue = "police" && LevelValue1 = "senior") || (ProfessionValue = "police" && LevelValue2 = "junior") ||
    (ProfessionValue = "army" && LevelValue1 = "senior"),
    1,
    0
)


emailpct =
IF(
    COUNTROWS(FILTER(REGADUIT, REGAUDIT[EMAIL] = 1)) = 0 || COUNTROWS(REGAUDIT) = 0,
    BLANK(),
    DIVIDE(
        COUNTROWS(FILTER(REGADUIT, REGAUDIT[EMAIL] = 1)),
        COUNTROWS(REGAUDIT)
    )
)








from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when

# Create a SparkSession
spark = SparkSession.builder.appName("Check and Flag").getOrCreate()

# Load your DataFrame (replace 'your_data_file.csv' with your actual file path or use 'spark.createDataFrame()' if you have a list or dictionary)
df = spark.read.csv("your_data_file.csv", header=True, inferSchema=True)

# Define the regular expression pattern to check for special characters in 'col_a'
special_char_pattern = r'[^\w\s]'

# Add a new column 'cola1' and set it as 1 if 'col_a', 'col_b', or 'col_c' is null, blank, or doesn't contain any letter from A to Z
# Also, check the 'age' column to see if it's not between 20 and 80 (inclusive) or if it's null/missing
# Additionally, check the 'email' column to see if it matches the proper email address format
df = df.withColumn("cola1", when((col("col_a").isNull()) | (col("col_a") == "") | 
                                 (col("col_b").isNull()) | (col("col_b") == "") | 
                                 (col("col_c").isNull()) | (col("col_c") == "") |
                                 ~col("col_a").rlike("[A-Za-z]") |
                                 ~col("col_b").rlike("[A-Za-z]") |
                                 ~col("col_c").rlike("[A-Za-z]") |
                                 (~col("age").between(20, 80)) | (col("age").isNull()) |
                                 ~col("email").rlike(email_pattern), 1).otherwise(0))

# Add a new column 'special_char_flag' and set it as 1 if 'col_a' contains special characters, otherwise set to 0
df = df.withColumn("special_char_flag", when(col("col_a").rlike(special_char_pattern), 1).otherwise(0))

# Show the resulting DataFrame
df.show()

# If you want to write the DataFrame to a new file, you can use the following line
# df.write.csv("path_to_output_file.csv", header=True, mode="overwrite")

# Don't forget to stop the SparkSession
spark.stop()







