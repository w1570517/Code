
## how to do filters
from pyspark.sql.functions import col
# join empDF ,deptData and addData
mainDF=empDF.join(deptDF,empDF.emp_dept_id==deptDF.dept_id).join(addDF,empDF.emp_id==addDF.emp_id)\
.select(empDF.emp_id.alias("emp_name"),addDF.emp_id,empDF.name,deptDF.dept_name,addDF.addline1,addDF.city)\
.filter( (addDF.state  == "CA") & (addDF.city  == "SFO") ) 
mainDF.show()

print(pivoteentitlementdf.select(pivoteentitlementdf["entitlement_id"]).distinct().count())
##
from pyspark.sql.functions import distinct

distinct number and count 
## print count of unique pivoteentitlementdf values and show the table result aswell 
pivoteentitlementdf.select(pivoteentitlementdf["INDEPENT PRES"]).distinct().count()




if len(mainDF.columns)>2 and len(mainDF.columns)<5:
    print("good")
else:
    print("data frame lenght is bad")
if mainDF.select(mainDF.emp_id).distinct().count()==1:
    print("good")
elif mainDF.select(mainDF.emp_id).distinct().count()>1:
    print("bad")
elif mainDF.select(mainDF.emp_id).distinct().count()<1:
    print("bad")
if mainDF.select(mainDF.dept_name).distinct().count()==1:
    print("good")
elif mainDF.select(mainDF.dept_name).distinct().count()>1:
    print("bad")
elif mainDF.select(mainDF.dept_name).distinct().count()<1:
    print("bad")
    
    
    
    columns_name=mainDF1.columns

if columns_name == addDF2.columns:
    print("Columns matched")
else:
    print("Columns not matched")
    print("Columns not matched are:")
    for i in columns_name:
        if i not in addDF2.columns:  # if i not in mainDF1.columns then print i else print nothing 
            print(i)


    
    
    
    
    
    # combine two backslash and code comment\
mainDF=(empDF.join( deptDF,empDF.emp_dept_id==deptDF.dept_id).join(addDF,empDF.emp_id==addDF.emp_id) # this is to join empDF ,deptData and addData
.select(empDF.emp_id.alias("emp_name"),addDF.emp_id,empDF.name,deptDF.dept_name,addDF.addline1,addDF.city) # this is to select the columns
.filter( (addDF.state  == "CA") & (addDF.city  == "SFO"))                                       # this is to filter the data
)
mainDF.show()





from pyspark.sql.types import StructType, IntegerType, StringType
## transform the use indexer to encode the alphanumarical column guid_column
from pyspark.ml.feature import StringIndexer
indexer = StringIndexer(inputCol="guid_column", outputCol="indexed")
indexerModel = indexer.fit(empDF)
indexed = indexerModel.transform(empDF)
#indexed.show()


# change indexed column from decimal to integer column type
#indexed.select(col("indexed").cast(IntegerType())).show()

# change indexed column from decimal to integer column type and select all columns
indexed.select("*",col("indexed").cast(IntegerType())).show()




https://towardsdatascience.com/adding-sequential-ids-to-a-spark-dataframe-fa0df5566ff6

https://stackoverflow.com/questions/45035940/how-to-pivot-on-multiple-columns-in-spark-sql

azure cost link
https://docs.microsoft.com/en-us/azure/cost-management-billing/costs/tutorial-export-acm-data?tabs=azure-portal




empDF.select("*").where(col("DATE") == empDF.agg({"DATE":"max"}).first()["max(DATE)"]).show()


solving ambiguity 
empDF.alias("a").join(addDF.alias("b"),empDF.emp_id==addDF.emp_id).select(col('a.emp_id').alias('1'),'b.*').show()


# write a function to sort the columns by alphabetical order
def sortColumns(df):
    return df.select([col(c).alias(c) for c in sorted(df.columns)])
comboDF.show()
sortColumns(comboDF).show()

# test the function
comboDF.select(sortColumns(comboDF)).show()

