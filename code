from pyspark.sql.functions import udf, col, broadcast
from pyspark.sql.types import DoubleType
from difflib import SequenceMatcher

# Create a UDF to calculate the Levenshtein distance
def levenshtein_distance(x,y):
  if x is None or y is None:
    return None
  else:
    return SequenceMatcher(None, x, y).ratio()

levenshtein_udf = udf(levenshtein_distance, DoubleType())

from pyspark.sql.functions import coalesce

# Replace null values in columns "Primary_Address_City2" and "place20nm2" with empty strings
dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2", coalesce(col("Primary_Address_City2"), F.lit("")))
df2_postalcode = df2_postalcode.withColumn("place20nm2", coalesce(col("place20nm2"), F.lit("")))

# Join the two dataframes on the fuzzy match
similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(broadcast(df2_postalcode.alias("df2")), (levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")) > similarity_threshold))

# Select columns from both dataframes and calculate Levenshtein distance
match_data = df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.ctry20nm", levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")).alias("levenshtein_distance"))




WITH subquery AS (
  SELECT studentid, studentcourse
  FROM student
  GROUP BY studentid, studentcourse
  HAVING COUNT(*) = 1
)
SELECT studentid, 
       STRING_AGG(studentcourse, ', ') WITHIN GROUP (ORDER BY studentcourse) AS courses
FROM subquery
GROUP BY studentid
HAVING COUNT(*) = 2;












WITH subquery AS (
  SELECT studentid, studentcourse
  FROM student
  GROUP BY studentid, studentcourse
  HAVING COUNT(*) = 1
)
SELECT studentid, 
       STRING_AGG(studentcourse, ', ') WITHIN GROUP (ORDER BY studentcourse) AS courses,
       CASE 
         WHEN COUNT(*) = 2 THEN 'Yes'
         ELSE 'No'
       END AS meeting_condition
FROM subquery
GROUP BY studentid;

















WITH subquery AS (
  SELECT studentid, studentcourse
  FROM student
  GROUP BY studentid, studentcourse
  HAVING COUNT(*) = 1
)
SELECT TOP 10 studentid, 
       STRING_AGG(studentcourse, ', ') WITHIN GROUP (ORDER BY studentcourse) AS courses,
       CASE 
         WHEN COUNT(*) = 2 THEN 'Yes'
         ELSE 'No'
       END AS meeting_condition
FROM subquery
GROUP BY studentid
ORDER BY COUNT(*) DESC;
















WITH subquery AS (
  SELECT studentid, studentcourse
  FROM student
  GROUP BY studentid, studentcourse
  HAVING COUNT(*) = 1
)
SELECT TOP 10 r.request_id, 
       STRING_AGG(s.studentcourse, ', ') WITHIN GROUP (ORDER BY s.studentcourse) AS courses,
       CASE 
         WHEN COUNT(*) = 2 THEN 'Yes'
         ELSE 'No'
       END AS meeting_condition
FROM registration r
LEFT JOIN (
  SELECT studentid, 
         studentcourse
  FROM subquery
  GROUP BY studentid
) s
ON r.studentid = s.studentid
GROUP BY r.request_id, r.studentid
ORDER BY COUNT(*) DESC;









WITH subquery AS (
  SELECT Registrant_ID, 
         Registrant_Name,
         CASE 
           WHEN COUNT(*) = 2 THEN 'Yes'
           ELSE 'No'
         END AS meeting_condition
  FROM (
    SELECT Registrant_ID, Registrant_Name
    FROM registration2
    GROUP BY Registrant_ID, Registrant_Name
    HAVING COUNT(*) = 1
  ) sub
  GROUP BY Registrant_ID
)
SELECT r.Registrant_ID,
       r.Registrant_Name,
       r.active,
       s.registrantdifference,
       s.meeting_condition
FROM registration r
LEFT JOIN (
  SELECT Registrant_ID, 
         STRING_AGG(Registrant_Name, ', ') WITHIN GROUP (ORDER BY Registrant_Name) AS registrantdifference,
         meeting_condition
  FROM subquery
) s
ON r.Registrant_ID = s.Registrant_ID;



WITH subquery AS (
  SELECT Registrant_ID, 
         STRING_AGG(Registration_Name, ', ') WITHIN GROUP (ORDER BY Registration_Name) AS registrantdifference,
         CASE 
           WHEN COUNT(DISTINCT Registration_Name) = 2 THEN 'Yes'
           ELSE 'No'
         END AS meeting_condition
  FROM (
    SELECT Registrant_ID, Registration_Name
    FROM Registration2
    GROUP BY Registrant_ID, Registration_Name
  ) sub
  GROUP BY Registrant_ID
)
SELECT r.Registrant_ID,
       r.Registrant_Name,
       r.active,
       s.registrantdifference,
       s.meeting_condition
FROM registration r
LEFT JOIN subquery s
ON r.Registrant_ID = s.Registrant_ID;






















































WITH subquery AS (
SELECT Registrant_ID,
STRING_AGG(Profession_Name, ', ') WITHIN GROUP (ORDER BY Profession_Name) AS registrantdifference,
CASE
WHEN COUNT(DISTINCT Profession_Name) = 2 THEN 'Yes'
ELSE 'No'
END AS meeting_condition,
CASE
WHEN SUM(CASE WHEN Regsitration_State_Description IN ('inactive', 'blank') THEN 1 ELSE 0 END) = 2 THEN 1
ELSE 0
END AS flag
FROM (
SELECT Registrant_ID,
Profession_Name,
Regsitration_State_Description
FROM [dbo].[registrations]
GROUP BY Registrant_ID,
Profession_Name,
Regsitration_State_Description
) sub
GROUP BY Registrant_ID
)

SELECT Optevia_Contact_ID AS Contact_ID ,
r.Registrant_ID,
r.[Regsitration_State_Description],
r.[Registration_Status_Description],
r.[Profession_Name],
r.[Date_Registered],
r.Registration_Name,
r.[First_Registration_Date] AS [First Registration Date],
r.[Date_Deregistered] AS [Date Deregistered],
r.[Date_Registered] AS [Date most recently registered],
r.[Date_Registered_From] AS [Date registered from],
r.[Date_Registered_To] AS [Date registered to],
s.registrantdifference,
s.meeting_condition AS [ProfessionChange(Yes_No)],
s.flag AS [Flag for Inactive and Blank]
FROM [dbo].[registrations] r
LEFT JOIN subquery s
ON r.Registrant_ID = s.Registrant_ID
Left join [dbo].[contacts] c
ON r.Registrant_ID = c.[Contact_ID]
GROUP BY Optevia_Contact_ID,
r.Registrant_ID,
r.Registration_Name,
r.[Regsitration_State_Description],
r.[Registration_Status_Description],
r.[Profession_Name],
r.[First_Registration_Date],
r.[Date_Registered],
r.[Date_Deregistered],
r.[Date_Registered_From],
r.[Date_Registered_To],
s.registrantdifference,
s.meeting_condition,
s.flag
































Hearingconcludedontime? =
IF(
ISBLANK(Raw_Data2[Panel Member Cancelled Date]),
"Unknown",
IF(
Raw_Data2[Panel Member Cancelled Date] > 0,
"Cancelled",
IF(
Raw_Data2[Hearing To] = Raw_Data2[Actual Hearing End Date],
"On time",
IF(
Raw_Data2[Hearing To] > Raw_Data2[Actual Hearing End Date],
"Early",
"Later"
)
)
)
)


= IF(Raw_Data2[Panel Member Cancelled Date] > 0, "Cancelled",
IF(Raw_Data2[Actual Hearing End Date]=Raw_Data2[Hearing To],"On time",
IF(Raw_Data2[Actual Hearing End Date] > Raw_Data2[Hearing To],"Early","Later")
)
)









Join the two dataframes on the fuzzy match
similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(df2_postalcode.alias("df2"), 
    (levenshtein_udf(
        col("df1.Primary_Address_City2").cast("string").alias("text1"),
        col("df2.place20nm2").cast("string").alias("text2")) > similarity_threshold))

Select columns from both dataframes
matched_data=df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Primary_Address_City2"
,"df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.place20nm2"
,"df2.eer20nm","df2.ctry20nm",levenshtein_udf(
    col("df1.Primary_Address_City2").cast("string"), 
    col("df2.place20nm2").cast("string")).alias("levenshtein_distance"))
    
    
    
    
    
    
    
    
    
    
 from diff_match_patch import diff_match_patch
from pyspark.sql.functions import udf, col, coalesce
from pyspark.sql import functions as F
from pyspark.sql.types import DoubleType

def levenshtein_distance(text1,text2):
    if text1 is None or text2 is None:
        return 0
    else:
        dmp = diff_match_patch()
        dmp.Diff_Timeout = 0.0
        diff = dmp.diff_main(text1, text2, False)
        common_text = sum([len(txt) for op, txt in diff if op == 0])
        text_length = max(len(text1), len(text2))
        sim = common_text / text_length
        return sim

levenshtein_udf = udf(levenshtein_distance, DoubleType())

# Make sure that the columns are of string type
dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2", coalesce(F.cast(col("Primary_Address_City2"), "string"), F.lit("")))
df2_postalcode = df2_postalcode.withColumn("place20nm2", coalesce(F.cast(col("place20nm2"), "string"), F.lit("")))

similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(df2_postalcode.alias("df2"), (levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")) > similarity_threshold))

matched_data=df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Primary_Address_City2",
"df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.place20nm2",
"df2.eer20nm","df2.ctry20nm",levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")).alias("levenshtein_distance"))






from diff_match_patch import diff_match_patch
from pyspark.sql.functions import coalesce, udf, lit, col, when
from pyspark.sql.types import DoubleType

def levenshtein_distance(text1,text2):
    if text1 is None or text2 is None:
        return 0
    else:
        dmp = diff_match_patch()
        dmp.Diff_Timeout = 0.0
        diff = dmp.diff_main(text1, text2, False)
        common_text = sum([len(txt) for op, txt in diff if op == 0])
        text_length = max(len(text1), len(text2))
        sim = common_text / text_length
        return sim

levenshtein_udf = udf(levenshtein_distance, DoubleType())

dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2", coalesce(col("Primary_Address_City2"), lit("")))
df2_postalcode = df2_postalcode.withColumn("place20nm2", coalesce(col("place20nm2"), lit("")))

similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(df2_postalcode.alias("df2"), when(levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")) > similarity_threshold, 1).otherwise(0) == 1)

matched_data = df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Primary_Address_City2",
                                "df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm",
                                "df2.eer20nm","df2.place20nm2","df2.eer20nm","df2.ctry20nm",
                                levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")).alias("levenshtein_distance"))
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                from diff_match_patch import diff_match_patch
from pyspark.sql.functions import udf, col, coalesce, when
from pyspark.sql.types import DoubleType, StringType

def levenshtein_distance(text1,text2):
    if text1 is None or text2 is None:
        return 0
    else:
        dmp = diff_match_patch()
        dmp.Diff_Timeout = 0.0
        diff = dmp.diff_main(text1, text2, False)
        common_text = sum([len(txt) for op, txt in diff if op == 0])
        text_length = max(len(text1), len(text2))
        sim = common_text / text_length
        return sim

levenshtein_udf = udf(levenshtein_distance, DoubleType())

# Cast the columns to string type
dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2", when(col("Primary_Address_City2").isNotNull(), col("Primary_Address_City2").cast(StringType())).otherwise(""))
df2_postalcode = df2_postalcode.withColumn("place20nm2", when(col("place20nm2").isNotNull(), col("place20nm2").cast(StringType())).otherwise(""))

# Join the two dataframes on the fuzzy match
similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(df2_postalcode.alias("df2"), (levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")) > similarity_threshold))

# Select columns from both dataframes
matched_data = df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Primary_Address_City2","df1.Work_PostalCode","df1.Work_PostalCode2",
                                "df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.place20nm2","df2.eer20nm","df2.ctry20nm",
                                levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")).alias("levenshtein_distance"))
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
import pyspark.sql.functions as F
from fuzzywuzzy import fuzz
from pyspark.sql.types import IntegerType

def matchstring(s1, s2):
    return fuzz.ratio(s1, s2)

MatchUDF = F.udf(matchstring, IntegerType())

similarity_threshold = 90
df_joined = dff_regrandcon.alias("df1").join(df2_postalcode.alias("df2"), (MatchUDF(F.col("df1.Primary_Address_City2"), F.col("df2.place20nm2")) > similarity_threshold))

df_result = df_joined.groupBy("df1.Primary_Address_City2").agg(F.max(MatchUDF(F.col("df1.Primary_Address_City2"), F.col("df2.place20nm2"))).alias("max_score")).filter(F.col("max_score") >= similarity_threshold)
df_result = df_result.select("df1.*").join(df_joined, (df_result["df1.Primary_Address_City2"] == df_joined["df1.Primary_Address_City2"]) & (df_result["max_score"] == MatchUDF(F.col("df1.Primary_Address_City2"), F.col("df2.place20nm2"))))
#









DJoined = dff_regrandcon.crossJoin(df2_postalcode)
matched_data = DJoined.withColumn("score", MatchUDF(F.col("Primary_Address_City2"), F.col("place20nm2")))
max_score_row = matched_data.agg(F.max("score").alias("max_score")).collect()[0]
selected_row = matched_data.filter(F.col("score") == max_score_row["max_score"]).collect()[0]



from pyspark.sql.functions import broadcast
DJoined = dff_regrandcon.crossJoin(broadcast(df2_postalcode))
matched_data = DJoined.withColumn("score", MatchUDF(F.col("Primary_Address_City2"), F.col("place20nm2")))
max_score_row = matched_data.agg(F.max("score").alias("max_score")).collect()[0]
selected_row = matched_data.filter(F.col("score") == max_score_row["max_score"]).collect()[0]

from pyspark.sql.functions import broadcast
DJoined = dff_regrandcon.crossJoin(broadcast(df2_postalcode))
matched_data = DJoined.withColumn("score", MatchUDF(F.col("Primary_Address_City2"), F.col("place20nm2")))
max_score_row = matched_data.agg(F.max("score").alias("max_score")).collect()[0]
selected_row = matched_data.filter(F.col("score") == max_score_row["max_score"]).collect()[0]




df2_postalcode = df.withColumn("place20nm2", regexp_replace(df.place20nm, "[^a-zA-Z0-9]+", ""))




from pyspark.sql.functions import trim, regexp_replace, lower

df2_postalcode = df
df2_postalcode = df2_postalcode.withColumn("place20nm2", regexp_replace(df2_postalcode.place20nm, "\s+", " "))
df2_postalcode = df2_postalcode.withColumn("place20nm2", regexp_replace(df2_postalcode.place20nm2, "[.']+", ""))
df2_postalcode = df2_postalcode.withColumn("place20nm2", regexp_replace(df2_postalcode.place20nm2, "[^a-zA-Z0-9]+", ""))
df2_postalcode = df2_postalcode.withColumn("place20nm2", trim(df2_postalcode.place20nm2))
df2_postalcode = df2_postalcode.withColumn("place20nm2", regexp_replace(df2_postalcode.place20nm2, "1", "one"))
df2_postalcode = df2_postalcode.withColumn("place20nm2", regexp_replace(df2_postalcode.place20nm2, "2", "two"))
df2_postalcode = df2_postalcode.withColumn("place20nm2", regexp_replace(df2_postalcode.place20nm2, "St", "Street"))
df2_postalcode = df2_postalcode.withColumn("place20nm2", lower(df2_postalcode.place20nm2))





from pyspark.sql.functions import col, expr

matched_data = dff_regrandcon.alias("xrt") \
                              .join(df2_postalcode.alias("xrtx"), 
                                    expr("xrt.Primary_Address_City like concat('%', xrtx.place20nm, '%')")) \
                              .select("xrt.*", "xrtx.*")



from pyspark.sql.functions import col, expr

matched_data = dff_regrandcon.alias("xrt") \
                              .join(df2_postalcode.alias("xrtx"), 
                                    expr("xrt.Primary_Address_City like concat(xrtx.place20nm, '_')")) \
                              .select("xrt.*", "xrtx.*")



SELECT *
FROM dff_regrandcon xrt
JOIN df2_postalcode xrtx
ON xrt.Primary_Address_City LIKE concat(xrtx.place20nm, '_')


















let
    Source = List.Dates(#date(2000, 1, 1), #date(2030, 12, 31)),
    #"Converted to Table" = Table.FromList(Source, Splitter.SplitByNothing(), {"Date"}),
    #"Expanded Date" = Table.ExpandColumn(#"Converted to Table", "Date", {"Year", "Quarter", "Month", "Week", "Day", "Weekday"}),
    #"Added Custom" = Table.AddColumn(#"Expanded Date", "Year Month", each [Year] * 100 + [Month]),
    #"Added Custom1" = Table.AddColumn(#"Added Custom", "Year Month Day", each [Year Month] * 100 + [Day]),
    #"Added Custom2" = Table.AddColumn(#"Added Custom1", "Month Name", each Date.MonthName([Month]), type text),
    #"Added Custom3" = Table.AddColumn(#"Added Custom2", "Quarter Name", each "Q" & Number.ToText([Quarter]), type text),
    #"Added Custom4" = Table.AddColumn(#"Added Custom3", "Weekday Name", each Date.DayOfWeekName([Weekday]), type text)
in
    #"Added Custom4"










https://exceleratorbi.com.au/conditional-formatting-using-icons-in-power-bi/

https://learn.microsoft.com/en-us/power-bi/visuals/power-bi-visualization-scorecard-visual

https://learn.microsoft.com/en-us/power-bi/visuals/power-bi-visualization-tables?tabs=powerbi-desktop


Current Month Sales =
CALCULATE(SUM(Sales[SalesValue]), DATESYTD(Calendar[Date]), Sales[Category]=EARLIER(Sales[Category]))

Last Month Sales =
CALCULATE(SUM(Sales[SalesValue]), DATESYTD(Calendar[Date])-1, Sales[Category]=EARLIER(Sales[Category]))




Last Month Sales =
CALCULATE(SUM(Sales[SalesValue]), FILTER(Sales, Sales[Date] >= EOMONTH(TODAY(), -1) && Sales[Date] < EOMONTH(TODAY(), 0)))







Last Month Sales =
CALCULATE(
    SUM(Sales[SalesValue]),
    FILTER(
        ALL(Calendar),
        Calendar[Date] >= MIN(Calendar[Date]) &&
        Calendar[Date] <= MAX(Calendar[Date]) &&
        MONTH(Calendar[Date]) = MONTH(TODAY()) - 1 &&
        YEAR(Calendar[Date]) = YEAR(TODAY())
    )
)





Current and Last Month Value =
VAR CurrentMonth = MONTH(TODAY())
VAR CurrentYear = YEAR(TODAY())
VAR LastMonth = MONTH(TODAY())-1
VAR LastYear = YEAR(TODAY())

RETURN
SUMX(
    FILTER(
        CALCULATETABLE(PeopleData, CALENDAR2),
        MONTH(MIN(Calendar2[Date])) = CurrentMonth && YEAR(MIN(Calendar2[Date])) = CurrentYear
    ),
    PeopleData[Value]
)
+
SUMX(
    FILTER(
        CALCULATETABLE(PeopleData, CALENDAR2),
        MONTH(MIN(Calendar2[Date])) = LastMonth && YEAR(MIN(Calendar2[Date])) = LastYear
    ),
    PeopleData[Value]
)














Last Month Value =
SUMX(
    FILTER(
        CALCULATETABLE(PeopleData, CALENDAR2),
        MONTH(MIN(Calendar2[Date])) = MONTH(TODAY())-1 && YEAR(MIN(Calendar2[Date])) = YEAR(TODAY())
    ),
    PeopleData[Value]
)










Previous Value = 
CALCULATE(
    SUM(PeopleData[Value]),
    FILTER(
        ALL(PeopleData),
        PeopleData[Date] = MAX(PeopleData[Date]) - 1
    )
)







Latest Month Value =
CALCULATE(SUM(peopledata[value]),
FILTER(ALL(peopledata), peopledata[date] = MAX(peopledata[date])))

And here's the DAX measure for the "Previous Month Value":

Previous Month Value =
CALCULATE(SUM(peopledata[value]),
FILTER(ALL(peopledata), peopledata[date] =
MAX(peopledata[date]) - 1))





Previous Month Value =
CALCULATE(
SUM(peopledata[value]),
FILTER(
peopledata,
peopledata[date] >= EOMONTH(MAX(peopledata[date]), -1) &&
peopledata[date] < MAX(peopledata[date])
)
)

















Previous Month Value =
CALCULATE(
SUM(peopledata[value]),
FILTER(
peopledata,
peopledata[categorypeopledata] = SELECTEDVALUE(peopledata[categorypeopledata]) &&
peopledata[date] >= EOMONTH(MAX(peopledata[date]), -1) &&
peopledata[date] < MAX(peopledata[date])
)
)


Previous Month Value =
VAR CurrentCategory = SELECTEDVALUE(peopledata[categorypeopledata])
VAR CurrentDate = MAX(peopledata[date])
VAR PreviousMonth = EOMONTH(CurrentDate, -1)

RETURN
SUMX(
FILTER(peopledata, peopledata[categorypeopledata] = CurrentCategory && peopledata[date] >= PreviousMonth && peopledata[date] < CurrentDate),
peopledata[value]
)










Previous Month Value =
VAR CurrentCategory = SELECTEDVALUE(peopledata[categorypeopledata])
VAR CurrentDate = MAX(peopledata[date])
VAR PreviousMonth = EOMONTH(CurrentDate, -1)

RETURN
SUMX(
FILTER(peopledata, peopledata[categorypeopledata] = CurrentCategory && peopledata[date] >= PreviousMonth && peopledata[date] < CurrentDate),
peopledata[value]
)


















Previous Value Reformatted = 
    VAR CurrentColumn = SELECTEDVALUE(df[col1])
    VAR PreviousValue = [Your Measure Name]
RETURN 
    IF(CurrentColumn = "food name", PreviousValue/1000 & "k", 
    IF(CurrentColumn = "house name", PreviousValue/1000 & "k", 
    IF(CurrentColumn = "invoice name", PreviousValue, BLANK())))
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    Previous Value Reformatted = 
    VAR CurrentColumn = SELECTEDVALUE(df[col1])
    VAR PreviousValue = [Your Measure Name]
RETURN 
    IF(CurrentColumn = "food name", FORMAT(PreviousValue, "0,0") & "k", 
    IF(CurrentColumn = "house name", FORMAT(PreviousValue, "0,0") & "k", 
    IF(CurrentColumn = "invoice name", FORMAT(PreviousValue, "0.0%"), BLANK())))
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
from pyspark.sql.functions import when, col, concat_ws

sex_values = df1.select("Sex").distinct().rdd.flatMap(lambda x: x).collect()

df1 = df1.withColumn(
    "gender/sex",
    when(
        col("Sex").isin(sex_values) & col("Sex").isNotNull() & (col("Sex") != ""),
        col("Sex")
    ).otherwise(
        col("Gender")
    )
)

df1 = df1.withColumn(
    "gender/sex",
    when(
        col("gender/sex").isNull(),
        col("Gender")
    ).otherwise(
        col("gender/sex")
    )
)

df1 = df1.withColumn(
    "gender/sex",
    when(
        col("gender/sex").isNull(),
        "Null"
    ).otherwise(
        col("gender/sex")
    )
)

df1 = df1.withColumn(
    "gender/sex",
    concat_ws("/", col("gender/sex"), col("contact_id"))
)
    
    
    
    
    
    
    
    from pyspark.sql.functions import regexp_replace, col

# Create sample DataFrame
data = [("John", "Doe", "   "), ("Jane", "Doe", "â€€"), ("Bob", "Smith", "   ")]
df = spark.createDataFrame(data, ["first_name", "last_name", "middle_name"])

# Replace invisible characters with "N/A"
df = df.withColumn("middle_name", regexp_replace(col("middle_name"), "[^\\p{Print}]", "N/A"))

# Show updated DataFrame
df.show()

    
from pyspark.sql.functions import ascii, length

# Display the ASCII code and length of each character in the "middle_name" column
df.select("middle_name", length("middle_name"), ascii("middle_name")).show(truncate=False)


from pyspark.sql.functions import regexp_replace

# Replace all whitespace characters with "N/A"
df = df.withColumn("middle_name", regexp_replace("middle_name", "\\s+", "N/A"))




import pyspark.sql.functions as F

# assume df is your Synapse DataFrame and col is the column you want to change
df = df.withColumn(col, F.when(F.col(col).isNull(), None).otherwise(F.col(col)))




import pyspark.sql.functions as F

# assume df is your Synapse DataFrame and col is the column you want to change
df = df.withColumn("book", F.when(F.col("book").isNull(), None).otherwise(F.col("book")))
























from pyspark.sql.functions import when, col, concat_ws

df1 = df1.withColumn(
    "gender/sex",
    when(
        col("Sex").isNotNull() & (col("Sex") != ""),
        col("Sex")
    ).otherwise(
        col("Gender")
    )
)

df1 = df1.withColumn(
    "gender/sex",
    when(
        col("gender/sex").isNull() & col("Gender").isNotNull(),
        col("Gender")
    ).otherwise(
        col("gender/sex")
    )
)

df1 = df1.withColumn(
    "gender/sex",
    when(
        col("gender/sex").isNull(),
        None
    ).otherwise(
        col("gender/sex")
    )
)

df1 = df1.withColumn(
    "gender/sex",
    concat_ws("/", col("gender/sex"), col("contact_id"))
)

https://stackoverflow.com/questions/72144956/how-to-get-the-keyvault-name-in-the-notebook-from-the-keyvault-link-in-synapse









from pyspark.sql.functions import udf, col, broadcast
from pyspark.sql.types import DoubleType
from difflib import SequenceMatcher

# Create a UDF to calculate the Levenshtein distance
def levenshtein_distance(x,y):
  if x is None or y is None:
    return None
  else:
    return SequenceMatcher(None, x, y).ratio()

levenshtein_udf = udf(levenshtein_distance, DoubleType())

from pyspark.sql.functions import coalesce

# Replace null values in columns "Primary_Address_City2" and "place20nm2" with empty strings
dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2", coalesce(col("Primary_Address_City2"), F.lit("")))
df2_postalcode = df2_postalcode.withColumn("place20nm2", coalesce(col("place20nm2"), F.lit("")))

# Create an index on the join column for faster performance
df2_postalcode.createOrReplaceTempView("df2_postalcode")
spark.sql("CREATE INDEX index_place20nm2 ON df2_postalcode (place20nm2)")

# Broadcast the smaller dataframe for faster performance
df2_postalcode = broadcast(df2_postalcode)

# Join the two dataframes on the fuzzy match
similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(df2_postalcode.alias("df2"), (levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")) > similarity_threshold))

# Select columns from both dataframes and calculate Levenshtein distance
match_data = df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.ctry20nm", levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")).alias("levenshtein_distance"))

# Cache the resulting DataFrame for faster performance in subsequent operations
match_data.cache()

# Show the first 500 rows of the DataFrame
match_data.show(500)







# Create a UDF to calculate the Levenshtein distance
def levenshtein_distance(x,y):
  if x is None or y is None:
    return None
  else:
    return SequenceMatcher(None, x, y).ratio()

levenshtein_udf = udf(levenshtein_distance, DoubleType())

from pyspark.sql.functions import coalesce

# Replace null values in columns "Primary_Address_City2" and "place20nm2" with empty strings
dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2", coalesce(col("Primary_Address_City2"), F.lit("")))
df2_postalcode = df2_postalcode.withColumn("place20nm2", coalesce(col("place20nm2"), F.lit("")))

# Enable Automatic Data Skew Optimization on the dataframes
spark.sql("ALTER TABLE dff_regrandcon SET TBLPROPERTIES ('AutoDataSkew'='True')")
spark.sql("ALTER TABLE df2_postalcode SET TBLPROPERTIES ('AutoDataSkew'='True')")

# Join the two dataframes on the fuzzy match
similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(broadcast(df2_postalcode.alias("df2")), (levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")) > similarity_threshold))

# Select columns from both dataframes and calculate Levenshtein distance
match_data = df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.ctry20nm", levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")).alias("levenshtein_distance"))






from pyspark.sql.functions import udf, col, broadcast
from pyspark.sql.types import DoubleType
from difflib import SequenceMatcher

# Create a UDF to calculate the Levenshtein distance
def levenshtein_distance(x,y):
  if x is None or y is None:
    return None
  else:
    return SequenceMatcher(None, x, y).ratio()

levenshtein_udf = udf(levenshtein_distance, DoubleType())

from pyspark.sql.functions import coalesce

# Replace null values in columns "Primary_Address_City2" and "place20nm2" with empty strings
dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2", coalesce(col("Primary_Address_City2"), F.lit("")))
df2_postalcode = df2_postalcode.withColumn("place20nm2", coalesce(col("place20nm2"), F.lit("")))

# Join the two dataframes on the fuzzy match
similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(broadcast(df2_postalcode.alias("df2")), (levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")) > similarity_threshold))

# Select columns from both dataframes and calculate Levenshtein distance
match_data = df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.ctry20nm", levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")).alias("levenshtein_distance"))






















from pyspark.sql.functions import udf, col, broadcast, soundex
from pyspark.sql.types import DoubleType

# Create a UDF to calculate the Levenshtein distance
def levenshtein_distance(x,y):
  if x is None or y is None:
    return None
  else:
    return SequenceMatcher(None, x, y).ratio()

levenshtein_udf = udf(levenshtein_distance, DoubleType())

# Create a UDF to calculate the soundex code
soundex_udf = udf(soundex)

# Replace null values in columns "Primary_Address_City2" and "place20nm2" with empty strings
dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2", coalesce(col("Primary_Address_City2"), F.lit("")))
df2_postalcode = df2_postalcode.withColumn("place20nm2", coalesce(col("place20nm2"), F.lit("")))

# Filter out any rows where either column is null
dff_regrandcon = dff_regrandcon.filter(col("Primary_Address_City2") != "")
df2_postalcode = df2_postalcode.filter(col("place20nm2") != "")

# Calculate the soundex code for both columns
dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2_soundex", soundex_udf(col("Primary_Address_City2")))
df2_postalcode = df2_postalcode.withColumn("place20nm2_soundex", soundex_udf(col("place20nm2")))

# Join the two dataframes on the fuzzy match using soundex code
similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(broadcast(df2_postalcode.alias("df2")), (soundex_udf(col("df1.Primary_Address_City2")) == soundex_udf(col("df2.place20nm2")))\
                  & (levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")) > similarity_threshold))

# Select columns from both dataframes and calculate Levenshtein distance
match_data = df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.ctry20nm", levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")).alias("levenshtein_distance"))


















































from pyspark.sql.functions import udf, col, broadcast
from pyspark.sql.types import DoubleType
from pyspark.ml.feature import Tokenizer
from pyspark.ml.feature import StopWordsRemover

# Create a UDF to calculate the Jaccard similarity
def jaccard_similarity(x,y):
  if x is None or y is None:
    return None
  else:
    x = set(x.split())
    y = set(y.split())
    return float(len(x & y))/float(len(x | y))

jaccard_udf = udf(jaccard_similarity, DoubleType())

# Tokenize and remove stop words from "Primary_Address_City2" and "place20nm2" columns
tokenizer = Tokenizer(inputCol="Primary_Address_City2", outputCol="tokens")
dff_regrandcon = tokenizer.transform(dff_regrandcon)
remover = StopWordsRemover(inputCol="tokens", outputCol="Primary_Address_City2_filtered")
dff_regrandcon = remover.transform(dff_regrandcon)

tokenizer = Tokenizer(inputCol="place20nm2", outputCol="tokens")
df2_postalcode = tokenizer.transform(df2_postalcode)
remover = StopWordsRemover(inputCol="tokens", outputCol="place20nm2_filtered")
df2_postalcode = remover.transform(df2_postalcode)

# Join the two dataframes on the fuzzy match
similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(broadcast(df2_postalcode.alias("df2")), (jaccard_udf(col("df1.Primary_Address_City2_filtered"), col("df2.place20nm2_filtered")) > similarity_threshold))

# Select columns from both dataframes and calculate Jaccard similarity
match_data = df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.ctry20nm", jaccard_udf(col("df1.Primary_Address_City2_filtered"), col("df2.place20nm2_filtered")).alias("jaccard_similarity"))

# Filter for rows with Jaccard similarity > 0.9
match_data_filtered = match_data.filter(col("jaccard_similarity") > 0.9)

# Show the resulting dataframe
match_data_filtered.show()


df2_postalcode = df2_postalcode.withColumn("place20nm2", coalesce(col("place20nm2"), F.lit("")).cast("string"))
df2_postalcode = df2_postalcode.withColumn("place20nm2_arr", split(col("place20nm2"), " "))




















from pyspark.sql.functions import udf
from pyspark.sql.types import DoubleType

# Create a UDF to calculate Jaccard similarity
def jaccard_similarity(x,y):
  if x is None or y is None:
    return None
  else:
    set_x = set(str(x).split())
    set_y = set(str(y).split())
    return float(len(set_x.intersection(set_y)) / len(set_x.union(set_y)))

jaccard_udf = udf(jaccard_similarity, DoubleType())

# Join the two dataframes on the fuzzy match
similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(broadcast(df2_postalcode.alias("df2")), (jaccard_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")) > similarity_threshold))

# Select columns from both dataframes and calculate Jaccard similarity
match_data = df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.ctry20nm", jaccard_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")).alias("jaccard_similarity"))

# Filter for rows with Jaccard similarity > threshold
match_data = match_data.filter(col("jaccard_similarity") > similarity_threshold)

# Show results
match_data.show()

















Switch Background Color = 
SWITCH(TRUE(),
    [Variable 1] > [Variable 2], 
        "Red",
    [Variable 1] < [Variable 2], 
        "Blue",
    "No color"
)


Background Text = 
IF([Switch Background Color] = "Red", "R",
    IF([Switch Background Color] = "Blue", "B", BLANK())
)














Switch Background Color =
SWITCH(TRUE(),
    NOT(ISBLANK([Variable 1])) && ISBLANK([Variable 2]), "Yellow",
    ISBLANK([Variable 1]) && NOT(ISBLANK([Variable 2])), "White",
    [Variable 1] > [Variable 2], "Red",
    [Variable 1] < [Variable 2], "Blue",
    "No color"
)




























# Import Libraries
from fuzzywuzzy import fuzz
from pyspark.sql.functions import col, udf, current_date
from pyspark.sql.types import StringType

# Read Files
df1_regrandcon = read("FUZZY.Roster")
df2_postalcode = read("FUZZY.Customer")

# Get Column Names in Data Frame
print(df1_regrandcon.columns)
print(df2_postalcode.columns)

# Create Temp Views
df1_regrandcon.createOrReplaceTempView("df1_regrandcon")
df2_postalcode.createOrReplaceTempView("df2_postalcode")

# Join
df = spark.sql("SELECT df1_regrandcon.registrant_id, df1_regrandcon.Work_PostalCode2, df1_regrandcon.Work_PostalCode, df2_postalcode.pcd, df2_postalcode.pcd2 FROM df2_postalcode CROSS JOIN df1_regrandcon ON df1_regrandcon.Work_PostalCode2 = df2_postalcode.pcd2")

# Apply Fuzzy Function
def matchstring(s1, s2):
    return fuzz.token_sort_ratio(s1, s2)

MatchUDF = udf(matchstring, StringType())

# Add columns and filter score more than 90
dvf = df.withColumn("similarity_score", MatchUDF(col("Work_PostalCode2"), col("pcd2"))).withColumn("run_date", current_date())
dvf = dvf.filter(dvf.similarity_score > 90)
dvf = dvf.drop("Address1")
dvf.show()

# Save Data Frame
dvf.write.save("output_folder", format="csv", header=True)


def matchstring(s1,s2):
    return float(fuzz.token_sort_ratio(s1,s2))/100

MatchUDF = udf(matchstring, FloatType())












from pyspark.sql.functions import col

df.select('a', 'b', 'c', 'd').where((col('a').isNotNull()) & (col('a') != '') & (col('b').isNotNull()) & (col('b') != ''))






from pyspark.sql.functions import col, regexp_extract, when

df = spark.read.csv('path/to/data.csv', header=True)

# Extract any letters from postcode column
postcode_letters = regexp_extract(col('postcode'), r'[a-zA-Z]+', 0)

# Replace city with blank if postcode contains only numbers
df = df.withColumn('city', when(postcode_letters == '', '', col('city')))


























from pyspark.sql.functions import input_file_name

# Set the path for the CSV files
path = '<your-path>'

# Load all CSV files into a single DataFrame
df = spark.read.csv(path + '/nspl21_aug_2022_uk*.csv', header=True)

# Add a column to the DataFrame to indicate the source file for each row
df = df.withColumn('filename', input_file_name())


































from pyspark.sql.functions import udf, col, round
from pyspark.sql.types import DoubleType, StringType
from difflib import SequenceMatcher

# Define the UDF for string comparison
compare_strings = udf(lambda x, y: SequenceMatcher(None, x, y).ratio() * 100, DoubleType())

# Set the similarity threshold as a decimal value
similarity_threshold = 0.9

# Join the dataframes and perform the string comparison
matched_dataJoin2 = df1_regrandconH.join(df2_postalcodeH, df1_regrandconH.Home_PostalCode == df2_postalcodeH.pcd2) \
    .select(df1_regrandconH.registrant_id, df1_regrandconH.Primary_Address_PostalCode, df1_regrandconH.Home_PostalCode,
            df2_postalcodeH.pcd, df2_postalcodeH.pcd2, compare_strings(df1_regrandconH.Home_PostalCode,
                                                                      df2_postalcodeH.pcd2).alias("similarity"))

# Round the similarity score to 2 decimal places and filter the results by the similarity threshold
matched_data2 = matched_dataJoin2.withColumn("similarity", round(col("similarity"), 2)) \
    .filter(col("similarity") > similarity_threshold)















from fuzzywuzzy import fuzz
from pyspark.sql.functions import udf, lower, levenshtein
from pyspark.sql.types import DoubleType, StringType
from pyspark.sql.functions import round

compare_strings = udf(lambda x, y: fuzz.token_sort_ratio(x, y), StringType())

similarity_threshold = 90.0

matched_data = df1_regrandconH.join(df2_postalcodeH, lower(df1_regrandconH.Home_PostalCode) == lower(df2_postalcodeH.pcd2)) \
    .withColumn("similarity", levenshtein(lower(df1_regrandconH.Home_PostalCode), lower(df2_postalcodeH.pcd2))) \
    .filter(compare_strings(lower(df1_regrandconH.Home_PostalCode), lower(df2_postalcodeH.pcd2)) > similarity_threshold) \
    .withColumn("similarity", round(col("similarity").cast(DoubleType()), 2)) \
    .select(df1_regrandconH.registrant_id, df1_regrandconH.Primary_Address_PostalCode, df1_regrandconH.Home_PostalCode, df2_postalcodeH.pcd, df2_postalcodeH.pcd2, col("similarity"))





https://www.data.gov.uk/dataset/7ec10db7-c8f4-4a40-8d82-8921935b4865/national-statistics-postcode-lookup-uk















Questions 1:  To determine those who do not Geocode to the UK, their postcode/address will have to be invalid as a UK geodata or not existing in the geodata (ONS) dictionary for us to identify this group.  
Question 2:  For the UK region, my suspension is this is the flat structure I have used to map the data which is also coming from ONS 2021. The one you shared IPN_GB_2021 does has limited regions so was not able to use it.   
Question 3: for this, I did not considered if they where in the UK, I can tweak this further by filtering nationality | country  to be in UK before doing the match










Days_Hours_Minutes = 
VAR Minutes = SUM('df1'[value])
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
RETURN
    SWITCH (
        TRUE (),
        Days >= 7, INT(Days / 7) & " weeks, " & MOD(Days, 7) & " days, ",
        Days > 1, Days & " days, ",
        Days = 1, "1 day, ",
        Days = 0, "",
        "",
    ) & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00")















Days_Hours_Minutes = 
VAR Minutes = SUM('df1'[value])
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
RETURN
    Days + Hours / 24 + Minutes_Remaining / 1440
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    Days_Hours_Minutes = 
VAR Minutes = SUM('df1'[value])
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
RETURN
    SWITCH (
        TRUE (),
        Days >= 7, INT(Days / 7) & " weeks, " & INT(MOD(Days, 7)) & " days, ",
        Days > 1, Days & " days, ",
        Days = 1, "1 day, ",
        Days = 0, "",
        TRUE (), ""
    ) & 
    IF(Hours > 0, FORMAT(Hours, "0") & " hours, ", "") &
    FORMAT(Minutes_Remaining, "0") & " minutes"

















Days_Hours_Minutes = 
VAR Minutes = SUM('df1'[value])
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
RETURN
    IF(
        Days >= 7,
        INT(Days / 7) & " weeks, " & INT(MOD(Days, 7)) & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
        IF(
            Days > 1,
            Days & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
            IF(
                Days = 1,
                "1 day, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
                FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00")
            )
        )
    )













Days_Hours_Minutes Label = 
VAR TotalMinutes = SUM('df1'[value])
VAR TotalHours = TotalMinutes / 60
VAR Days = INT(TotalHours / 24)
VAR Hours = INT(MOD(TotalHours, 24))
VAR Minutes = INT(MOD(TotalMinutes, 60))
RETURN
    Days & " days, " &
    FORMAT(Hours, "00") & " hours, " &
    FORMAT(Minutes, "00") & " minutes"







Days_Hours_Minutes = 
VAR Minutes = SUM('df1'[value])
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
RETURN
    TIME(Days * 24 + Hours, Minutes_Remaining, 0)




Days_Hours_Minutes_formatted = 
FORMAT(
    Days_Hours_Minutes, 
    IF(DAYS(Days_Hours_Minutes) > 0, "d.", "") & "hh:mm"
)




If you are getting an error that says "Cannot convert type text to type number", it's likely because you are trying to use a text-based DAX measure in a numerical context.

To fix this error, you can modify the DAX measure to return a numerical value instead of a text value. Here's the modified DAX measure:

Days_Hours_Minutes =
VAR Minutes = SUM('df1'[value])
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
RETURN
Days * 1440 + Hours * 60 + Minutes_Remaining

This DAX measure will return the total number of minutes as a numerical value. You can then use this measure in the field value in Power BI and format the result using the "Duration" format in the "Modeling" tab.

To display the result in days, hours, and minutes format, you can create a calculated column using the original DAX measure and format the result as a text value. Here's the modified DAX measure for the calculated column:

Days_Hours_Minutes_Text =
VAR Minutes = [Days_Hours_Minutes]
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
RETURN
IF(
Days >= 7,
INT(Days / 7) & " weeks, " & INT(MOD(Days, 7)) & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
IF(
Days > 1,
Days & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
IF(
Days = 1,
"1 day, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00")
)
)
)

This DAX measure will return a text value in the days, hours, and minutes format. You can use this calculated column in the field value in Power BI.



Days_Hours = 
VAR Minutes = SUM('df1'[value])
VAR Days = INT(Minutes / 1440)
VAR Hours = INT(MOD(Minutes, 1440) / 60)
RETURN
    Days & " days, " & FORMAT(Hours, "00") & " hours"
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
Days_Hours_Minutes = 
VAR Minutes = SUM('df1'[value])
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
RETURN
    IF(
        Days >= 7,
        INT(Days / 7) & " weeks, " & INT(MOD(Days, 7)) & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
        IF(
            Days > 1,
            Days & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
            IF(
                Days = 1,
                "1 day, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
                FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00")
            )
        )
    )
    
    
    
    
    
  Days_Hours_Minutes = 
VAR Minutes = SUM('df1'[value])
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
VAR Label = 
    IF(
        Days >= 7,
        INT(Days / 7) & " weeks, " & INT(MOD(Days, 7)) & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
        IF(
            Days > 1,
            Days & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
            IF(
                Days = 1,
                "1 day, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
                FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00")
            )
        )
    )
RETURN
    IF(HASONEVALUE('df1'[value]), Label, BLANK())
  
    











Minutes = SUM('df1'[value])
Create a measure that calculates the number of days, hours, and minutes in the format that you want:
less
Copy code
Days_Hours_Minutes = 
VAR Minutes = SUM('df1'[value])
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
RETURN
    IF(
        Days >= 7,
        INT(Days / 7) & " weeks, " & INT(MOD(Days, 7)) & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
        IF(
            Days > 1,
            Days & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
            IF(
                Days = 1,
                "1 day, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
                FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00")
            )
        )
    )
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    VAR Minutes = SELECTEDVALUE('df1'[value])
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
RETURN
    IF(
        Days >= 7,
        INT(Days / 7) & " weeks, " & INT(MOD(Days, 7)) & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
        IF(
            Days > 1,
            Days & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
            IF(
                Days = 1,
                "1 day, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
                FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00")
            )
        )
    )












Days_Hours_Minutes = 
VAR Minutes = [Your_Column_Name]
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
RETURN
    IF(
        Days >= 7,
        INT(Days / 7) & " weeks, " & INT(MOD(Days, 7)) & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
        IF(
            Days > 1,
            Days & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
            IF(
                Days = 1,
                "1 day, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
                FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00")
            )
        )
    )
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    Duration =
VAR Minutes = SUM('df1'[value])
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
RETURN
    IF(
        Days >= 7,
        INT(Days / 7) & " weeks, " & INT(MOD(Days, 7)) & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
        IF(
            Days > 1,
            Days & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
            IF(
                Days = 1,
                "1 day, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
                FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00")
            )
        )
    )










=CONCATENATEX(VALUES('df1'[value]), [Duration], ", ")































To display the result of the above DAX expression as a label in the value field of a Power BI visual, you can use a workaround by converting the text output of the measure into a numeric value.

Here's an example of how to do it:

Create a new measure in the Power BI data model and name it "Duration (in minutes)":

scss
Copy code
Duration (in minutes) = SUM('df1'[value])
This measure simply calculates the sum of the "value" field in the 'df1' table.

Create another new measure in the Power BI data model and name it "Duration (text)":

less
Copy code
Duration (text) =
VAR Minutes = [Duration (in minutes)]
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
RETURN
    IF(
        Days >= 7,
        INT(Days / 7) & " weeks, " & INT(MOD(Days, 7)) & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
        IF(
            Days > 1,
            Days & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
            IF(
                Days = 1,
                "1 day, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
                FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00")
            )
        )
    )
This measure uses the same DAX expression as before to calculate the duration in days, hours, and minutes, but returns the result as a text string.

Create a new visual on your report canvas and drag the "Duration (in minutes)" measure into the Values field.

Click on the "Duration (in minutes)" measure in the Values field and select "Don't summarize" from the dropdown list.

Create a new measure and name it "Duration (numeric)":

scss
Copy code
Duration (numeric) = VALUE([Duration (text)])
This measure converts the text output of the "Duration (text)" measure into a numeric value.

Drag the "Duration (numeric)" measure into the Values field of the visual.

Click on the "Duration (numeric)" measure in the Values field and select "Don't summarize" from the dropdown list.

In the Visualizations pane, select the visual type that you want to use to display the duration.

Add a new data label to the visual and select "Duration (text)" as the label field.

Adjust the formatting of the visual as desired.

Once you have completed these steps, the visual should display the duration as a label in the value field, while still allowing you to use a numeric measure for aggregation and other calculations.




Duration (numeric) =
IF(
    ISNUMBER(VALUE([Duration (text)])),
    VALUE([Duration (text)]),
    BLANK()
)










Duration (text) = 
VAR Minutes = SUM('df1'[value])
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
RETURN
    IF(
        Days >= 7,
        (Days + (INT(Days / 7) * 2)) & ":" & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
        IF(
            Days > 1,
            (Days * 1440 + Hours * 60 + Minutes_Remaining) & "",
            IF(
                Days = 1,
                (1440 + Hours * 60 + Minutes_Remaining) & "",
                (Hours * 60 + Minutes_Remaining) & ""
            )
        )
    )


Duration (numeric) = 
VAR DurationText = [Duration (text)]
RETURN
    VALUE(
        SUBSTITUTE(
            REPLACE(
                SUBSTITUTE(DurationText, ":", "."),
                ".",
                "",
                FIND(".", SUBSTITUTE(DurationText, ":", "."))
            ),
            ",",
            "."
        )
    )





Days_Hours_Minutes = 
VAR SelectedValue = SELECTEDVALUE('Table'[Column])
VAR Minutes = CALCULATE(SUM('df1'[value]), 'Table'[Column] = SelectedValue)
VAR Days = INT(Minutes / 1440)
VAR Hours = INT((Minutes - (Days * 1440)) / 60)
VAR Minutes_Remaining = MOD(Minutes, 60)
RETURN
    IF(
        Days >= 7,
        INT(Days / 7) & " weeks, " & INT(MOD(Days, 7)) & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
        IF(
            Days > 1,
            Days & " days, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
            IF(
                Days = 1,
                "1 day, " & FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00"),
                FORMAT(Hours, "00") & ":" & FORMAT(Minutes_Remaining, "00")
            )
        )
    )
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
Formatted Time =
VAR TotalMinutes = SUM(TechData2[value])
VAR Days = INT(TotalMinutes / 1440)
VAR Hours = INT((TotalMinutes - (Days * 1440)) / 60)
VAR Minutes = TotalMinutes - (Days * 1440) - (Hours * 60)
RETURN
IF(
    Days > 0,
    Days & " day, " & Hours & " hr, " & Minutes & " mins",
    IF(
        Hours > 0,
        Hours & " hr, " & Minutes & " mins",
        Minutes & " mins"
    )
)




Formatted Time =
VAR TotalMinutes = SUM(TechData2[value])
VAR Days = INT(TotalMinutes / 1440)
VAR Hours = INT((TotalMinutes - (Days * 1440)) / 60)
VAR Minutes = TotalMinutes - (Days * 1440) - (Hours * 60)
RETURN
VALUE(
    IF(
        Days > 0,
        Days & " day, " & Hours & " hr, " & Minutes & " mins",
        IF(
            Hours > 0,
            Hours & " hr, " & Minutes & " mins",
            Minutes & " mins"
        )
    )
)

[>1440]d" day, "h" hr, "m" mins";[>60]h" hr, "m" mins";m" mins"


























Formatted Time =
VAR TotalMinutes = [Total Minutes]
VAR Days = INT(TotalMinutes / 1440)
VAR Hours = INT(MOD(TotalMinutes, 1440) / 60)
VAR Minutes = MOD(TotalMinutes, 60)
VAR Result =
    SWITCH(
        TRUE(),
        Days > 0, Days & " day" & IF(Days > 1, "s", "") & ", " & Hours & " hr" & IF(Hours > 1, "s", "") & ", " & Minutes & " min" & IF(Minutes > 1, "s", ""),
        Hours > 0, Hours & " hr" & IF(Hours > 1, "s", "") & ", " & Minutes & " min" & IF(Minutes > 1, "s", ""),
        TRUE(), Minutes & " min" & IF(Minutes > 1, "s", "")
    )
RETURN Result


















Formatted Time =
VAR TotalMinutes = [Total Minutes]
VAR Days = INT(TotalMinutes / 1440)
VAR Hours = INT(MOD(TotalMinutes, 1440) / 60)
VAR Minutes = MOD(TotalMinutes, 60)
VAR Result =
    SWITCH(
        TRUE(),
        Days > 0, Days & " day" & IF(Days > 1, "s", "") & ", " & Hours & " hr" & IF(Hours > 1, "s", "") & ", " & Minutes & " min" & IF(Minutes > 1, "s", ""),
        Hours > 0, Hours & " hr" & IF(Hours > 1, "s", "") & ", " & Minutes & " min" & IF(Minutes > 1, "s", ""),
        TRUE(), Minutes & " min" & IF(Minutes > 1, "s", "")
    )
RETURN Result







Formatted Total = 
VAR TotalMinutes = TechData2[value]
VAR Days = INT(TotalMinutes / 1440)
VAR Hours = INT(MOD(TotalMinutes, 1440) / 60)
VAR Minutes = MOD(TotalMinutes, 60)
VAR Result =
    SWITCH(
        TRUE(),
        Days > 0, Days & " day" & IF(Days > 1, "s", "") & ", " & Hours & " hr" & IF(Hours > 1, "s", "") & ", " & Minutes & " min" & IF(Minutes > 1, "s", ""),
        Hours > 0, Hours & " hr" & IF(Hours > 1, "s", "") & ", " & Minutes & " min" & IF(Minutes > 1, "s", ""),
        TRUE(), Minutes & " min" & IF(Minutes > 1, "s", "")
    )
RETURN TotalMinutes & " mins, " & Result












Chelsie Eiden's Duration = 
VAR DurationInMins = SUM(TechData2[Value])
// There are 60 minutes in an hour
VAR Hours = INT(DurationInMins / 60)
// Remaining minutes are the remainder of the minutes divided by 60 after subtracting out the hours 
VAR Minutes = ROUNDUP(MOD(DurationInMins, 60), 0) // We round up here to get a whole number
RETURN
// We put the hours and minutes into the proper "place"
Hours * 100 + Minutes





Chelsie Eiden's Duration = 
VAR DurationInMins = SUM(TechData2[Value])
// There are 60 minutes in an hour
VAR TotalHours = INT(DurationInMins / 60)
// Remaining minutes are the remainder of the minutes divided by 60 after subtracting out the hours 
VAR RemainingMinutes = MOD(DurationInMins, 60)
// There are 24 hours in a day
VAR TotalDays = INT(TotalHours / 24)
// Remaining hours are the remainder of the hours divided by 24 after subtracting out the days
VAR RemainingHours = MOD(TotalHours, 24)
// Round up the remaining minutes to get a whole number
VAR RoundedMinutes = ROUNDUP(RemainingMinutes, 0)
RETURN
// We put the days, hours, and minutes into the proper "place"
TotalDays & "d " & RemainingHours & "h " & RoundedMinutes & "m"














Chelsie Eiden's Duration = 
VAR DurationInMins = SUM(TechData2[Value])
// There are 1440 minutes in a day (24 hours x 60 minutes)
VAR Days = INT(DurationInMins / 1440)
// There are 60 minutes in an hour
VAR Hours = INT(MOD(DurationInMins, 1440) / 60)
// Remaining minutes are the remainder of the minutes divided by 60 after subtracting out the hours 
VAR Minutes = ROUNDUP(MOD(DurationInMins, 60), 0) // We round up here to get a whole number
RETURN
// We put the days, hours, and minutes into the proper "place"
Days * 1440 + Hours * 60 + Minutes

[>=1440]d \d\a\y\s hh\:mm;[>=60]hh\:mm;mm


[>=1440]d\d\ h\h\:mm\m;[>=60]h\h\:mm\m;mm\m

FirstDateOfMonth = DATE(YEAR('Calendar'[Date]), MONTH('Calendar'[Date]), 1)





=IF(ISERROR(MATCH(A1, {"Country1","Country2","Country3","Country4"},0)),"No","Yes")
















from pyspark.sql.functions import when

# Create a dataframe with sample data
data = [("A", "gf"), ("B", "hj"), ("C", "tr"), ("D", "xyz")]
df = spark.createDataFrame(data, ["col1", "beter"])

# Define a mapping dictionary for recoding values
mapping = {"gf": "tg", "hj": "yu", "tr": "ts"}

# Use the 'when' function to create a new column 'new_beter' with recoded values
df = df.withColumn("new_beter", when(df.beter.isin(list(mapping.keys())), mapping[df.beter]).otherwise(df.beter))

# Show the final dataframe
df.show()













from pyspark.sql.functions import when

# Create a dataframe with sample data
data = [("A", "gf"), ("B", "hj"), ("C", "tr"), ("D", "xyz")]
df = spark.createDataFrame(data, ["col1", "beter"])

# Define a mapping dictionary for recoding values
mapping = {"gf": "tg", "hj": "yu", "tr": "ts"}

# Use the 'when' function to create a new column 'new_beter' with recoded values
df = df.withColumn("new_beter", when(df.beter.isin(list(mapping.keys())), mapping[df.beter]).otherwise(df.beter))

# Show the final dataframe
df.show()



from pyspark.sql.functions import when

# Create a dataframe with sample data
data = [("A", "gf"), ("B", "hj"), ("C", "tr"), ("D", "xyz")]
df = spark.createDataFrame(data, ["col1", "beter"])

# Use the 'when' function to create a new column 'new_beter' with recoded values
df = df.withColumn("new_beter", when(df.beter == "gf", "tg").when(df.beter == "hj", "yu").when(df.beter == "tr", "ts").otherwise(df.beter))

# Show the final dataframe
df.show()





from pyspark.sql.functions import when, col

# create example dataframe
df = spark.createDataFrame([(1, None, ''), (2, 'foo', 'bar'), (3, 'baz', None)], ['id', 'c', 'd'])

# add flag column
df_with_flag = df.withColumn('flag', when(col('c').isNull() | (col('c') == '') | col('d').isNull() | (col('d') == ''), 0).otherwise(1))

# show result
df_with_flag.show()






from pyspark.sql.functions import col, concat, sum

# Assuming your dataframe is called "df"
df_with_total = df.withColumn("total", concat(
    sum(col("a")).cast("string"),
    sum(col("b")).cast("string"),
    sum(col("c")).cast("string"),
    sum(col("d")).cast("string"),
    sum(col("e")).cast("string"),
    sum(col("f")).cast("string")
))

# Show the resulting dataframe
df_with_total.show()



from pyspark.sql.functions import col, expr, sum

# Define a list of column names to sum up
cols_to_sum = ["a", "b", "c", "d", "e", "f"]

# Sum up the columns in each row and create a new column called "total"
my_df_with_total = my_df.withColumn("total", sum(expr("+".join(cols_to_sum)).cast("int")).cast("string"))

# Show the resulting DataFrame
my_df_with_total.show()






from pyspark.sql.functions import col, expr, struct, sum

# Define a list of column names to sum up
cols_to_sum = ["a", "b", "c", "d", "e", "f"]

# Create a struct of all columns to sum up
cols_struct = struct(*[col(c).cast("int") for c in cols_to_sum])

# Sum up the columns in each row and create a new column called "total"
my_df_with_total = my_df.withColumn("total", sum(cols_struct).cast("string"))

# Show the resulting DataFrame
my_df_with_total.show()










from pyspark.sql.functions import col, sum

# Define a list of column names to sum up
cols_to_sum = ['Art_psychotherapist', 'Art_therapist', 'Clinical_psychologist', 
               'Counselling_psychologist', 'Diagnostic_radiographer', 'Drama_therapist',
               'Educational_psychologist', 'Health_psychologist', 'Forensic_psychologist',
               'Music_therapist', 'Occupational_psychologist', 'Orthotist', 'Prosthetist',
               'Sport_and_exercise_psychologist', 'Therapeutic_radiographer']

# Cast each column to an integer and sum them up
total_col = sum([col(c).cast('int') for c in cols_to_sum])

# Add the new column to the DataFrame
df1RegConmp1 = df1RegConmp1.withColumn('total', total_col.cast('string'))

# Show the resulting DataFrame
df1RegConmp1.show()









from pyspark.sql.functions import col, sum, try_cast

cols_to_sum = ['Art_psychotherapist', 'Art_therapist', 'Clinical_psychologist', 
               'Counselling_psychologist', 'Diagnostic_radiographer', 'Drama_therapist',
               'Educational_psychologist', 'Health_psychologist', 'Forensic_psychologist',
               'Music_therapist', 'Occupational_psychologist', 'Orthotist', 'Prosthetist',
               'Sport_and_exercise_psychologist', 'Therapeutic_radiographer']

total_col = sum([try_cast(col(c), 'integer') for c in cols_to_sum])

df1RegConmp1 = df1RegConmp1.withColumn('total', total_col.cast('string'))

df1RegConmp1.show()









amendfRegConmp = dfRegConmp.withColumn("Art_psychotherapist", when(dfRegConmp["Art_psychotherapist"].isNull(), lit(0)).otherwise(lit(1))) \
                           .withColumn("Art_therapist", when(dfRegConmp["Art_therapist"].isNull(), lit(0)).otherwise(lit(1))) \
                           .withColumn("Clinical_psychologist", when(dfRegConmp["Clinical_psychologist"].isNull(), lit(0)).otherwise(lit(1))) \
                           .withColumn("Counselling_psychologist", when(dfRegConmp["Counselling_psychologist"].isNull(), lit(0)).otherwise(lit(1))) \
                           .withColumn("Diagnostic_radiographer", when(dfRegConmp["Diagnostic_radiographer"].isNull(), lit(0)).otherwise(lit(1))) \
                           .withColumn("Drama_therapist", when(dfRegConmp["Drama_therapist"].isNull(), lit(0)).otherwise(lit(1))) \
                           .withColumn("Educational_psychologist", when(dfRegConmp["Educational_psychologist"].isNull(), lit(0)).otherwise(lit(1))) \
                           .withColumn("Forensic_psychologist", when(dfRegConmp["Forensic_psychologist"].isNull(), lit(0)).otherwise(lit(1))) \
                           .withColumn("Health_psychologist", when(dfRegConmp["Health_psychologist"].isNull(), lit(0)).otherwise(lit(1))) \
                           .withColumn("Music_therapist", when(dfRegConmp["Music_therapist"].isNull(), lit(0)).otherwise(lit(1))) \
                           .withColumn("Occupational_psychologist", when(dfRegConmp["Occupational_psychologist"].isNull(), lit(0)).otherwise(lit(1))) \
                           .withColumn("Orthotist", when(dfRegConmp["Orthotist"].isNull(), lit(0)).otherwise(lit(1))) \
                           .withColumn("Prosthetist", when(dfRegConmp["Prosthetist"].isNull(), lit(0)).otherwise(lit(1))) \
                           .withColumn("Sport_and_exercise_psychologist", when(dfRegConmp["Sport_and_exercise_psychologist"].isNull(), lit(0)).otherwise(lit(1))) \
                           .withColumn("Therapeutic_radiographer", when(dfRegConmp["Therapeutic_radiographer"].isNull(), lit(0)).otherwise(lit(1))) \
                           .drop('Registration_ID')
                           
                           
                           
                           
                           
                           
   https://digital.nhs.uk/services/organisation-data-service/export-data-files/csv-downloads/office-for-national-statistics-data          
                           
                           https://geoportal.statistics.gov.uk/datasets/national-statistics-postcode-lookup-2011-census-august-2022-1/about
                           
                           https://www.data.gov.uk/dataset/02c0321b-e9a4-4acd-97d9-d7e1914d57bb/national-statistics-postcode-lookup-camden

                           https://www.data.gov.uk/dataset/7ec10db7-c8f4-4a40-8d82-8921935b4865/national-statistics-postcode-lookup-uk
                           
                           
                           
                           
                           
                           
                           
                           
from pyspark.sql.functions import expr

cols_to_transpose = ['c', 'd', 'e']

stack_expr = ', '.join([f"stack(3, '{col}', {col})" for col in cols_to_transpose])
df = df.selectExpr("a", "b", expr(stack_expr).alias("cdet"))

df = df.selectExpr("a", "b", "cdet.col0 as c", "cdet.col1 as d", "cdet.col2 as e")







from pyspark.sql.functions import expr

cols_to_transpose = ['c', 'd', 'e']

stack_expr = ', '.join([f"stack(3, '{col}', {col})" for col in cols_to_transpose])
df = df.selectExpr("a", "b", f"explode(array({stack_expr})) as cdet")

df = df.selectExpr("a", "b", "cdet['0'] as c", "cdet['1'] as d", "cdet['2'] as e")




















from pyspark.sql.functions import posexplode

cols_to_transpose = ['c', 'd', 'e']

df = df.selectExpr("a", "b", f"posexplode(array({', '.join(cols_to_transpose)})) as (pos, cde)")

df = df.selectExpr("a", "b", "CASE pos WHEN 0 THEN cde ELSE NULL END as c", 
                   "CASE pos WHEN 1 THEN cde ELSE NULL END as d", 
                   "CASE pos WHEN 2 THEN cde ELSE NULL END as e")













from pyspark.sql.functions import concat_ws, array

cols_to_transpose = ['c', 'd', 'e']

df = df.selectExpr("a", "b", f"concat_ws(',', array({', '.join(cols_to_transpose)})) as cde")

df = df.selectExpr("a", "b", "split(cde, ',')[0] as c", 
                   "split(cde, ',')[1] as d", 
                   "split(cde, ',')[2] as e")














from pyspark.sql.functions import array, when, lit

cols_to_transpose = ['c', 'd', 'e']

df = df.selectExpr("a", "b", f"array({', '.join(cols_to_transpose)}) as cde")

for i, col in enumerate(cols_to_transpose):
    df = df.withColumn(f"modality_{i}", when(df.cde[i] == 1, col).otherwise(None))

modality_cols = [f"modality_{i}" for i in range(len(cols_to_transpose))]
df = df.selectExpr("a", "b", f"array({', '.join(modality_cols)}) as modality")

df = df.withColumn("modality_value", lit(cols_to_transpose)) \
       .select("a", "b", "modality", "modality_value")













from pyspark.sql.functions import concat_ws, array, expr

cols_to_transpose = ['c', 'd', 'e']

df = df.withColumn("cde", array(*[expr(f"IF({col}=1,'{col}',NULL)") for col in cols_to_transpose]))

modality_expr = f"filter(cde, x -> x is not NULL)"
df = df.select("a", "b", expr(modality_expr).alias("modality"))

df = df.withColumn("single", concat_ws(",", *[expr(f"coalesce({col},'')") for col in cols_to_transpose]))

df.show()






from pyspark.sql.functions import concat_ws, array, expr

cols_to_transpose = ['prostest', 'redio']

df = dftest.withColumn("cde", array(*[expr(f"IF({col}=1,'{col}',NULL)") for col in cols_to_transpose]))

modality_expr = f"filter(cde, x -> x is not NULL)"
df = df.select("createdbyname", "registrationname", expr(modality_expr).alias("modality"))

df = df.withColumn("single", concat_ws(",", *[expr(f"coalesce({col},'')") for col in cols_to_transpose]))

df.show()

