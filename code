from pyspark.sql.functions import udf, col, broadcast
from pyspark.sql.types import DoubleType
from difflib import SequenceMatcher

# Create a UDF to calculate the Levenshtein distance
def levenshtein_distance(x,y):
  if x is None or y is None:
    return None
  else:
    return SequenceMatcher(None, x, y).ratio()

levenshtein_udf = udf(levenshtein_distance, DoubleType())

from pyspark.sql.functions import coalesce

# Replace null values in columns "Primary_Address_City2" and "place20nm2" with empty strings
dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2", coalesce(col("Primary_Address_City2"), F.lit("")))
df2_postalcode = df2_postalcode.withColumn("place20nm2", coalesce(col("place20nm2"), F.lit("")))

# Join the two dataframes on the fuzzy match
similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(broadcast(df2_postalcode.alias("df2")), (levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")) > similarity_threshold))

# Select columns from both dataframes and calculate Levenshtein distance
match_data = df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.ctry20nm", levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")).alias("levenshtein_distance"))




WITH subquery AS (
  SELECT studentid, studentcourse
  FROM student
  GROUP BY studentid, studentcourse
  HAVING COUNT(*) = 1
)
SELECT studentid, 
       STRING_AGG(studentcourse, ', ') WITHIN GROUP (ORDER BY studentcourse) AS courses
FROM subquery
GROUP BY studentid
HAVING COUNT(*) = 2;












WITH subquery AS (
  SELECT studentid, studentcourse
  FROM student
  GROUP BY studentid, studentcourse
  HAVING COUNT(*) = 1
)
SELECT studentid, 
       STRING_AGG(studentcourse, ', ') WITHIN GROUP (ORDER BY studentcourse) AS courses,
       CASE 
         WHEN COUNT(*) = 2 THEN 'Yes'
         ELSE 'No'
       END AS meeting_condition
FROM subquery
GROUP BY studentid;

















WITH subquery AS (
  SELECT studentid, studentcourse
  FROM student
  GROUP BY studentid, studentcourse
  HAVING COUNT(*) = 1
)
SELECT TOP 10 studentid, 
       STRING_AGG(studentcourse, ', ') WITHIN GROUP (ORDER BY studentcourse) AS courses,
       CASE 
         WHEN COUNT(*) = 2 THEN 'Yes'
         ELSE 'No'
       END AS meeting_condition
FROM subquery
GROUP BY studentid
ORDER BY COUNT(*) DESC;
















WITH subquery AS (
  SELECT studentid, studentcourse
  FROM student
  GROUP BY studentid, studentcourse
  HAVING COUNT(*) = 1
)
SELECT TOP 10 r.request_id, 
       STRING_AGG(s.studentcourse, ', ') WITHIN GROUP (ORDER BY s.studentcourse) AS courses,
       CASE 
         WHEN COUNT(*) = 2 THEN 'Yes'
         ELSE 'No'
       END AS meeting_condition
FROM registration r
LEFT JOIN (
  SELECT studentid, 
         studentcourse
  FROM subquery
  GROUP BY studentid
) s
ON r.studentid = s.studentid
GROUP BY r.request_id, r.studentid
ORDER BY COUNT(*) DESC;









WITH subquery AS (
  SELECT Registrant_ID, 
         Registrant_Name,
         CASE 
           WHEN COUNT(*) = 2 THEN 'Yes'
           ELSE 'No'
         END AS meeting_condition
  FROM (
    SELECT Registrant_ID, Registrant_Name
    FROM registration2
    GROUP BY Registrant_ID, Registrant_Name
    HAVING COUNT(*) = 1
  ) sub
  GROUP BY Registrant_ID
)
SELECT r.Registrant_ID,
       r.Registrant_Name,
       r.active,
       s.registrantdifference,
       s.meeting_condition
FROM registration r
LEFT JOIN (
  SELECT Registrant_ID, 
         STRING_AGG(Registrant_Name, ', ') WITHIN GROUP (ORDER BY Registrant_Name) AS registrantdifference,
         meeting_condition
  FROM subquery
) s
ON r.Registrant_ID = s.Registrant_ID;



WITH subquery AS (
  SELECT Registrant_ID, 
         STRING_AGG(Registration_Name, ', ') WITHIN GROUP (ORDER BY Registration_Name) AS registrantdifference,
         CASE 
           WHEN COUNT(DISTINCT Registration_Name) = 2 THEN 'Yes'
           ELSE 'No'
         END AS meeting_condition
  FROM (
    SELECT Registrant_ID, Registration_Name
    FROM Registration2
    GROUP BY Registrant_ID, Registration_Name
  ) sub
  GROUP BY Registrant_ID
)
SELECT r.Registrant_ID,
       r.Registrant_Name,
       r.active,
       s.registrantdifference,
       s.meeting_condition
FROM registration r
LEFT JOIN subquery s
ON r.Registrant_ID = s.Registrant_ID;






















































WITH subquery AS (
SELECT Registrant_ID,
STRING_AGG(Profession_Name, ', ') WITHIN GROUP (ORDER BY Profession_Name) AS registrantdifference,
CASE
WHEN COUNT(DISTINCT Profession_Name) = 2 THEN 'Yes'
ELSE 'No'
END AS meeting_condition,
CASE
WHEN SUM(CASE WHEN Regsitration_State_Description IN ('inactive', 'blank') THEN 1 ELSE 0 END) = 2 THEN 1
ELSE 0
END AS flag
FROM (
SELECT Registrant_ID,
Profession_Name,
Regsitration_State_Description
FROM [dbo].[registrations]
GROUP BY Registrant_ID,
Profession_Name,
Regsitration_State_Description
) sub
GROUP BY Registrant_ID
)

SELECT Optevia_Contact_ID AS Contact_ID ,
r.Registrant_ID,
r.[Regsitration_State_Description],
r.[Registration_Status_Description],
r.[Profession_Name],
r.[Date_Registered],
r.Registration_Name,
r.[First_Registration_Date] AS [First Registration Date],
r.[Date_Deregistered] AS [Date Deregistered],
r.[Date_Registered] AS [Date most recently registered],
r.[Date_Registered_From] AS [Date registered from],
r.[Date_Registered_To] AS [Date registered to],
s.registrantdifference,
s.meeting_condition AS [ProfessionChange(Yes_No)],
s.flag AS [Flag for Inactive and Blank]
FROM [dbo].[registrations] r
LEFT JOIN subquery s
ON r.Registrant_ID = s.Registrant_ID
Left join [dbo].[contacts] c
ON r.Registrant_ID = c.[Contact_ID]
GROUP BY Optevia_Contact_ID,
r.Registrant_ID,
r.Registration_Name,
r.[Regsitration_State_Description],
r.[Registration_Status_Description],
r.[Profession_Name],
r.[First_Registration_Date],
r.[Date_Registered],
r.[Date_Deregistered],
r.[Date_Registered_From],
r.[Date_Registered_To],
s.registrantdifference,
s.meeting_condition,
s.flag
































Hearingconcludedontime? =
IF(
ISBLANK(Raw_Data2[Panel Member Cancelled Date]),
"Unknown",
IF(
Raw_Data2[Panel Member Cancelled Date] > 0,
"Cancelled",
IF(
Raw_Data2[Hearing To] = Raw_Data2[Actual Hearing End Date],
"On time",
IF(
Raw_Data2[Hearing To] > Raw_Data2[Actual Hearing End Date],
"Early",
"Later"
)
)
)
)


= IF(Raw_Data2[Panel Member Cancelled Date] > 0, "Cancelled",
IF(Raw_Data2[Actual Hearing End Date]=Raw_Data2[Hearing To],"On time",
IF(Raw_Data2[Actual Hearing End Date] > Raw_Data2[Hearing To],"Early","Later")
)
)









Join the two dataframes on the fuzzy match
similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(df2_postalcode.alias("df2"), 
    (levenshtein_udf(
        col("df1.Primary_Address_City2").cast("string").alias("text1"),
        col("df2.place20nm2").cast("string").alias("text2")) > similarity_threshold))

Select columns from both dataframes
matched_data=df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Primary_Address_City2"
,"df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.place20nm2"
,"df2.eer20nm","df2.ctry20nm",levenshtein_udf(
    col("df1.Primary_Address_City2").cast("string"), 
    col("df2.place20nm2").cast("string")).alias("levenshtein_distance"))
    
    
    
    
    
    
    
    
    
    
 from diff_match_patch import diff_match_patch
from pyspark.sql.functions import udf, col, coalesce
from pyspark.sql import functions as F
from pyspark.sql.types import DoubleType

def levenshtein_distance(text1,text2):
    if text1 is None or text2 is None:
        return 0
    else:
        dmp = diff_match_patch()
        dmp.Diff_Timeout = 0.0
        diff = dmp.diff_main(text1, text2, False)
        common_text = sum([len(txt) for op, txt in diff if op == 0])
        text_length = max(len(text1), len(text2))
        sim = common_text / text_length
        return sim

levenshtein_udf = udf(levenshtein_distance, DoubleType())

# Make sure that the columns are of string type
dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2", coalesce(F.cast(col("Primary_Address_City2"), "string"), F.lit("")))
df2_postalcode = df2_postalcode.withColumn("place20nm2", coalesce(F.cast(col("place20nm2"), "string"), F.lit("")))

similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(df2_postalcode.alias("df2"), (levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")) > similarity_threshold))

matched_data=df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Primary_Address_City2",
"df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.place20nm2",
"df2.eer20nm","df2.ctry20nm",levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")).alias("levenshtein_distance"))






from diff_match_patch import diff_match_patch
from pyspark.sql.functions import coalesce, udf, lit, col, when
from pyspark.sql.types import DoubleType

def levenshtein_distance(text1,text2):
    if text1 is None or text2 is None:
        return 0
    else:
        dmp = diff_match_patch()
        dmp.Diff_Timeout = 0.0
        diff = dmp.diff_main(text1, text2, False)
        common_text = sum([len(txt) for op, txt in diff if op == 0])
        text_length = max(len(text1), len(text2))
        sim = common_text / text_length
        return sim

levenshtein_udf = udf(levenshtein_distance, DoubleType())

dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2", coalesce(col("Primary_Address_City2"), lit("")))
df2_postalcode = df2_postalcode.withColumn("place20nm2", coalesce(col("place20nm2"), lit("")))

similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(df2_postalcode.alias("df2"), when(levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")) > similarity_threshold, 1).otherwise(0) == 1)

matched_data = df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Primary_Address_City2",
                                "df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm",
                                "df2.eer20nm","df2.place20nm2","df2.eer20nm","df2.ctry20nm",
                                levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")).alias("levenshtein_distance"))
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                from diff_match_patch import diff_match_patch
from pyspark.sql.functions import udf, col, coalesce, when
from pyspark.sql.types import DoubleType, StringType

def levenshtein_distance(text1,text2):
    if text1 is None or text2 is None:
        return 0
    else:
        dmp = diff_match_patch()
        dmp.Diff_Timeout = 0.0
        diff = dmp.diff_main(text1, text2, False)
        common_text = sum([len(txt) for op, txt in diff if op == 0])
        text_length = max(len(text1), len(text2))
        sim = common_text / text_length
        return sim

levenshtein_udf = udf(levenshtein_distance, DoubleType())

# Cast the columns to string type
dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2", when(col("Primary_Address_City2").isNotNull(), col("Primary_Address_City2").cast(StringType())).otherwise(""))
df2_postalcode = df2_postalcode.withColumn("place20nm2", when(col("place20nm2").isNotNull(), col("place20nm2").cast(StringType())).otherwise(""))

# Join the two dataframes on the fuzzy match
similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(df2_postalcode.alias("df2"), (levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")) > similarity_threshold))

# Select columns from both dataframes
matched_data = df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Primary_Address_City2","df1.Work_PostalCode","df1.Work_PostalCode2",
                                "df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.place20nm2","df2.eer20nm","df2.ctry20nm",
                                levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")).alias("levenshtein_distance"))
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
import pyspark.sql.functions as F
from fuzzywuzzy import fuzz
from pyspark.sql.types import IntegerType

def matchstring(s1, s2):
    return fuzz.ratio(s1, s2)

MatchUDF = F.udf(matchstring, IntegerType())

similarity_threshold = 90
df_joined = dff_regrandcon.alias("df1").join(df2_postalcode.alias("df2"), (MatchUDF(F.col("df1.Primary_Address_City2"), F.col("df2.place20nm2")) > similarity_threshold))

df_result = df_joined.groupBy("df1.Primary_Address_City2").agg(F.max(MatchUDF(F.col("df1.Primary_Address_City2"), F.col("df2.place20nm2"))).alias("max_score")).filter(F.col("max_score") >= similarity_threshold)
df_result = df_result.select("df1.*").join(df_joined, (df_result["df1.Primary_Address_City2"] == df_joined["df1.Primary_Address_City2"]) & (df_result["max_score"] == MatchUDF(F.col("df1.Primary_Address_City2"), F.col("df2.place20nm2"))))
#









DJoined = dff_regrandcon.crossJoin(df2_postalcode)
matched_data = DJoined.withColumn("score", MatchUDF(F.col("Primary_Address_City2"), F.col("place20nm2")))
max_score_row = matched_data.agg(F.max("score").alias("max_score")).collect()[0]
selected_row = matched_data.filter(F.col("score") == max_score_row["max_score"]).collect()[0]



from pyspark.sql.functions import broadcast
DJoined = dff_regrandcon.crossJoin(broadcast(df2_postalcode))
matched_data = DJoined.withColumn("score", MatchUDF(F.col("Primary_Address_City2"), F.col("place20nm2")))
max_score_row = matched_data.agg(F.max("score").alias("max_score")).collect()[0]
selected_row = matched_data.filter(F.col("score") == max_score_row["max_score"]).collect()[0]

from pyspark.sql.functions import broadcast
DJoined = dff_regrandcon.crossJoin(broadcast(df2_postalcode))
matched_data = DJoined.withColumn("score", MatchUDF(F.col("Primary_Address_City2"), F.col("place20nm2")))
max_score_row = matched_data.agg(F.max("score").alias("max_score")).collect()[0]
selected_row = matched_data.filter(F.col("score") == max_score_row["max_score"]).collect()[0]




df2_postalcode = df.withColumn("place20nm2", regexp_replace(df.place20nm, "[^a-zA-Z0-9]+", ""))




from pyspark.sql.functions import trim, regexp_replace, lower

df2_postalcode = df
df2_postalcode = df2_postalcode.withColumn("place20nm2", regexp_replace(df2_postalcode.place20nm, "\s+", " "))
df2_postalcode = df2_postalcode.withColumn("place20nm2", regexp_replace(df2_postalcode.place20nm2, "[.']+", ""))
df2_postalcode = df2_postalcode.withColumn("place20nm2", regexp_replace(df2_postalcode.place20nm2, "[^a-zA-Z0-9]+", ""))
df2_postalcode = df2_postalcode.withColumn("place20nm2", trim(df2_postalcode.place20nm2))
df2_postalcode = df2_postalcode.withColumn("place20nm2", regexp_replace(df2_postalcode.place20nm2, "1", "one"))
df2_postalcode = df2_postalcode.withColumn("place20nm2", regexp_replace(df2_postalcode.place20nm2, "2", "two"))
df2_postalcode = df2_postalcode.withColumn("place20nm2", regexp_replace(df2_postalcode.place20nm2, "St", "Street"))
df2_postalcode = df2_postalcode.withColumn("place20nm2", lower(df2_postalcode.place20nm2))





from pyspark.sql.functions import col, expr

matched_data = dff_regrandcon.alias("xrt") \
                              .join(df2_postalcode.alias("xrtx"), 
                                    expr("xrt.Primary_Address_City like concat('%', xrtx.place20nm, '%')")) \
                              .select("xrt.*", "xrtx.*")



from pyspark.sql.functions import col, expr

matched_data = dff_regrandcon.alias("xrt") \
                              .join(df2_postalcode.alias("xrtx"), 
                                    expr("xrt.Primary_Address_City like concat(xrtx.place20nm, '_')")) \
                              .select("xrt.*", "xrtx.*")



SELECT *
FROM dff_regrandcon xrt
JOIN df2_postalcode xrtx
ON xrt.Primary_Address_City LIKE concat(xrtx.place20nm, '_')


















let
    Source = List.Dates(#date(2000, 1, 1), #date(2030, 12, 31)),
    #"Converted to Table" = Table.FromList(Source, Splitter.SplitByNothing(), {"Date"}),
    #"Expanded Date" = Table.ExpandColumn(#"Converted to Table", "Date", {"Year", "Quarter", "Month", "Week", "Day", "Weekday"}),
    #"Added Custom" = Table.AddColumn(#"Expanded Date", "Year Month", each [Year] * 100 + [Month]),
    #"Added Custom1" = Table.AddColumn(#"Added Custom", "Year Month Day", each [Year Month] * 100 + [Day]),
    #"Added Custom2" = Table.AddColumn(#"Added Custom1", "Month Name", each Date.MonthName([Month]), type text),
    #"Added Custom3" = Table.AddColumn(#"Added Custom2", "Quarter Name", each "Q" & Number.ToText([Quarter]), type text),
    #"Added Custom4" = Table.AddColumn(#"Added Custom3", "Weekday Name", each Date.DayOfWeekName([Weekday]), type text)
in
    #"Added Custom4"










https://exceleratorbi.com.au/conditional-formatting-using-icons-in-power-bi/

https://learn.microsoft.com/en-us/power-bi/visuals/power-bi-visualization-scorecard-visual

https://learn.microsoft.com/en-us/power-bi/visuals/power-bi-visualization-tables?tabs=powerbi-desktop


Current Month Sales =
CALCULATE(SUM(Sales[SalesValue]), DATESYTD(Calendar[Date]), Sales[Category]=EARLIER(Sales[Category]))

Last Month Sales =
CALCULATE(SUM(Sales[SalesValue]), DATESYTD(Calendar[Date])-1, Sales[Category]=EARLIER(Sales[Category]))




Last Month Sales =
CALCULATE(SUM(Sales[SalesValue]), FILTER(Sales, Sales[Date] >= EOMONTH(TODAY(), -1) && Sales[Date] < EOMONTH(TODAY(), 0)))







Last Month Sales =
CALCULATE(
    SUM(Sales[SalesValue]),
    FILTER(
        ALL(Calendar),
        Calendar[Date] >= MIN(Calendar[Date]) &&
        Calendar[Date] <= MAX(Calendar[Date]) &&
        MONTH(Calendar[Date]) = MONTH(TODAY()) - 1 &&
        YEAR(Calendar[Date]) = YEAR(TODAY())
    )
)





Current and Last Month Value =
VAR CurrentMonth = MONTH(TODAY())
VAR CurrentYear = YEAR(TODAY())
VAR LastMonth = MONTH(TODAY())-1
VAR LastYear = YEAR(TODAY())

RETURN
SUMX(
    FILTER(
        CALCULATETABLE(PeopleData, CALENDAR2),
        MONTH(MIN(Calendar2[Date])) = CurrentMonth && YEAR(MIN(Calendar2[Date])) = CurrentYear
    ),
    PeopleData[Value]
)
+
SUMX(
    FILTER(
        CALCULATETABLE(PeopleData, CALENDAR2),
        MONTH(MIN(Calendar2[Date])) = LastMonth && YEAR(MIN(Calendar2[Date])) = LastYear
    ),
    PeopleData[Value]
)














Last Month Value =
SUMX(
    FILTER(
        CALCULATETABLE(PeopleData, CALENDAR2),
        MONTH(MIN(Calendar2[Date])) = MONTH(TODAY())-1 && YEAR(MIN(Calendar2[Date])) = YEAR(TODAY())
    ),
    PeopleData[Value]
)










Previous Value = 
CALCULATE(
    SUM(PeopleData[Value]),
    FILTER(
        ALL(PeopleData),
        PeopleData[Date] = MAX(PeopleData[Date]) - 1
    )
)







Latest Month Value =
CALCULATE(SUM(peopledata[value]),
FILTER(ALL(peopledata), peopledata[date] = MAX(peopledata[date])))

And here's the DAX measure for the "Previous Month Value":

Previous Month Value =
CALCULATE(SUM(peopledata[value]),
FILTER(ALL(peopledata), peopledata[date] =
MAX(peopledata[date]) - 1))





Previous Month Value =
CALCULATE(
SUM(peopledata[value]),
FILTER(
peopledata,
peopledata[date] >= EOMONTH(MAX(peopledata[date]), -1) &&
peopledata[date] < MAX(peopledata[date])
)
)

















Previous Month Value =
CALCULATE(
SUM(peopledata[value]),
FILTER(
peopledata,
peopledata[categorypeopledata] = SELECTEDVALUE(peopledata[categorypeopledata]) &&
peopledata[date] >= EOMONTH(MAX(peopledata[date]), -1) &&
peopledata[date] < MAX(peopledata[date])
)
)


Previous Month Value =
VAR CurrentCategory = SELECTEDVALUE(peopledata[categorypeopledata])
VAR CurrentDate = MAX(peopledata[date])
VAR PreviousMonth = EOMONTH(CurrentDate, -1)

RETURN
SUMX(
FILTER(peopledata, peopledata[categorypeopledata] = CurrentCategory && peopledata[date] >= PreviousMonth && peopledata[date] < CurrentDate),
peopledata[value]
)










Previous Month Value =
VAR CurrentCategory = SELECTEDVALUE(peopledata[categorypeopledata])
VAR CurrentDate = MAX(peopledata[date])
VAR PreviousMonth = EOMONTH(CurrentDate, -1)

RETURN
SUMX(
FILTER(peopledata, peopledata[categorypeopledata] = CurrentCategory && peopledata[date] >= PreviousMonth && peopledata[date] < CurrentDate),
peopledata[value]
)


















Previous Value Reformatted = 
    VAR CurrentColumn = SELECTEDVALUE(df[col1])
    VAR PreviousValue = [Your Measure Name]
RETURN 
    IF(CurrentColumn = "food name", PreviousValue/1000 & "k", 
    IF(CurrentColumn = "house name", PreviousValue/1000 & "k", 
    IF(CurrentColumn = "invoice name", PreviousValue, BLANK())))
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    Previous Value Reformatted = 
    VAR CurrentColumn = SELECTEDVALUE(df[col1])
    VAR PreviousValue = [Your Measure Name]
RETURN 
    IF(CurrentColumn = "food name", FORMAT(PreviousValue, "0,0") & "k", 
    IF(CurrentColumn = "house name", FORMAT(PreviousValue, "0,0") & "k", 
    IF(CurrentColumn = "invoice name", FORMAT(PreviousValue, "0.0%"), BLANK())))
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
from pyspark.sql.functions import when, col, concat_ws

sex_values = df1.select("Sex").distinct().rdd.flatMap(lambda x: x).collect()

df1 = df1.withColumn(
    "gender/sex",
    when(
        col("Sex").isin(sex_values) & col("Sex").isNotNull() & (col("Sex") != ""),
        col("Sex")
    ).otherwise(
        col("Gender")
    )
)

df1 = df1.withColumn(
    "gender/sex",
    when(
        col("gender/sex").isNull(),
        col("Gender")
    ).otherwise(
        col("gender/sex")
    )
)

df1 = df1.withColumn(
    "gender/sex",
    when(
        col("gender/sex").isNull(),
        "Null"
    ).otherwise(
        col("gender/sex")
    )
)

df1 = df1.withColumn(
    "gender/sex",
    concat_ws("/", col("gender/sex"), col("contact_id"))
)
    
    
    
    
    
    
    
    from pyspark.sql.functions import regexp_replace, col

# Create sample DataFrame
data = [("John", "Doe", "   "), ("Jane", "Doe", "â€€"), ("Bob", "Smith", "   ")]
df = spark.createDataFrame(data, ["first_name", "last_name", "middle_name"])

# Replace invisible characters with "N/A"
df = df.withColumn("middle_name", regexp_replace(col("middle_name"), "[^\\p{Print}]", "N/A"))

# Show updated DataFrame
df.show()

    
from pyspark.sql.functions import ascii, length

# Display the ASCII code and length of each character in the "middle_name" column
df.select("middle_name", length("middle_name"), ascii("middle_name")).show(truncate=False)


from pyspark.sql.functions import regexp_replace

# Replace all whitespace characters with "N/A"
df = df.withColumn("middle_name", regexp_replace("middle_name", "\\s+", "N/A"))




import pyspark.sql.functions as F

# assume df is your Synapse DataFrame and col is the column you want to change
df = df.withColumn(col, F.when(F.col(col).isNull(), None).otherwise(F.col(col)))




import pyspark.sql.functions as F

# assume df is your Synapse DataFrame and col is the column you want to change
df = df.withColumn("book", F.when(F.col("book").isNull(), None).otherwise(F.col("book")))
























from pyspark.sql.functions import when, col, concat_ws

df1 = df1.withColumn(
    "gender/sex",
    when(
        col("Sex").isNotNull() & (col("Sex") != ""),
        col("Sex")
    ).otherwise(
        col("Gender")
    )
)

df1 = df1.withColumn(
    "gender/sex",
    when(
        col("gender/sex").isNull() & col("Gender").isNotNull(),
        col("Gender")
    ).otherwise(
        col("gender/sex")
    )
)

df1 = df1.withColumn(
    "gender/sex",
    when(
        col("gender/sex").isNull(),
        None
    ).otherwise(
        col("gender/sex")
    )
)

df1 = df1.withColumn(
    "gender/sex",
    concat_ws("/", col("gender/sex"), col("contact_id"))
)

https://stackoverflow.com/questions/72144956/how-to-get-the-keyvault-name-in-the-notebook-from-the-keyvault-link-in-synapse









from pyspark.sql.functions import udf, col, broadcast
from pyspark.sql.types import DoubleType
from difflib import SequenceMatcher

# Create a UDF to calculate the Levenshtein distance
def levenshtein_distance(x,y):
  if x is None or y is None:
    return None
  else:
    return SequenceMatcher(None, x, y).ratio()

levenshtein_udf = udf(levenshtein_distance, DoubleType())

from pyspark.sql.functions import coalesce

# Replace null values in columns "Primary_Address_City2" and "place20nm2" with empty strings
dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2", coalesce(col("Primary_Address_City2"), F.lit("")))
df2_postalcode = df2_postalcode.withColumn("place20nm2", coalesce(col("place20nm2"), F.lit("")))

# Create an index on the join column for faster performance
df2_postalcode.createOrReplaceTempView("df2_postalcode")
spark.sql("CREATE INDEX index_place20nm2 ON df2_postalcode (place20nm2)")

# Broadcast the smaller dataframe for faster performance
df2_postalcode = broadcast(df2_postalcode)

# Join the two dataframes on the fuzzy match
similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(df2_postalcode.alias("df2"), (levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")) > similarity_threshold))

# Select columns from both dataframes and calculate Levenshtein distance
match_data = df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.ctry20nm", levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")).alias("levenshtein_distance"))

# Cache the resulting DataFrame for faster performance in subsequent operations
match_data.cache()

# Show the first 500 rows of the DataFrame
match_data.show(500)







# Create a UDF to calculate the Levenshtein distance
def levenshtein_distance(x,y):
  if x is None or y is None:
    return None
  else:
    return SequenceMatcher(None, x, y).ratio()

levenshtein_udf = udf(levenshtein_distance, DoubleType())

from pyspark.sql.functions import coalesce

# Replace null values in columns "Primary_Address_City2" and "place20nm2" with empty strings
dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2", coalesce(col("Primary_Address_City2"), F.lit("")))
df2_postalcode = df2_postalcode.withColumn("place20nm2", coalesce(col("place20nm2"), F.lit("")))

# Enable Automatic Data Skew Optimization on the dataframes
spark.sql("ALTER TABLE dff_regrandcon SET TBLPROPERTIES ('AutoDataSkew'='True')")
spark.sql("ALTER TABLE df2_postalcode SET TBLPROPERTIES ('AutoDataSkew'='True')")

# Join the two dataframes on the fuzzy match
similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(broadcast(df2_postalcode.alias("df2")), (levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")) > similarity_threshold))

# Select columns from both dataframes and calculate Levenshtein distance
match_data = df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.ctry20nm", levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")).alias("levenshtein_distance"))






from pyspark.sql.functions import udf, col, broadcast
from pyspark.sql.types import DoubleType
from difflib import SequenceMatcher

# Create a UDF to calculate the Levenshtein distance
def levenshtein_distance(x,y):
  if x is None or y is None:
    return None
  else:
    return SequenceMatcher(None, x, y).ratio()

levenshtein_udf = udf(levenshtein_distance, DoubleType())

from pyspark.sql.functions import coalesce

# Replace null values in columns "Primary_Address_City2" and "place20nm2" with empty strings
dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2", coalesce(col("Primary_Address_City2"), F.lit("")))
df2_postalcode = df2_postalcode.withColumn("place20nm2", coalesce(col("place20nm2"), F.lit("")))

# Join the two dataframes on the fuzzy match
similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(broadcast(df2_postalcode.alias("df2")), (levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")) > similarity_threshold))

# Select columns from both dataframes and calculate Levenshtein distance
match_data = df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.ctry20nm", levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")).alias("levenshtein_distance"))






















from pyspark.sql.functions import udf, col, broadcast, soundex
from pyspark.sql.types import DoubleType

# Create a UDF to calculate the Levenshtein distance
def levenshtein_distance(x,y):
  if x is None or y is None:
    return None
  else:
    return SequenceMatcher(None, x, y).ratio()

levenshtein_udf = udf(levenshtein_distance, DoubleType())

# Create a UDF to calculate the soundex code
soundex_udf = udf(soundex)

# Replace null values in columns "Primary_Address_City2" and "place20nm2" with empty strings
dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2", coalesce(col("Primary_Address_City2"), F.lit("")))
df2_postalcode = df2_postalcode.withColumn("place20nm2", coalesce(col("place20nm2"), F.lit("")))

# Filter out any rows where either column is null
dff_regrandcon = dff_regrandcon.filter(col("Primary_Address_City2") != "")
df2_postalcode = df2_postalcode.filter(col("place20nm2") != "")

# Calculate the soundex code for both columns
dff_regrandcon = dff_regrandcon.withColumn("Primary_Address_City2_soundex", soundex_udf(col("Primary_Address_City2")))
df2_postalcode = df2_postalcode.withColumn("place20nm2_soundex", soundex_udf(col("place20nm2")))

# Join the two dataframes on the fuzzy match using soundex code
similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(broadcast(df2_postalcode.alias("df2")), (soundex_udf(col("df1.Primary_Address_City2")) == soundex_udf(col("df2.place20nm2")))\
                  & (levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")) > similarity_threshold))

# Select columns from both dataframes and calculate Levenshtein distance
match_data = df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.ctry20nm", levenshtein_udf(col("df1.Primary_Address_City2"), col("df2.place20nm2")).alias("levenshtein_distance"))


















































from pyspark.sql.functions import udf, col, broadcast
from pyspark.sql.types import DoubleType
from pyspark.ml.feature import Tokenizer
from pyspark.ml.feature import StopWordsRemover

# Create a UDF to calculate the Jaccard similarity
def jaccard_similarity(x,y):
  if x is None or y is None:
    return None
  else:
    x = set(x.split())
    y = set(y.split())
    return float(len(x & y))/float(len(x | y))

jaccard_udf = udf(jaccard_similarity, DoubleType())

# Tokenize and remove stop words from "Primary_Address_City2" and "place20nm2" columns
tokenizer = Tokenizer(inputCol="Primary_Address_City2", outputCol="tokens")
dff_regrandcon = tokenizer.transform(dff_regrandcon)
remover = StopWordsRemover(inputCol="tokens", outputCol="Primary_Address_City2_filtered")
dff_regrandcon = remover.transform(dff_regrandcon)

tokenizer = Tokenizer(inputCol="place20nm2", outputCol="tokens")
df2_postalcode = tokenizer.transform(df2_postalcode)
remover = StopWordsRemover(inputCol="tokens", outputCol="place20nm2_filtered")
df2_postalcode = remover.transform(df2_postalcode)

# Join the two dataframes on the fuzzy match
similarity_threshold = 0.9
df_joined = dff_regrandcon.alias("df1").join(broadcast(df2_postalcode.alias("df2")), (jaccard_udf(col("df1.Primary_Address_City2_filtered"), col("df2.place20nm2_filtered")) > similarity_threshold))

# Select columns from both dataframes and calculate Jaccard similarity
match_data = df_joined.select("df1.registrant_id","df1.Primary_Address_City2","df1.Primary_Address_City","df1.Work_PostalCode","df1.Work_PostalCode2","df1.pcd","df1.pcd2","df2.place20nm","df2.eer20nm","df2.ctry20nm", jaccard_udf(col("df1.Primary_Address_City2_filtered"), col("df2.place20nm2_filtered")).alias("jaccard_similarity"))

# Filter for rows with Jaccard similarity > 0.9
match_data_filtered = match_data.filter(col("jaccard_similarity") > 0.9)

# Show the resulting dataframe
match_data_filtered.show()


df2_postalcode = df2_postalcode.withColumn("place20nm2", coalesce(col("place20nm2"), F.lit("")).cast("string"))
df2_postalcode = df2_postalcode.withColumn("place20nm2_arr", split(col("place20nm2"), " "))
