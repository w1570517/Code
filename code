
from pyspark.sql import functions as F

# Create a new column "total_hit" as the sum of all hits in columns "col a", "col b" and "col c"
df = df.withColumn("total_hit", F.sum(F.when(F.col("col a") == 'dd', 1).otherwise(0), F.when(F.col("col b") == 'dd', 1).otherwise(0), F.when(F.col("col c") == 'dd', 1).otherwise(0)))

# Create a new column "percentage" based on the percentage of hits in the "total_hit" column
df = df.withColumn("percentage", F.round((F.col("total_hit") / F.count("*")) * 100, 2))


https://docs.google.com/presentation/d/1WrkeJ9-CjuotTXoa4ZZlB3UPBXpxe4B3FMs9R9tn34I/edit#slide=id.g164b1bac824_0_5291


https://www.oreilly.com/radar/radar-trends-to-watch-december-2022/

https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai-in-2022-and-a-half-decade-in-review



from pyspark.sql.functions import col
from pyspark.sql.functions import udf
from pyspark.sql.types import DoubleType
from pyspark_fuzzy.functions import jaro_winkler_similarity

# Define a UDF for calculating the Jaro-Winkler similarity
udf_similarity = udf(lambda s1, s2: jaro_winkler_similarity(s1, s2), DoubleType())

# Join the two datasets
df1.join(df2, udf_similarity(col("cola"), col("col e")) >= 0.6)

# Select the columns of interest
result = df1.select("cola", "colb", "colc", "col e", "col f", udf_similarity(col("cola"), col("col e")).alias("similarity"))





from pyspark.sql.functions import levenshtein

# Join the two datasets
df_join = df1.alias("df1").join(df2.alias("df2"), levenshtein(col("df1.cola"), col("df2.col e")) <= 60)

# Select the columns of interest
result = df_join.select("df1.cola", "df1.colb", "df1.colc", "df2.col e", "df2.col f", levenshtein(col("df1.cola"), col("df2.col e")).alias("similarity"))





from pyspark.sql.functions import levenshtein, jaro_winkler

# Define a UDF to take the average of Jaro-Winkler and Levenshtein distance similarity scores
def combined_similarity(s1, s2):
    return (jaro_winkler(s1, s2) + levenshtein(s1, s2)) / 2

udf_combined_similarity = udf(combined_similarity, DoubleType())

# Join the two datasets
df_join = df1.alias("df1").join(df2.alias("df2"), udf_combined_similarity(col("df1.cola"), col("df2.col e")) >= threshold)

# Select the columns of interest
result = df_join.select("df1.cola", "df1.colb", "df1.colc", "df2.col e", "df2.col f", udf_combined_similarity(col("df1.cola"), col("df2.col e")).alias("similarity"))




from pyspark.ml.feature import TokenSimilarity
from pyspark.ml import Pipeline
from pyspark.sql.functions import col

# Define the input columns
input_cols = ["cola", "col e"]

# Create the TokenSimilarity estimator
similarity = TokenSimilarity(inputCols=input_cols, outputCol="similarity", similarityType="jw")

# Create a pipeline with the TokenSimilarity estimator
pipeline = Pipeline(stages=[similarity])

# Fit the pipeline on the data
model = pipeline.fit(df1.alias("df1").join(df2.alias("df2"), on=col("df1.cola") == col("df2.col e")))

# Get the transformed data
result = model.transform(df1.alias("df1").join(df2.alias("df2"), on=col("df1.cola") == col("df2.col e")))

# Select the columns of interest
result = result.select("df1.cola", "df1.colb", "df1.colc", "df2.col e", "df2.col f", "similarity")







from pyspark.sql.functions import udf
from pyspark.sql.types import DoubleType
from fuzzywuzzy import fuzz

# Define a UDF for calculating the Jaro-Winkler similarity
udf_jw_similarity = udf(lambda s1, s2: fuzz.token_set_ratio(s1, s2), DoubleType())

# Define a UDF for calculating the Levenshtein distance
udf_levenshtein_distance = udf(lambda s1, s2: fuzz.token_set_ratio(s1, s2), DoubleType())

# Join the two datasets
df_join = df1.alias("df1").join(df2.alias("df2"))

# Select the columns of interest
result = df_join.select("df1.cola", "df1.colb", "df1.colc", "df2.col e", "df2.col f", udf_jw_similarity(col("df1.cola"), col("df2.col e")).alias("jw_similarity"), udf_levenshtein_distance(col("df1.cola"), col("df2.col e")).alias("levenshtein_distance"))













from pyspark.sql.functions import col
from pyspark.sql.types import DoubleType
from spark_sequencer import Sequencer

# Initialize the Sequencer
sequencer = Sequencer(threshold=80, similarity_type='jaccard')

# Define a UDF for calculating the similarity
udf_similarity = udf(lambda s1, s2: sequencer.similarity(s1, s2), DoubleType())

# Join the two datasets
df_join = df1.alias("df1").join(df2.alias("df2"))

# Select the columns of interest
result = df_join.select("df1.cola", "df1.colb", "df1.colc", "df2.col e", "df2.col f", udf_similarity(col("df1.cola"), col("df2.col e")).alias("similarity"))


















from pyspark.sql.functions import udf
from pyspark.sql.types import DoubleType
import difflib

# Define a UDF for calculating the similarity
udf_similarity = udf(lambda s1, s2: difflib.SequenceMatcher(None, s1, s2).ratio(), DoubleType())

# Join the two datasets
df_join = df1.alias("df1").join(df2.alias("df2"), (udf_similarity(col("df1.cola"), col("df2.col e")) >= 0.8))

# Select the columns of interest
result = df_join.select("df1.cola", "df1.colb", "df1.colc", "df2.col e", "df2.col f", udf_similarity(col("df1.cola"), col("df2.col e")).alias("similarity"))



df = df.withColumn("age", when((col("age") < 5) | (col("age") > 90), lit(None)).otherwise(col("age")))





Create a new column called "Gender & Profession" with the following formula:
=CONCATENATE([Gender]," & ", [Profession])

Create a new table using the "Gender & Profession" column and the "COUNTROWS" function with the following formula:
=SUMMARIZE(GROUPBY(TableName, "Gender & Profession"), "Total", COUNTROWS(TableName))

Create a new measure called "Percentage" with the following formula:
=DIVIDE(COUNTROWS(FILTER(TableName, [Gender & Profession] = EARLIER([Gender & Profession]))),COUNTROWS(TableName))

Drag the "Gender & Profession" column to the Rows field, and the "Percentage" measure to the Values field, and you will have a percentage breakdown of gender and profession.







Create a new measure called "Total Count" with the following formula:
=COUNTROWS(TableName)

Create a new measure called "Gender Count" with the following formula:
=COUNTROWS(FILTER(TableName, [Gender] = "male"||[Gender] = "female"))

Create a new measure called "Percentage" with the following formula:
=DIVIDE([Gender Count], [Total Count])

Use the "Gender" and "Profession" columns to group or pivot the data in your visual, and then drag the "Percentage" measure to the values field to show the breakdown of gender against profession in percentage.




=COUNTROWS(FILTER(TableName, [Gender] = SELECTEDVALUE(TableName[Gender]) && [Profession] = SELECTEDVALUE(TableName[Profession]))) / [Gender Count]







from pyspark.sql.functions import udf
from fuzzywuzzy import fuzz

# Create a UDF to calculate the similarity ratio
similarity_udf = udf(lambda x, y: fuzz.ratio(x, y), IntegerType())

# Join the two dataframes on the fuzzy match
similarity_threshold = 80
df_joined = df.alias("df1").join(df2.alias("df2"), (similarity_udf(col("df1.a"), col("df2.e")) > similarity_threshold))

# Select columns from both dataframes
df_joined.select("df1.*", "df2.*", "fuzzy_similarity")


%pip install fuzzywuzzy










from pyspark.sql.functions import udf
from difflib import SequenceMatcher

# Create a UDF to calculate the Levenshtein distance
levenshtein_udf = udf(lambda x, y: SequenceMatcher(None, x, y).ratio(), DoubleType())

# Join the two dataframes on the fuzzy match
similarity_threshold = 0.8
df_joined = df.alias("df1").join(df2.alias("df2"), (levenshtein_udf(col("df1.a"), col("df2.e")) > similarity_threshold))

# Select columns from both dataframes
df_joined.select("df1.*", "df2.*", "levenshtein_distance")












from pyspark.sql.functions import udf, col
from pyspark.sql.types import DoubleType
from difflib import SequenceMatcher

# Create a UDF to calculate the Levenshtein distance
def levenshtein_distance(x,y):
    if x is None or y is None:
        return None
    else:
        return SequenceMatcher(None, x, y).ratio()

levenshtein_udf = udf(levenshtein_distance, DoubleType())

# Join the two dataframes on the fuzzy match
similarity_threshold = 0.8
df_joined = df.alias("df1").join(df2.alias("df2"), (levenshtein_udf(col("df1.a"), col("df2.e")) > similarity_threshold))

# Select columns from both dataframes
df_joined.select("df1.*", "df2.*", "levenshtein_distance")



from pyspark.sql.functions import coalesce

df = df.withColumn("a",coalesce(col("a"),F.lit("")))
df2 = df2.withColumn("e",coalesce(col("e"),F.lit("")))



df_joined.select("df1.a","df1.b","df2.e","df2.f",levenshtein_udf(col("df1.a"), col("df2.e")).alias("levenshtein_distance"))






from pyspark.sql import SparkSession

# Create a Spark session
spark = SparkSession.builder.appName("DataLakeWriteExample").getOrCreate()

# Load the data into a DataFrame
df = spark.read.csv("path/to/large/file.csv", header=True, inferSchema=True)

# Write the DataFrame in batches to Azure Data Lake Storage
batch_size = 10000
num_batches = df.count() // batch_size + 1
for i in range(num_batches):
    batch_df = df.limit(batch_size).offset(i * batch_size)
    batch_df.write.parquet("adl://{}/data/batch_{}".format(adl_account_name, i), mode="overwrite")

# Stop the Spark session
spark.stop()













from pyspark.sql import functions as F

# Create a Window to partition the data by student_id and order by student_id and date
window = Window.partitionBy("student_id").orderBy("student_id", "student_active_degree")

# Use the window to calculate the previous value for each student's degree
df = df.withColumn("previous_degree", F.lag(df["student_degree"]).over(window))

# Filter the DataFrame to show only the students who have changed their degree from 0 to 1
df = df.filter((df["student_degree"] == 1) & (df["previous_degree"] == 0))

# Show the results
df.show()














from pyspark.sql import functions as F

# Create a Window to partition the data by student_id and order by student_id and date
window = Window.partitionBy("student_id").orderBy("student_id", "student_active_degree")

# Use the window to calculate the previous value for each student's degree
df = df.withColumn("previous_degree", F.lag(df["student_degree"]).over(window))

# Add a new column with a flag to indicate if the student has changed their degree
df = df.withColumn("degree_change", F.when((df["student_degree"] == 1) & (df["previous_degree"] == 0), 1).otherwise(0))

# Filter the DataFrame to show only the students who have changed their degree
df = df.filter(df["degree_change"] == 1)

# Show the results
df.show()










from pyspark.sql import functions as F

# Create a Window to partition the data by student_id and order by student_id and date
window = Window.partitionBy("student_id").orderBy("student_id", "student_degree")

# Use the window to calculate the previous value for each student's active degree
df = df.withColumn("previous_active_degree", F.lag(df["student_active_degree"]).over(window))

# Add a new column with a flag to indicate if the student has changed their active degree
df = df.withColumn("degree_change", F.when(((df["student_active_degree"] == "active") | (df["student_active_degree"].isNull())) & (df["previous_active_degree"] == "inactive"), 1).otherwise(0))

# Filter the DataFrame to show only the students who have changed their active degree
df = df.filter(df["degree_change"] == 1)

# Show the results
df.show()



# Define a user-defined function to calculate the Jaccard similarity
jaccard = udf(lambda x, y: len(set(x).intersection(y)) / len(set(x).union(y)))
